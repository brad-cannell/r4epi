[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R 4 Epidemiology",
    "section": "",
    "text": "Welcome\nWelcome to R for Epidemiology!\nThis electronic textbook was originally created to accompany the Introduction to R Programming for Epidemiologic Research course at the University of Texas Health Science Center School of Public Health. However, we hope it will be useful to anyone who is interested in R, epidemiology, or human health and well-being.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "R 4 Epidemiology",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book is currently a work in progress (and probably always will be); however, there are already many people who have played an important role (some unknowingly) in helping develop it thus far. First, we’d like to offer our gratitude to all past, current, and future members of the R Core Team for maintaining this amazing, free software. We’d also like to express our gratitude to everyone at Posit. You are also developing and giving away some amazing software. In particular, we’d like to acknowledge Garrett Grolemund and Hadley Wickham. Both have had a huge impact on how we use and teach R. We’d also like to thank our students for all the feedback they’ve given us while taking our courses. In particular, we want to thank Jared Wiegand and Yiqun Wang for their many edits and suggestions.\nThis electronic textbook was created and published using R, RStudio, the Quarto, GitHub, and Netlify.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Goals\nWe’re going to start the introduction by writing down some basic goals that underlie the construction and content of this book. We’re writing this for you, the reader, but also to hold ourselves accountable as we write. So, feel free to read if you are interested or skip ahead if you aren’t.\nThe goals of this book are:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#goals",
    "href": "chapters/introduction.html#goals",
    "title": "Introduction",
    "section": "",
    "text": "To teach you how to use R and RStudio as tools for applied epidemiology.1 Our goal is not to teach you to be a computer scientist or an advanced R programmer. Therefore, some readers who are experienced programmers may catch some technical inaccuracies regarding what we consider to be the fine points of what R is doing “under the hood.”\nTo make this writing as accessible and practically useful as possible without stripping out all of the complexity that makes doing epidemiology in real life a challenge. In other words, We’re going to try to give you all the tools you need to do epidemiology in “real world” conditions (as opposed to ideal conditions) without providing a whole bunch of extraneous (often theoretical) stuff that detracts from doing. Having said that, we will strive to add links to the other (often theoretical) stuff for readers who are interested.\nTo teach you to accomplish common tasks, rather than teach you to use functions or families of functions. In many R courses and texts, there is a focus on learning all the things a function, or set of related functions, can do. It’s then up to you, the reader, to sift through all of these capabilities and decided which, if any, of the things that can be done will accomplish the tasks that you are actually trying to accomplish. Instead, we will strive to start with the end in mind. What is the task we are actually trying to accomplish? What are some functions/methods we could use to accomplish that task? What are the strengths and limitations of each?\nTo start each concept by showing you the end result and then deconstruct how we arrived at that result, where possible. We find that it is easier for many people to understand new concepts when learning them as a component of a final product.\nTo learn concepts with data instead of (or alongside) mathematical formulas and text descriptions, where possible. We find that it is easier for many people to understand new concepts by seeing them in action.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#text-conventions-used-in-this-book",
    "href": "chapters/introduction.html#text-conventions-used-in-this-book",
    "title": "Introduction",
    "section": "Text conventions used in this book",
    "text": "Text conventions used in this book\n\nWe will hyperlink many keywords or phrases to their glossary entry.\nAdditionally, we may use bold face for a word or phrase that we want to call attention to, but it is not necessarily a keyword or phrase that we want to define in the glossary.\nHighlighted inline code is used to emphasize small sections of R code and program elements such as variable or function names.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#other-reading",
    "href": "chapters/introduction.html#other-reading",
    "title": "Introduction",
    "section": "Other reading",
    "text": "Other reading\nIf you are interested in R4Epi, you may also be interested in:\n\nHands-on Programming with R by Garrett Grolemund. This book is designed to provide a friendly introduction to the R language.\nR for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. This book is designed to teach readers how to do data science with R.\nStatistical Inference via Data Science: A ModernDive into R and the Tidyverse. This book is designed to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians, data journalists, and other researchers would.\nReproducable Research with R and RStudio by Christopher Gandrud. This book gives you tools for data gathering, analysis, and presentation of results so that you can create dynamic and highly reproducible research.\nAdvanced R by Hadley Wickham. This book is designed primarily for R users who want to improve their programming skills and understanding of the language.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "In this case, “tools for applied epidemiology” means (1) understanding epidemiologic concepts; and (2) completing and interpreting epidemiologic analyses.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/contributing/contributing.html",
    "href": "chapters/contributing/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Typos\nOver the years, we have learned so much from our students and colleagues, and we anticipate that there is much more we can learn from you – our readers. Therefore, we welcome and appreciate all constructive contributions to R4Epi!\nThe easiest way for you to contribute is to help us clean up the little typos and grammatical errors that inevitably sneak into the text.\nIf you spot a typo, you can offer a correction directly in GitHub. You will first need to create a free GitHub account: sign-up at github.com. Later in the book, we will cover using GitHub in greater depth in see Using-git-and-Github. Here, we’re just going to walk you through how to fix a typo without much explanation of how GitHub works.\nLet’s say you spot a typo while reading along.\nNext, click the edit button in the toolbar as shown in the screenshot below.\nThe first time you click the icon, you will be taken to the R4Epi repository on GitHub and asked to fork it. For our purposes, you can think of a GitHub repository as being similar to a shared folder on Dropbox or Google Drive.\n“Forking the repository” basically just means “make a copy of the repository” on your GitHub account. In other words, copy all of the files that make up the R4Epi textbook to your GitHub account. Then, you can fix the typos you found in your copy of the files that make up the book instead of directly editing the actual files that make up the book. This is a safeguard to prevent people from accidentally making changes that shouldn’t be made.\nAfter you fork the repository, you will see a text editor on your screen.\nThe text editor will display the contents of the file used to make the chapter you were looking at when you clicked the edit button. In this example, it was a file named contributing.qmd. The .qmd file extension means that the file is a Quarto/file. We will learn more about Quarto files, but for now just know that Quarto/ files can be used to create web pages and other documents that contain a mix of R code, text, and images.\nNext, scroll down through the text until you find the typo and fix it. In this case, line 11 contains the word “typoo”. To fix it, you just need to click in the editor window and begin typing. In this case, you would click next to the word “typoo” and delete the second “o”.\nNow, the only thing left to do is propose your typo fix to the authors. To do so, click the green Commit changes... button on the right side of the screen above the text editor (surrounded with a red box in the screenshot above). When you click it, a new Propose changes box will appear on your screen. Type a brief (i.e., 72 characters or less) summary of the change you made in the Commit message box. There is also an Extended description box where you can add a more detailed description of what you did. In the screenshot below, shows an example commit message and extended description that will make it easy for the author to quickly figure out exactly what changes are being proposed.\nNext, click the Propose changes button. That will take you to another screen where you will be able to create a pull request. This screen is kind of busy, but try not to let it overwhelm you.\nFor now, we will focus on the three different sections of the screen that are highlighted with a red outline. We will start at the bottom and work our way up. The red box that is closest to the bottom of the screenshot shows us that the change that made was on line 11. The word “typoo” (highlighted in red) was replaced with the word “typo” (highlighted in green). The red box in the middle of the screenshot shows us the brief description that was written for our proposed change – “Fix a typo in contributing.qmd”. Finally, the red box closest to the top of the screenshot is surrounding the Create pull request button. You will click it to move on with your pull request.\nAfter doing so, you will get one final chance to amend the description of your proposed changes. If you are happy with the commit message and description, then click the Create pull request button one more time. At this point, your job is done! It is now up to the authors to review the changes you’ve proposed and “pull” them into the file in their repository.\nIn case you are curious, here is what the process looks like on the authors’ end. First, when we open the R4Epi repository page on GitHub, we will see that there is a new pull request.\nWhen we open the pull request, we can see the proposed changes to the file.\nThen, all we have to do is click the Merge pull request button and the fixed file is “pulled in” to replace the file with the typo.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing/contributing.html#typos",
    "href": "chapters/contributing/contributing.html#typos",
    "title": "Contributing",
    "section": "",
    "text": "Note\n\n\n\nForking the R4Epi repository does not cost any money or add any files to your computer.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing/contributing.html#issues",
    "href": "chapters/contributing/contributing.html#issues",
    "title": "Contributing",
    "section": "Issues",
    "text": "Issues\nThere may be times when you see a problem that you don’t know how to fix, but you still want to make the authors aware of. In that case, you can create an issue in the R4Epi repository. To do so, navigate to the issue tracker using this link: https://github.com/brad-cannell/r4epi/issues.\n\n\n\n\n\n\n\n\n\nOnce there, you can check to see if someone has already raised the issue you are concerned about. If not, you can click the green “New issue” button to raise it yourself.\nPlease note that R4Epi uses a Contributor Code of Conduct. By contributing to this book, you agree to abide by its terms.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/contributing/contributing.html#license-information",
    "href": "chapters/contributing/contributing.html#license-information",
    "title": "Contributing",
    "section": "License Information",
    "text": "License Information\nThis book was created by Brad Cannell and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "chapters/about_the_authors/about_the_authors.html",
    "href": "chapters/about_the_authors/about_the_authors.html",
    "title": "About the Authors",
    "section": "",
    "text": "Brad Cannell\nMichael (Brad) Cannell, PhD, MPH\n  Associate Professor\nElder Mistreatment Lead, UTHealth Institute of Aging\nDirector, Research Informatics Core, Cizik Nursing Research Institute\nUTHealth Houston\nMcGovern Medical School\nJoan and Stanford Alexander Division of Geriatric & Palliative Medicine\nwww.bradcannell.com\nDr. Cannell received his PhD in Epidemiology, and Graduate Certificate in Gerontology, in 2013 from the University of Florida. He received his MPH with a concentration in Epidemiology from the University of Louisville in 2009, and his BA in Political Science and Marketing from the University of North Texas in 2005. During his doctoral studies, he was a Graduate Research Assistant for the Florida Office on Disability and Health, an affiliated scholar with the Claude D. Pepper Older Americans Independence Center, and a student-inducted member of the Delta Omega Honorary Society in Public Health. In 2016, Dr. Cannell received a Graduate Certificate in Predictive Analytics from the University of Maryland University College, and a Certificate in Big Data and Social Analytics from the Massachusetts Institute of Technology.\nHe previously held professional staff positions in the Louisville Metro Health Department and the Northern Kentucky Independent District Health Department. He spent three years as a project epidemiologist for the Florida Office on Disability and Health at the University of Florida. He also served as an Environmental Science Officer in the United States Army Reserves from 2009 to 2013.\nDr. Cannell’s research is broadly focused on healthy aging and health-related quality of life. Specifically, he has published research focusing on preservation of physical and cognitive function, living and aging with disability, and understanding and preventing elder mistreatment. Additionally, he has a strong background and training in epidemiologic methods and predictive analytics. He has been principal or co-investigator on multiple trials and observational studies in community and healthcare settings. He is currently the principal investigator on multiple data-driven federally funded projects that utilize technological solutions to public health issues in novel ways.\nContact\nConnect with Dr. Cannell and follow his work.",
    "crumbs": [
      "About the Authors"
    ]
  },
  {
    "objectID": "chapters/about_the_authors/about_the_authors.html#melvin-livingston",
    "href": "chapters/about_the_authors/about_the_authors.html#melvin-livingston",
    "title": "About the Authors",
    "section": "Melvin Livingston",
    "text": "Melvin Livingston\nMelvin (Doug) Livingston, PhD\n  Research Associate Professor\nDepartment of Behavioral, Social, and Health Education Sciences\nEmory University Woodruff Health Sciences Center\nRollins School of Public Health\nDr. Livingston’s Faculty Profile\nDr. Livingston is a methodologist with expertise in the the application of quasi-experimental design principals to the evaluation for both community interventions and state policies. He has particular expertise in time series modeling, mixed effects modeling, econometric methods, and power analysis. As part of his work involving community trials, he has been the statistician on the long term follow-up study of a school based cluster randomized trial in low-income communities with a focus on explaining the etiology of risky alcohol, drug, and sexual behaviors. Additionally, he was the statistician for a longitudinal study examining the etiology of alcohol use among racially diverse and economically disadvantaged urban youth, and co-investigator for a NIAAA- and NIDA-funded trial to prevent alcohol use and alcohol-related problems among youth living in high-risk, low-income communities within the Cherokee Nation. Prevention work at the community level led him to an interest in the impact of state and federal socioeconomic policies on health outcomes. He is a Co-Investigator of a 50-state, 30-year study of effects of state-level economic and education policies on a diverse set of public health outcomes, explicitly examining differential effects across disadvantaged subgroups of the population.\nHis current research interests center around the application of quasi-experimental design and econometric methods to the evaluation of the health effects of state and federal policy.\nContact\nConnect with Dr. Livingston and follow his work.",
    "crumbs": [
      "About the Authors"
    ]
  },
  {
    "objectID": "chapters/installing_r_and_rstudio/installing_r_and_rstudio.html",
    "href": "chapters/installing_r_and_rstudio/installing_r_and_rstudio.html",
    "title": "1  Installing R and RStudio",
    "section": "",
    "text": "1.1 Download and install on a Mac\nBefore we can do any programming with R, we first have to download it to our computer. Fortunately, R is free, easy to install, and runs on all major operating systems (i.e., Mac and Windows). However, R is even easier to use as when we combine it with another program called RStudio. Fortunately, RStudio is also free and will also run on all major operating systems.\nAt this point, you may be wondering what R is, what RStudio is, and how they are related. We will answer those questions in the near future. However, in the interest of keeping things brief and simple, We’re not going to get into them right now. Instead, all you have to worry about is getting the R programming language and the RStudio IDE (IDE is short for integrated development environment) downloaded and installed on your computer. The steps involved are slightly different depending on whether you are using a Mac or a PC (i.e., Windows). Therefore, please feel free to use the table of contents on the right-hand side of the screen to navigate directly to the instructions that you need for your computer.\nStep 1: Regardless of which operating system you are using, please make sure your computer is on, properly functioning, connected to the internet, and has enough space on your hard drive to save R and RStudio.\nStep 2: Navigate to the Comprehensive R Archive Network (CRAN), which is located at https://cran.r-project.org/.\nStep 3: Click on Download R for macOS.\nStep 4: Click on the link for the latest version of R. As you are reading this, the newest version may be different than the version you see in this picture, but the location of the newest version should be roughly in the same place – the middle of the screen under “Latest release:”. After clicking the link, R should start to download to your computer automatically.\nStep 5: Locate the package file you just downloaded and double click it. Unless you’ve changed your download settings, this file will probably be in your “downloads” folder. That is the default location for most web browsers. After you locate the file, just double click it.\nStep 6: A dialogue box will open and ask you to make some decisions about how and where you want to install R on your computer. We typically just click “continue” at every step without changing any of the default options.\nIf R installed properly, you should now see it in your applications folder.\nStep 7: Now, we need to install the RStudio IDE. To do this, navigate to the RStudio desktop download website, which is located at https://posit.co/download/rstudio-desktop/. On that page, click the button to download the latest version of RStudio for your computer. Note that the website may look different that what you see in the screenshot below because websites change over time.\nStep 8: Again, locate the DMG file you just downloaded and double click it. Unless you’ve changed your download settings, this file should be in the same location as the R package file you already downloaded.\nStep 9: A new finder window should automatically pop up that looks like the one you see below. Click on the RStudio icon and drag it into the Applications folder.\nYou should now see RStudio in your Applications folder. Double click the icon to open RStudio.\nIf this warning pops up, just click Open.\nThe RStudio IDE should open and look something like the window you see here. If so, you are good to go! 🎉",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/installing_r_and_rstudio/installing_r_and_rstudio.html#download-and-install-on-a-pc",
    "href": "chapters/installing_r_and_rstudio/installing_r_and_rstudio.html#download-and-install-on-a-pc",
    "title": "1  Installing R and RStudio",
    "section": "1.2 Download and install on a PC",
    "text": "1.2 Download and install on a PC\nStep 2: Navigate to the Comprehensive R Archive Network (CRAN), which is located at https://cran.r-project.org/.\n\n\n\n\n\n\n\n\n\nStep 3: Click on Download R for Windows.\n\n\n\n\n\n\n\n\n\nStep 4: Click on the base link.\n\n\n\n\n\n\n\n\n\nStep 5: Click on the link for the latest version of R. As you are reading this, the newest version may be different than the version you see in this picture, but the location of the newest version should be roughly the same. After clicking, R should start to download to your computer.\n\n\n\n\n\n\n\n\n\nStep 6: Locate the installation file you just downloaded and double click it. Unless you’ve changed your download settings, this file will probably be in your downloads folder. That is the default location for most web browsers.\n\n\n\n\n\n\n\n\n\nStep 7: A dialogue box will open that asks you to make some decisions about how and where you want to install R on your computer. We typically just click “Next” at every step without changing any of the default options.\n\n\n\n\n\n\n\n\n\nIf R installed properly, you should now see it in the Windows start menu.\n\n\n\n\n\n\n\n\n\nStep 8: Now, we need to install the RStudio IDE. To do this, navigate to the RStudio desktop download website, which is located at https://posit.co/download/rstudio-desktop/. On that page, click the button to download the latest version of RStudio for your computer. Note that the website may look different that what you see in the screenshot below because websites change over time.\n\n\n\n\n\n\n\n\n\nStep 9: Again, locate the installation file you just downloaded and double click it. Unless you’ve changed your download settings, this file should be in the same location as the R installation file you already downloaded.\n\n\n\n\n\n\n\n\n\nStep 10: Another dialogue box will open and ask you to make some decisions about how and where you want to install RStudio on your computer. We typically just click “Next” at every step without changing any of the default options.\n\n\n\n\n\n\n\n\n\nWhen RStudio is finished installing, you should see RStudio in the Windows start menu. Click the icon to open RStudio.\n\n\n\n\n\n\n\n\n\nThe RStudio IDE should open and look something like the window you see here. If so, you are good to go! 🎉",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/what_is_r/what_is_r.html",
    "href": "chapters/what_is_r/what_is_r.html",
    "title": "2  What is R?",
    "section": "",
    "text": "2.1 What is data?\nAt this point in the book, you should have installed R and RStudio on your computer, but you may be thinking to yourself, “I don’t even know what R is.” Well, in this chapter you’ll find out. We’ll start with an overview of the R language, and then briefly touch on its capabilities and uses. You’ll also see a complete R program and some complete documents generated by R programs. In this book you’ll learn how to create similar programs and documents, and by the end of the book you’ll be able to write your own R programs and present your results in the form of an issue brief written for general audiences who may or may not have public health expertise. But, before we discuss R let’s discuss something even more basic – data. Here’s a question for you: What is data?\nData is information about objects (e.g., people, places, schools) and observable phenomenon (e.g., weather, temperatures, and disease symptoms) that is recorded and stored somehow as a collection of symbols, numbers, and letters. So, data is just information that has been “written” down.\nHere we have a table, which is a common way of organizing data. In R, we will typically refer to these tables as data frames.\nEach box in a data frame is called a cell.\nMoving from left to right across the data frame are columns. Columns are also sometimes referred to as variables. In this book, we will often use the terms columns and variables interchangeably. Each column in a data frame has one, and only one, type. For now, know that the type tells us what kind of data is contained in a column and what we can do with that data. You may have already noticed that 3 of the columns in the table we’ve been looking at contain numbers and 1 of the columns contains words. These columns will have different types in R and we can do different things with them based on their type. For example, we could ask R to tell us what the average value of the numbers in the height column are, but it wouldn’t make sense to ask R to tell us the average value of the words in the Gender column. We will talk more about many of the different column types exist in R later in this book.\nThe information contained in the first cell of each column is called the column name (or variable) name.\nR gives us a lot of flexibility in terms of what we can name our columns, but there are a few rules.\nMoving from top to bottom across the table are rows, which are sometimes referred to as records.\nFinally, the contents of each cell are called values.\nWe should now be up to speed on some basic terminology used by R, as well as other analytic, database, and spreadsheet programs. These terms will be used repeatedly throughout the book.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "chapters/what_is_r/what_is_r.html#what-is-data",
    "href": "chapters/what_is_r/what_is_r.html#what-is-data",
    "title": "2  What is R?",
    "section": "",
    "text": "Column names can contain letters, numbers and the dot (.) or underscore (_) characters.\n\nAdditionally, they can begin with a letter or a dot – as long as the dot is not followed by a number. So, a name like “.2cats” is not allowed.\n\nFinally, R has some reserved words that you are not allowed to use for column names. These include: “if”, “else”, “repeat”, “while”, “function”, “for”, “in”, “next”, and “break”.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "chapters/what_is_r/what_is_r.html#what-is-r",
    "href": "chapters/what_is_r/what_is_r.html#what-is-r",
    "title": "2  What is R?",
    "section": "2.2 What is R?",
    "text": "2.2 What is R?\n\n\n\n\n\n\n\n\n\nSo, what is R? Well, R is an open source statistical programming language that was created in the 1990’s specifically for data analysis. We will talk more about what open source means later, but for now, just think of R as an easy (relatively 😂) way to ask our computer to do math and statistics for us. More specifically, by the end of this book we will be able to independently use R to transfer data, manage data, analyze data, and present the results of our analysis. Let’s quickly take a closer look at each of these.\n\n\n\n\n\n\n\n\n\n\n2.2.1 Transferring data\n\n\n\n\n\n\n\n\n\nSo, what do we mean by “transfer data”? Well, individuals and organizations store their data using different computer programs that use different file types. Some common examples that we may come across in epidemiology are database files, spreadsheets, raw data files, and SAS data sets. No matter how the data is stored, we can’t do anything with it until we can get it into R, in a form that R can use, and in a location that R can access.\n\n\n2.2.2 Managing data\n\n\n\n\n\n\n\n\n\nThis isn’t very specific, but managing data is all the things we may have to do to our data to get it ready for analysis. Some people also refer to this process as “data wrangling” or “data munging.” Some specific examples of data management tasks include:\n\nValidating and cleaning data. In other words, dealing with potential errors in the data.\n\nSubsetting data – using only some of the columns or some of the rows.\n\nCreating new variables. For example, we might want to create a new BMI variable from existing height and weight variables.\n\nCombining data frames. For example, we might want to combine a data frame containing sociodemographic data about study participants with a data frame containing intervention outcomes data about those same participants.\n\nWe may sometimes hear people refer to the 80/20 rule about data management. This “rule” says that in a typical data analysis project, roughly 80% of our time will be spent on data management, while only 20% will be spent on the analysis itself. We can’t provide you with any empirical evidence (i.e., data) to back this claim up. But as people who have been involved in many projects that involve the collection and analysis of data, we can tell you anecdotally that this ”rule” is probably pretty close to being accurate in most cases.\nAdditionally, it’s been our experience that most students of epidemiology are required to take one or more courses that emphasize methods for analyzing data; however, almost none of them have taken a course that emphasizes data management.\nTherefore, because data management is such a large component of most projects that involve the collection and analysis of data, and because most readers will have already been exposed to data analysis to a much greater extent than data management, this book will start by heavily emphasizing the latter.\n\n\n2.2.3 Analyzing data\n\n\n\n\n\n\n\n\n\nAs discussed above, this is probably the capability most people most closely associate with R, and there is no doubt that R is a powerful tool for analyzing data. However, in this book we won’t go beyond using R to calculate basic descriptive statistics. For our purposes, descriptive statistics include:\n\nMeasures of central tendency. For example, mean, median, and mode.\n\nMeasures of dispersion. For example, variance and standard error.\n\nMeasures for describing categorical variables. For example, counts and percentages.\n\nDescribing data using graphs and charts. With R, we can describe our data using beautiful and informative graphs.\n\n\n\n2.2.4 Presenting data\n\n\n\n\n\n\n\n\n\nAnd finally, the ultimate goal is typically to present our findings in some form or another. For example, a report, a website, or a journal article. With R we can present our results in many different formats with relative ease. In fact, this is one of our favorite things about R and RStudio. In this book we will learn how to publish our text, tabular, or graphical results in many different formats including Microsoft Word documents, html files that can be viewed in web browsers, and pdf documents. Let’s take a look at some examples.\n\nMicrosoft Word documents: Click here to view an example Word document created with R and the officedown package.\nPDF documents: Click here to view a gallery of documents, including PDF.\nHTML files: HTML (HyperText Markup Language) is the standard format for web pages. R can create HTML files that can be shared via email or published online for others to view in their browser. Click here to browse a gallery of interactive dashboards built with R.\nWeb applications: R can even be used to build full-featured web applications. Click here to explore examples created with the Shiny package.\n\nNow that we’ve explored what R is and how it can be used in public health and the health sciences, it’s time to start learning how to actually use it. In the next chapter, Navigating the RStudio Interface, we’ll begin by exploring the RStudio IDE and briefly introduce some of the basic building blocks of R code.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is R?</span>"
    ]
  },
  {
    "objectID": "chapters/navigating_rstudio/navigating_rstudio.html",
    "href": "chapters/navigating_rstudio/navigating_rstudio.html",
    "title": "3  Navigating the RStudio Interface",
    "section": "",
    "text": "3.1 The console pane\nIf you followed along with the previous chapters, you have R and RStudio installed on your computer and you have some idea of what R and RStudio are. At this point, it can be common for people to open RStudio and get totally overwhelmed. “What am I looking at?” ”What do I click first?” “Where do I even start?” Don’t worry if these, or similar, thoughts have crossed your mind. You are in good company and we will start to clear some of them up in this chapter.\nWhen we load RStudio, we should see a screen that looks very similar to Figure 3.1 below. There, we see three panes, and each pane has multiple tabs.\nThe first pane we are going to talk about is the console/terminal/background jobs pane.\nFigure 3.2: The R Console.\nIt’s called the “console/terminal/background jobs” pane because it has three tabs we can click on by default: “console”, “terminal”, and “background jobs”. However, we will refer to this pane as the “console pane” and will mostly ignore the terminal and background jobs tabs for now. We aren’t ignoring them because they aren’t useful; instead, we are ignoring them because using them isn’t essential for anything we will discuss in this chapter, and we want to keep things as simple as possible for now.\nThe console is the most basic way to interact with R. We can type a command to R into the console prompt (the prompt looks like “&gt;”) and R will respond to what we type. For example, below we typed “1 + 1,” pressed the return/enter key, and the R console returned the sum of the numbers 1 and 1.\nFigure 3.3: Doing some addition in the R console.\nThe number 1 we see in brackets before the 2 (i.e., [1]) is telling us that this line of results starts with the first result. That fact is obvious here because there is only one result. So, let’s look at a result that spans multiple lines to make this idea clearer.\nFigure 3.4: Demonstrating a function that returns multiple results.\nIn Figure 3.4 we see examples of a couple of new concepts that are worth discussing.\nFirst, as promised, we have more than one line of results (or output). The first line of results starts with a 1 in brackets (i.e., [1]), which indicates that this line of results starts with the first result. In this case, the first result is the number 2. The second line of results starts with a 33 in brackets (i.e., [33]), which indicates that this line of results starts with the thirty-third result. In this case, the thirty-third result is the number 66. If we count the numbers in the first line, there should be 32 – results 1 through 32. We also want to make it clear that [1] and [33] are NOT results themselves. They are just helping us count the number of results per line.\nThe second new thing that you may have noticed in Figure 3.4 is our use of a function. Functions are a BIG DEAL in R. So much so that R is called a functional language. We don’t really need to know all the details of what that means; however, we should know that, in general, everything we do in R we will do with a function. By contrast, everything we create in R will be an object. If we wanted to make an analogy between the R language and the English language, we could think of functions as verbs – they do things – and objects as nouns – they are things. This distinction likely seems abstract and confusing at the moment, but we will make it more concrete soon.\nMost functions in R begin with the function name followed by parentheses. For example, seq(), sum(), and mean().\nQuestion: What is the name of the function we used in the example above?\nAnswer: We used the seq() function – short for sequence - in the example above.\nYou may notice that there are three pairs of words, equal symbols, and numbers that are separated by commas inside the seq() function. They are, from = 2, to = 100, and by = 2. The words from, to, and by are all arguments to the seq() function. We will learn more about functions and arguments later. For now, just know that arguments give functions the information they need to give us the result we want.\nIn this case, the seq() function returns a sequence of numbers. But first, we had to give it information about where that sequence should start, where it should end, and how many steps should be in the middle. Above, the sequence began with the value we passed to the from argument (i.e., 2), it ended with the value we passed to the to argument (i.e., 100), and it increased at each step by the number we passed to the by argument (i.e., 2). So, 2, 4, 6, 8 … 100.\nWhether you realize it or not, we’ve covered some important programming terms while discussing the seq() function above. Before we move on to discussing RStudio’s other panes, let’s quickly review and reinforce a few of terms we will use repeatedly in this book.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the RStudio Interface</span>"
    ]
  },
  {
    "objectID": "chapters/navigating_rstudio/navigating_rstudio.html#the-console-pane",
    "href": "chapters/navigating_rstudio/navigating_rstudio.html#the-console-pane",
    "title": "3  Navigating the RStudio Interface",
    "section": "",
    "text": "Arguments: Arguments always live inside the parentheses of R functions and receive information the function needs to generate the result we want.\nPass: In programming lingo, we pass a value to a function argument. For example, in the function call seq(from = 2, to = 100, by = 2) we could say that we passed a value of 2 to the from argument, we passed a value of 100 to the to argument, and we passed a value of 2 to the by argument.\nReturn: Instead of saying, “the seq() function gives us a sequence of numbers…” we say, “the seq() function returns a sequence of numbers…” In programming lingo, functions return one or more results.\n\n\n\n\n\n\n\nNote\n\n\n\nThe seq() function isn’t particularly important or noteworthy. We essentially chose it at random to illustrate some key points. However, arguments, passing values, and return values are extremely important concepts and we will return to them many times.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the RStudio Interface</span>"
    ]
  },
  {
    "objectID": "chapters/navigating_rstudio/navigating_rstudio.html#the-environment-pane",
    "href": "chapters/navigating_rstudio/navigating_rstudio.html#the-environment-pane",
    "title": "3  Navigating the RStudio Interface",
    "section": "3.2 The environment pane",
    "text": "3.2 The environment pane\nThe second pane we are going to talk about is the environment/history/connections pane in Figure 3.5. However, we will mostly refer to it as the environment pane and we will mostly ignore the history and connections tab. We aren’t ignoring them because they aren’t useful; rather, we are ignoring them because using them isn’t essential for anything we will discuss anytime soon, and we want to keep things as simple as possible.\n\n\n\n\n\n\n\n\nFigure 3.5: The environment pane\n\n\n\n\n\nThe Environment pane shows you all the objects that R can currently use for data management or analysis. In this picture, Figure 3.5 our environment is empty. Let’s create an object and add it to our environment.\n\n\n\n\n\n\n\n\nFigure 3.6: The vector x in the global environment.\n\n\n\n\n\nHere we see that we created a new object called x, which now appears in our Global Environment. Figure 3.6 This gives us another great opportunity to discuss some new concepts.\nFirst, we created the x object in the console by assigning the value 2 to the letter x. We did this by typing “x” followed by a less than symbol (&lt;), a dash symbol (-), and the number 2. R is kind of unique in this way. We have never seen another programming language (although I’m sure they are out there) that uses &lt;- to assign values to variables. By the way, &lt;- is called the assignment operator (or assignment arrow), and ”assign” here means “make x contain 2” or “put 2 inside x.”\nIn many other languages you would write that as x = 2. But, for whatever reason, in R it is &lt;-. Unfortunately, &lt;- is more awkward to type than =. Fortunately, RStudio gives us a keyboard shortcut to make it easier. To type the assignment operator in RStudio, just hold down Option + - (dash key) on a Mac or Alt + - (dash key) on a PC and RStudio will insert &lt;- complete with spaces on either side of the arrow. This may still seem awkward at first, but you will get used to it.\n\n\n\n\n\n\nNote\n\n\n\nA note about using the letter “x”: By convention, the letter “x” is a widely used variable name. You will see it used a lot in example documents and online. However, there is nothing special about the letter x. We could have just as easily used any other letter (a &lt;- 2), word (variable &lt;- 2), or descriptive name (my_favorite_number &lt;- 2) that is allowed by R.\n\n\nSecond, you can see that our Global Environment now includes the object x, which has a value of 2. In this case, we would say that x is a numeric vector of length 1 (i.e., it has one value stored in it). We will talk more about vectors and vector types soon. For now, just notice that objects that you can manipulate or analyze in R will appear in your Global Environment.\n\n\n\n\n\n\nWarning\n\n\n\nR is a case-sensitive language. That means that uppercase x (X) and lowercase x (x) are different things to R. So, if we assign 2 to lower case x (x &lt;- 2), and then later ask R to tell us what number we stored in uppercase X, we will get an error (Error: object 'X' not found).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the RStudio Interface</span>"
    ]
  },
  {
    "objectID": "chapters/navigating_rstudio/navigating_rstudio.html#the-files-pane",
    "href": "chapters/navigating_rstudio/navigating_rstudio.html#the-files-pane",
    "title": "3  Navigating the RStudio Interface",
    "section": "3.3 The files pane",
    "text": "3.3 The files pane\nNext, let’s talk about the Files/Plots/Packages/Help/Viewer pane (that’s a mouthful). Figure 3.7\n\n\n\n\n\n\n\n\nFigure 3.7: The Files/Plots/Packages/Help/Viewer pane.\n\n\n\n\n\nAgain, some of these tabs are more applicable for us than others. For us, the files tab and the help tab will probably be the most useful. You can think of the files tab as a mini Finder window (for Mac) or a mini File Explorer window (for PC). The help tab is also extremely useful once you get acclimated to it.\n\n\n\n\n\n\n\n\nFigure 3.8: The help tab.\n\n\n\n\n\nFor example, in the screenshot above Figure 3.8 we typed the seq into the search bar. The help pane then shows us a page of documentation for the seq() function. The documentation includes a brief description of what the function does, outlines all the arguments the seq() function recognizes, and, if you scroll down, gives examples of using the seq() function. Admittedly, this help documentation can seem a little like reading Greek (assuming you don’t speak Greek) at first. But, you will get more comfortable using it with practice. We hated the help documentation when we were learning R. Now, we use it all the time.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the RStudio Interface</span>"
    ]
  },
  {
    "objectID": "chapters/navigating_rstudio/navigating_rstudio.html#the-source-pane",
    "href": "chapters/navigating_rstudio/navigating_rstudio.html#the-source-pane",
    "title": "3  Navigating the RStudio Interface",
    "section": "3.4 The source pane",
    "text": "3.4 The source pane\nThere is actually a fourth pane available in RStudio. If you click on the icon shown below you will get the following dropdown box with a list of files you can create. Figure 3.9\n\n\n\n\n\n\n\n\nFigure 3.9: Click the new source file icon.\n\n\n\n\n\nIf you click any of these options, a new pane will appear. We will arbitrarily pick the first option – R Script.\n\n\n\n\n\n\n\n\nFigure 3.10: New source file options.\n\n\n\n\n\nWhen we do, a new pane appears. It’s called the source pane. In this case, the source pane contains an untitled R Script. We won’t get into the details now because we don’t want to overwhelm you, but soon you will do the majority of your R programming in the source pane.\n\n\n\n\n\n\n\n\nFigure 3.11: A blank R script in the source pane.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the RStudio Interface</span>"
    ]
  },
  {
    "objectID": "chapters/navigating_rstudio/navigating_rstudio.html#rstudio-preferences",
    "href": "chapters/navigating_rstudio/navigating_rstudio.html#rstudio-preferences",
    "title": "3  Navigating the RStudio Interface",
    "section": "3.5 RStudio preferences",
    "text": "3.5 RStudio preferences\nFinally, We’re going to recommend that you change a few settings in RStudio before we move on. Start by clicking Tools, and then Global Options in RStudio’s menu bar, which probably runs horizontally across the top of your computer’s screen.\n\n\n\n\n\n\n\n\nFigure 3.12: Select the preferences menu on Mac.\n\n\n\n\n\nIn the General tab, we recommend turning off the Restore .Rdata into workspace at startup option. We also recommend setting the Save workspace .Rdata on exit dropdown to Never. Finally, we recommend turning off the Always save history (even when not saving .Rdata) option.\n\n\n\n\n\n\n\n\nFigure 3.13: General options tab.\n\n\n\n\n\nWe change our editor theme to Twilight in the Appearance tab. We aren’t necessarily recommending that you change your theme – this is entirely personal preference – we’re just letting you know why our screenshots will look different from here on out.\n\n\n\n\n\n\n\n\nFigure 3.14: Appearance tab.\n\n\n\n\n\nIt’s likely that you still have lots of questions at this point. That’s totally natural. However, we hope you now feel like you have some idea of what you are looking at when you open RStudio. Most of you will naturally get more comfortable with RStudio as we move through the book. For those of you who want more resources now, here are some suggestions.\n\nRStudio IDE cheatsheet\nModernDive: What are R and RStudio?",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the RStudio Interface</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html",
    "href": "chapters/speaking_r/speaking_r.html",
    "title": "4  Speaking R’s Language",
    "section": "",
    "text": "4.1 R is a language\nIt has been our experience that students often come into statistical programming courses thinking they will be heavy in math or statistics. In reality, our R courses are probably much closer to a foreign language course. There is no doubt that we need a foundational understanding of math and statistics to understand the results we get from R, but R will take care of most of the complicated stuff for us. We only need to learn how to ask R to do what we want it to do. To some extent, this entire book is about learning to communicate with R, but in this chapter we will briefly introduce the R programming language from the 30,000-foot level.\nIn the same way that many people use the English language to communicate with each other, we will use the R programming language to communicate with R. Just like the English language, the R language comes complete with its own structure and vocabulary. Unfortunately, just like the English language, it also includes some weird exceptions and occasional miscommunications. We’ve already seen a couple examples of commands written to R in the R programming language. Specifically:\n# Store the value 2 in the variable x\nx &lt;- 2\n# Print the contents of x to the screen\nx\n\n[1] 2\nand\n# Print an example number sequence to the screen\nseq(from = 2, to = 100, by = 2)\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#r-is-a-language",
    "href": "chapters/speaking_r/speaking_r.html#r-is-a-language",
    "title": "4  Speaking R’s Language",
    "section": "",
    "text": "Note\n\n\n\nThe gray boxes you see above are called R code chunks and we created them (and this entire book) using something called Quarto files. Can you believe that you can write an entire book with R and RStudio? How cool is that? You will learn to use Quarto files later in this book. Quarto is great because it allows you to mix R code with narrative text and multimedia content as we’ve done throughout the page you’re currently looking at. This makes it really easy for us to add context and aesthetic appeal to our results.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#the-r-interpreter",
    "href": "chapters/speaking_r/speaking_r.html#the-r-interpreter",
    "title": "4  Speaking R’s Language",
    "section": "4.2 The R interpreter",
    "text": "4.2 The R interpreter\nQuestion: We keep talking about “speaking” to R, but when we speak to R using the R language, who are we actually speaking to?\nWell, we are speaking to something called the R interpreter. The R interpreter takes the commands we’ve written in the R language, sends them to our computer to do the actual work (e.g., get the mean of a set of numbers), and then translates the results of that work back to us in a form that we humans can understand (e.g., the mean is 25.5). At this stage, one of the key concepts for us to understand about the R language is that it is extremely literal! Understanding the literal nature of R is important because it will be the underlying cause of a lot of the errors in our R code.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#errors",
    "href": "chapters/speaking_r/speaking_r.html#errors",
    "title": "4  Speaking R’s Language",
    "section": "4.3 Errors",
    "text": "4.3 Errors\nIt’s inevitable: errors will happen in your R code. Even experienced programmers who have been working with R for many years get errors when they write code. The goal of this section is to help us begin to understand why errors happen, and to give us a shared language for talking about them.\nSo, what exactly do we mean when we say that the R interpreter is extremely literal? In the previous lesson, we learned that R is a case-sensitive language. That means that uppercase X and lowercase x are treated as two completely different objects.\nFor example, if we assign the value 2 to lowercase x using x &lt;- 2, and then later ask R to show us the contents of uppercase X, we’ll get an error (Error: object 'X' not found):\n\nx &lt;- 2\nX\n\nError: object 'X' not found\n\n\nSpecifically, this is an example of a logic error. Meaning, R understands what we are asking it to do – we want it to print the contents of the uppercase X object to the screen. However, it can’t complete our request because we are asking it to do something that doesn’t logically make sense – print the contents of a thing that doesn’t exist. Remember, R is literal and it will not try to guess that we actually meant to ask it to print the contents of lowercase x.\nAnother general type of error is known as a syntax error. In programming languages, syntax refers to the rules of the language. We can sort of think of syntax as the grammar of the language. In English, we could say something like, “giving dog water drink.” This sentence is grammatically incorrect; however, many people would roughly be able to figure out what’s being asked based on their life experience and knowledge of the situational context. The R interpreter, as awesome as it is, would not be able to make an assumption about what we want it to do. In this case, the R interpreter would say, “I don’t know what you’re asking me to do.” When the R interpreter says, “I don’t know what you’re asking me to do,” we’ve made a syntax error.\nThroughout the rest of the book, we will try to point out situations where R programmers often encounter errors and how we may be able to address them. The remainder of this chapter will discuss some key components of R’s syntax and the data structures (i.e., ways of storing data) that the R syntax interacts with.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#functions",
    "href": "chapters/speaking_r/speaking_r.html#functions",
    "title": "4  Speaking R’s Language",
    "section": "4.4 Functions",
    "text": "4.4 Functions\nR is a functional programming language, which simply means that functions play a central role in the R language. But what are functions? Well, factories are a common analogy used to represent functions. In this analogy, arguments are raw material inputs that go into the factory. For example, steel and rubber. The function is the factory where all the work takes place – converting raw materials into the desired output. Finally, the factory output represents the returned results. In this case, bicycles.\n\n\n\n\n\nA factory making bicycles.\n\n\n\n\nTo make this concept more concrete, in the Navigating RStudio chapter we used the seq() function as a factory. Specifically, we wrote seq(from = 2, to = 100, by = 2). The inputs (arguments) were from, to, and by. The output (returned result) was a set of numbers that went from 2 to 100 by 2’s. Most functions, like the seq() function, will be a word or word part followed by parentheses. Other examples are the sum() function for addition and the mean() function to calculate the average value of a set of numbers.\n\n\n\n\n\nA function factory making numbers.\n\n\n\n\n\n4.4.1 Passing values to function arguments\nWhen we supply a value to a function argument, that is called “passing” a value to the argument. Let’s take another look at the sequence function we previously wrote and use it to help us with this discussion.\n\n# Create a sequence of numbers beginning at 2 and ending at 100, incremented by 2.\nseq(from = 2, to = 100, by = 2)\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n\nIn the code above, we passed the value 2 to the from argument, we passed the value 100 to the to argument, and we passed the value 2 to the by argument. How do we know we passed the value 2 to the from argument? We know because we wrote from = 2. To R, this means “pass the value 2 to the from argument,” and it is an example of passing a value by name. Alternatively, we could have also gotten the same result if we had passed the same values to the seq() function by position. What does that mean? We’ll explain, but first take a look at the following R code.\n\n# Create a sequence of numbers beginning at 2 and ending at 100, incremented by 2.\nseq(2, 100, 2)\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n\nHow is code different from the code chunk before it? You got it! We didn’t explicitly write the names of the function arguments inside of the seq() function. So, how did we get the same results? We got the same results because R allows us to pass values to function arguments by name or by position. When we pass values to a function by position, R will pass the first input value to the first function argument, the second input value to the second function argument, the third input value to the third function argument, and so on.\nBut how do we know what the first, second, and third arguments to a function are? Do you remember our discussion about RStudio’s [help tab][The files pane] in the previous chapter? There, we saw the documentation for the seq() function.\n\n\n\n\n\nThe help tab.\n\n\n\n\nIn the “Usage” section of the documentation for the seq() function, we can see that all of the arguments that the seq() function accepts. These documentation files are a little cryptic until you get used to them but look directly underneath the part that says “## Default S3 method.” There, it tells us that the seq() function understands the from, to, by, length.out, along.with, and ... arguments. The from argument is first argument to the seq() function because it is listed there first, the to argument is second argument to the seq() function because it is listed there second, and so on. It is really that simple. Therefore, when we type seq(2, 100, 2), R automatically translates it to seq(from = 2, to = 100, by = 2). And this is called passing values to function arguments by position.\n\n\n\n\n\n\nNote\n\n\n\nAs an aside, we can view the documentation for any function by typing ?function name into the R console and then pressing the enter/return key. For example, we can type ?seq to view the documentation for the seq() function.\n\n\nPassing values to our functions by position has the benefit of making our code more compact, we don’t have to write out all the function names. But, as you might have already guessed, passing values to our functions by position also has some potential risks. First, it makes our code harder to read. If we give our code to someone who has never used the seq() function before, they will have to guess (or look up) what purpose 2, 100, and 2 serve. When we pass the values to the function by name, their purpose is typically easier to figure out even if we’ve never used a particular function before. The second, and potentially more important, risk is that we may accidentally pass a value to a different argument than the one we intended. For example, what if we mistakenly think the order of the arguments to the seq() function is from. by, to? In that case, we might write the following code:\n\n# Create a sequence of numbers beginning at 2 and ending at 100, incremented by 2.\nseq(2, 2, 100)\n\n[1] 2\n\n\nNotice that R still gives us a result, but it isn’t the result we want! What happened? Well, we passed the values 2, 2, and 100 to the seq() function by position, which R translated to seq(from = 2, to = 2, by = 100) because from is the first argument in the seq() function, to is the second argument in the seq() function, and by is the third argument in the seq() function.\nQuick review: is this an example of a syntax error or a logic error?\nThis is a logic error. We used perfectly valid R syntax in the code above, but we mistakenly asked R to do something different than we actually wanted it to do. In this simple example, it’s easy to see that this result is very different than what we were expecting and try to figure out what we did wrong. But that won’t always be the case. Therefore, we need to be really careful when passing values to function arguments by position.\nOne final note on passing values to functions. When we pass values to R functions by name, we can pass them in any order we want. For example:\n\n# Create a sequence of numbers beginning at 2 and ending at 100, incremented by 2.\nseq(from = 2, to = 100, by = 2)\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n\nand\n\n# Create a sequence of numbers beginning at 2 and ending at 100, incremented by 2.\nseq(to = 100, by = 2, from = 2)\n\n [1]   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38\n[20]  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74  76\n[39]  78  80  82  84  86  88  90  92  94  96  98 100\n\n\nreturn the exact same values. Why? Because we explicitly told R which argument to pass each value to by name. Of course, just because we can do something doesn’t mean we should do it. We really shouldn’t rearrange argument order like this unless there is a good reason.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#objects",
    "href": "chapters/speaking_r/speaking_r.html#objects",
    "title": "4  Speaking R’s Language",
    "section": "4.5 Objects",
    "text": "4.5 Objects\nIn addition to functions, the R programming language also includes objects. In the Navigating RStudio chapter we created an object called x with a value of 2 using the x &lt;- 2 R code. In general, you can think of objects as anything that lives in your R global environment. Objects may be single variables (also called vectors in R) or entire data sets (also called data frames in R).\nObjects can be a confusing concept at first. We think it’s because it is hard to precisely define exactly what an object is. We’ll say two things about this. First, you’re probably overthinking it (because we’ve overthought it too). When we use R, we create and save stuff. We have to call that stuff something in order to talk about it or write books about it. Somebody decided we would call that stuff “objects.” The second thing we’ll say is that this becomes much less abstract when we finally get to a place where you can really get your hands dirty doing some R programming.\n\n\n\n\n\nCreating the x object.\n\n\n\n\nSometimes it can be useful to relate the R language to English grammar. That is, when you are writing R code you can roughly think of functions as verbs and objects as nouns. Just like nouns are things in the English language, and verbs do things in the English language, objects are things and functions do things in the R language.\nSo, in the x &lt;- 2 command x is the object and &lt;- is the function. “Wait! Didn’t you just tell us that functions will be a word followed by parentheses?” Fair question. Technically, we said, “Most functions will be a word, or word part, followed by parentheses.” Just like English, R has exceptions. All operators in R are also functions. Operators are symbols like +, -, =, and &lt;-. There are many more operators, but you will notice that they all do things. In this case, they add, subtract, and assign values to objects.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#comments",
    "href": "chapters/speaking_r/speaking_r.html#comments",
    "title": "4  Speaking R’s Language",
    "section": "4.6 Comments",
    "text": "4.6 Comments\nAnd finally, there are comments. If our R code is a conversation we are having with the R interpreter, then comments are your inner thoughts taking place during the conversation. Comments don’t actually mean anything to R, but they will be extremely important for you. You actually already saw a couple examples of comments above.\n\n# Store the value 2 in the variable x\nx &lt;- 2\n# Print the contents of x to the screen\nx\n\n[1] 2\n\n\nIn this code chunk, “# Store the value 2 in the variable x” and “# Print the contents of x to the screen” are both examples of comments. Notice that they both start with the pound or hash sign (#). The R interpreter will ignore anything on the current line that comes after the hash sign. A carriage return (new line) ends the comment. However, comments don’t have to be written on their own line. They can also be written on the same line as R code as long as put them after the R code, like this:\n\nx &lt;- 2 # Store the value 2 in the variable x\nx      # Print the contents of x to the screen\n\n[1] 2\n\n\nMost beginning R programmers underestimate the importance of comments. In the silly little examples above, the comments are not that useful. However, comments will become extremely important as you begin writing more complex programs. When working on projects, you will often need to share your programs with others. Reading R code without any context is really challenging – even for experienced R programmers. Additionally, even if your collaborators can surmise what your R code is doing, they may have no idea why you are doing it. Therefore, your comments should tell others what your code does (if it isn’t completely obvious), and more importantly, what your code is trying to accomplish. Even if you aren’t sharing your code with others, you may need to come back and revise or reuse your code months or years down the line. You may be shocked at how foreign the code you wrote will seem months or years after you wrote it. Therefore, comments are not just important for others, they are also important for future you!\n\n\n\n\n\n\nNote\n\n\n\nRStudio has a handy little keyboard shortcut for creating comments. On a Mac, type shift + command + C. On Windows, Shift + Ctrl + C.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease put a space in between the pound/hash sign and the rest of your text when writing comments. For example, # here is my comment instead of #here is my comment. It just makes the comment easier to read.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#packages",
    "href": "chapters/speaking_r/speaking_r.html#packages",
    "title": "4  Speaking R’s Language",
    "section": "4.7 Packages",
    "text": "4.7 Packages\nIn addition to being a functional programming language, R is also a type of programming language called an open source programming language. For our purposes, this has two big advantages. First, it means that R is FREE! Second, it means that smart people all around the world get to develop new packages for the R language that can do cutting edge and/or very niche things.\nThat second advantage is probably really confusing if this is not a concept you are already familiar with. For example, when you install Microsoft Word on your computer all the code that makes that program work is owned and Maintained by the Microsoft corporation. If you need Word to do something that it doesn’t currently do, your only option is to make a feature request on Microsoft’s website. Microsoft may or may not every get around to fulfilling that request.\nR works a little differently. When you downloaded R from the CRAN website, you actually downloaded something called Base R. Base R is maintained by the R Core Team. However, anybody – even you – can write your own code (called packages) that add new functions to the R syntax. Like all functions, these new functions allow you to do things that you can’t do (or can’t do as easily) with Base R.\nAn analogy that we really like here is used by Ismay and Kim in ModernDive.\n\nA good analogy for R packages is they are like apps you can download onto a mobile phone. So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play.1\n\nSo, when you get a new smart phone it comes with apps for making phone calls, checking email, and sending text messages. But, what if you want to listen to music on Spotify? You may or may not be able to do that through your phone’s web browser, but it’s way more convenient and powerful to download and install the Spotify app.\nIn this course, we will make extensive use of packages developed by people and teams outside of the R Core Team. In particular, we will use a number of related packages that are collectively known as the Tidyverse. One of the most popular packages in the tidyverse collection (and one of the most popular R packages overall) is called the dplyr package for data management.\nIn the same way that you have to download and install Spotify on your mobile phone before you can use it, you have to download and install new R packages on your computer before you can use the functions they contain. Fortunately, R makes this really easy. For most packages, all you have to do is run the install.packages() function in the R console. For example, here is how you would install the dplyr package.\n\n# Make sure you remember to wrap the name of the package in single or double quotes.\ninstall.packages(\"dplyr\")\n\nOver time, you will download and install a lot of different packages. All those packages with all of those new functions start to create a lot of overhead. Therefore, R doesn’t keep them loaded and available for use at all times. Instead, every time you open RStudio, you will have to explicitly tell R which packages you want to use. So, when you close RStudio and open it again, the only functions that you will be able to use are Base R functions. If you want to use functions from any other package (e.g., dplyr) you will have to tell R that you want to do so using the library() function.\n\n# No quotes needed here\nlibrary(dplyr)\n\nTechnically, loading the package with the library() function is not the only way to use a function from a package you’ve downloaded. For example, the dplyr package contains a function called filter() that helps us keep or drop certain rows in a data frame. To use this function, we have to first download the dplyr package. Then we can use the filter function in one of two different ways.\n\nlibrary(dplyr)\nfilter(states_data, state == \"Texas\") # Keeps only the rows from Texas\n\nThe first way you already saw above. Load all the functions contained in the dplyr package using the library() function. Then use that function just like any other Base R function.\nThe second way is something called the double colon syntax. To use the double colon syntax, you type the package name, two colons, and the name of the function you want to use from the package. Here is an example of the double colon syntax.\n\ndplyr::filter(states_data, state == \"Texas\") # Keeps only the rows from Texas\n\nMost of the time you will load packages using the library() function. However, we wanted to show you the double colon syntax because you may come across it when you are reading R documentation and because there are times when it makes sense to use this syntax.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/speaking_r/speaking_r.html#programming-style",
    "href": "chapters/speaking_r/speaking_r.html#programming-style",
    "title": "4  Speaking R’s Language",
    "section": "4.8 Programming style",
    "text": "4.8 Programming style\nFinally, we want to discuss programming style. R can read any code you write as long as you write it using valid R syntax. However, R code can be much easier or harder for people (including you) to read depending on how it’s written. The coding best practices chapter of this book gives complete details on writing R code that is as easy as possible for people to read. So, please make sure to read it. It will make things so much easier for all of us!\n\n\n\n\n1. Ismay C, Kim AY. Chapter 1 getting started with data in R. Published online November 2019.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speaking R’s Language</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html",
    "href": "chapters/lets_get_programming/lets_get_programming.html",
    "title": "5  Let’s Get Programming",
    "section": "",
    "text": "5.1 Simulating data\nIn this chapter, we are going to tie together many of the concepts we’ve learned so far, and you are going to create your first basic R program. Specifically, you are going to write a program that simulates some data and analyzes it.\nData simulation can be really complicated, but it doesn’t have to be. It is simply the process of creating data as opposed to finding data in the wild. This can be really useful in several different ways.\nSo, let’s go ahead and write a complete R program to simulate and analyze some data. As we said, it doesn’t have to be complicated. In fact, in just a few lines of R code below we simulate and analyze some data about a hypothetical class.\nclass &lt;- data.frame(\n  names   = c(\"John\", \"Sally\", \"Brad\", \"Anne\"),\n  heights = c(68, 63, 71, 72)\n)\nclass\n\n  names heights\n1  John      68\n2 Sally      63\n3  Brad      71\n4  Anne      72\nmean(class$heights)\n\n[1] 68.5\nAs you can see, this data frame contains the students’ names and heights. We also use the mean() function to calculate the average height of the class. By the end of this chapter, you will understand all the elements of this R code and how to simulate your own data.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#simulating-data",
    "href": "chapters/lets_get_programming/lets_get_programming.html#simulating-data",
    "title": "5  Let’s Get Programming",
    "section": "",
    "text": "Simulating data is really useful for getting help with a problem you are trying to solve. Often, it isn’t feasible for you to send other people the actual data set you are working on when you encounter a problem you need help with. Sometimes, it may not even be legally allowed (i.e., for privacy reasons). Instead of sending them your entire data set, you can simulate a little data set that recreates the challenge you are trying to address without all the other complexity of the full data set. As a bonus,we have often found that we end up figuring out the solution to the problem we’re trying to solve as we recreate the problem in a simulated data set that we intended to share with others.\nSimulated data can also be useful for learning about and testing statistical assumptions. In epidemiology, we use statistics to draw conclusions about populations of people we are interested in based on samples of people drawn from the population. Because we don’t actually have data from all the people in the population, we have to make some assumptions about the population based on what we find in our sample. When we simulate data, we know the truth about our population because we created our population to have that truth. We can then use this simulated population to play “what if” games with our analysis. What if we only sampled half as many people? What if their heights aren’t actually normally distributed? What if we used a probit model instead of a logit model? Going through this process and answering these questions can help us understand how much, and under what circumstances, we can trust the answers we found in the real world.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#vectors",
    "href": "chapters/lets_get_programming/lets_get_programming.html#vectors",
    "title": "5  Let’s Get Programming",
    "section": "5.2 Vectors",
    "text": "5.2 Vectors\nVectors are the most fundamental data structure in R. Here, data structure means “container for our data.” There are other data structures as well; however, they are all built from vectors. That’s why we say vectors are the most fundamental data structure. Some of these other structures include matrices, lists, and data frames. In this book, we won’t use matrices or lists much at all, so you can forget about them for now. Instead, we will almost exclusively use data frames to hold and manipulate our data. However, because data frames are built from vectors, it can be useful to start by learning a little bit about them. Let’s create our first vector now.\n\n# Create an example vector\nnames &lt;- c(\"John\", \"Sally\", \"Brad\", \"Anne\")\n# Print contents to the screen\nnames\n\n[1] \"John\"  \"Sally\" \"Brad\"  \"Anne\" \n\n\n👆Here’s what we did above:\n\nWe created a vector of names with the c() (short for combine) function.\n\nThe vector contains four values: “John”, “Sally”, “Brad”, and “Anne”.\nAll of the values are character strings (i.e., words). We know this because all of the values are wrapped with quotation marks.\nHere we used double quotes above, but we could have also used single quotes. We cannot, however, mix double and single quotes for each character string. For example, c(\"John', ...) won’t work.\n\nWe assigned that vector of character strings to the word names using the &lt;- function.\n\nR now recognizes names as an object that we can do things with.\nR programmers may refer to the names object as “the names object”, “the names vector”, or “the names variable”. For our purposes, these all mean the same thing.\n\nWe printed the contents of the names object to the screen by typing the word “names”.\n\nR returns (shows us) the four character values (“John” “Sally” “Brad” “Anne”) on the computer screen.\n\n\nTry copying and pasting the code above into the RStudio console on your computer. You should notice the names vector appear in your global environment. You may also notice that the global environment pane gives you some additional information about this vector to the right of its name. Specifically, you should see chr [1:4] \"John\"  \"Sally\" \"Brad\"  \"Anne\". This is R telling us that names is a character vector (chr), with four values ([1:4]), and the first four values are \"John\"  \"Sally\" \"Brad\"  \"Anne\".\n\n\n5.2.1 Vector types\nThere are several different vector types, but each vector can have only one type. The type of the vector above was character. We can validate that with the typeof() function like so:\n\ntypeof(names)\n\n[1] \"character\"\n\n\nThe other vector types that we will use in this book are double, integer, and logical. Double vectors hold real numbers and integer vectors hold integers. Collectively, double vectors and integer vectors are known as numeric vectors. Logical vectors can only hold the values TRUE and FALSE. Here are some examples of each:\n\n\n5.2.2 Double vectors\n\n# A numeric vector\nmy_numbers &lt;- c(12.5, 13.98765, pi)\nmy_numbers\n\n[1] 12.500000 13.987650  3.141593\n\n\n\ntypeof(my_numbers)\n\n[1] \"double\"\n\n\n\n\n5.2.3 Integer vectors\nCreating integer vectors involves a weird little quirk of the R language. For some reason, and we have no idea why, we must type an “L” behind the number to make it an integer.\n\n# An integer vector - first attempt\nmy_ints_1 &lt;- c(1, 2, 3)\nmy_ints_1\n\n[1] 1 2 3\n\n\n\ntypeof(my_ints_1)\n\n[1] \"double\"\n\n\n\n# An integer vector - second attempt\n# Must put \"L\" behind the number to make it an integer. No idea why they chose \"L\".\nmy_ints_2 &lt;- c(1L, 2L, 3L)\nmy_ints_2\n\n[1] 1 2 3\n\n\n\ntypeof(my_ints_2)\n\n[1] \"integer\"\n\n\n\n\n5.2.4 Logical vectors\n\n# A logical vector\n# Type TRUE and FALSE in all caps\nmy_logical &lt;- c(TRUE, FALSE, TRUE)\nmy_logical\n\n[1]  TRUE FALSE  TRUE\n\n\n\ntypeof(my_logical)\n\n[1] \"logical\"\n\n\nRather than have an abstract discussion about the particulars of each of these vector types right now, we think it’s best to wait and learn more about them when they naturally arise in the context of a real challenge we are trying to solve with data. At this point, just having some vague idea that they exist is good enough.\n\n\n5.2.5 Factor vectors\nAbove, we said that we would only work with three vector types in this book: double, integer, and logical. Technically, that is true. Factors aren’t technically a vector type (we will explain below) but calling them a vector type is close enough to true for our purposes. We will briefly introduce you to factors here, and then discuss them in more depth later in the chapter on [Numerical Descriptions of Categorical Variables]. We cover them in greater depth there because factors are most useful in the context of working with categorical data – data that is grouped into discrete categories. Some examples of categorical variables commonly seen in public health data are sex, race or ethnicity, and level of educational attainment.\nIn R, we can represent a categorical variable in multiple different ways. For example, let’s say that we are interested in recording people’s highest level of formal education completed in our data. The discrete categories we are interested in are:\n\n1 = Less than high school\n2 = High school graduate\n3 = Some college\n4 = College graduate\n\nWe could then create a numeric vector to record the level of educational attainment for four hypothetical people as shown below.\n\n# A numeric vector of education categories\neducation_num &lt;- c(3, 1, 4, 1)\neducation_num\n\n[1] 3 1 4 1\n\n\nBut what is less-than-ideal about storing our categorical data this way? Well, it isn’t obvious what the numbers in education_num mean. For the purposes of this example, we defined them above, but if we didn’t have that information then we would likely have no idea what categories the numbers represent.\nWe could also create a character vector to record the level of educational attainment for four hypothetical people as shown below.\n\n# A character vector of education categories\neducation_chr &lt;- c(\n  \"Some college\", \"Less than high school\", \"College graduate\", \n  \"Less than high school\"\n)\neducation_chr\n\n[1] \"Some college\"          \"Less than high school\" \"College graduate\"     \n[4] \"Less than high school\"\n\n\nBut this strategy also has a few limitations that we will discuss in in the chapter on [Numerical Descriptions of Categorical Variables]. For now, we just need to quickly learn how to create and identify factor vectors.\nTypically, we don’t create factors from scratch. Instead, we typically convert (or “coerce”) an existing numeric or character vector into a factor. For example, we can coerce education_num to a factor like this:\n\n# Coerce education_num to a factor\neducation_num_f &lt;- factor(\n  x      = education_num,\n  levels = 1:4,\n  labels = c(\n    \"Less than high school\", \"High school graduate\", \"Some college\", \n    \"College graduate\"\n  )\n)\neducation_num_f\n\n[1] Some college          Less than high school College graduate     \n[4] Less than high school\n4 Levels: Less than high school High school graduate ... College graduate\n\n\n👆 Here’s what we did above:\n\nWe used the factor() function to create a new factor version of education_num.\n\nYou can type ?factor into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the factor() function is the x argument. The value passed to the x argument should be a vector of data. We passed the education_num vector to the x argument.\nThe second argument to the factor() function is the levels argument. This argument tells R the unique values that the new factor variable can take. We used the shorthand 1:4 to tell R that education_num_f can take the unique values 1, 2, 3, or 4.\nThe third argument to the factor() function is the labels argument. The value passed to the labels argument should be a character vector of labels (i.e., descriptive text) for each value in the levels argument. The order of the labels in the character vector we pass to the labels argument should match the order of the values passed to the levels argument. For example, the ordering of levels and labels above tells R that 1 should be labeled with “Less than high school”, 2 should be labeled with “High school graduate”, etc.\n\nWe used the assignment operator (&lt;-) to save our new factor vector in our global environment as education_num_f.\n\nIf we had used the name education_num instead, then the previous values in the education_num vector would have been replaced with the new values. That is sometimes what we want to happen. However, when it comes to creating factors, we typically keep the numeric version of the vector and create an additional factor version of the vector. We just often find that it can be useful to have both versions of the variable hanging around during the analysis process.\nWe also use the _f naming convention in our code. That means that when we create a new factor vector, we name it the same thing the original vector was named with the addition of _f (for factor) at the end.\n\nWe printed the vector to the screen. The values in education_num_f look similar to the character strings displayed in education_chr. Notice, however, that the values no longer have quotes around them and R displays Levels: Less than high school High school graduate Some college College graduate below the data values. This is R telling us the possible categorical values that this factor could take on. This is a telltale sign that the vector being printed to the screen is a factor.\n\nInterestingly, although R uses labels to make factors look like character vectors, they are still integer vectors under the hood. For example:\n\ntypeof(education_num_f)\n\n[1] \"integer\"\n\n\nAnd we can still view them as such.\n\nas.numeric(education_num_f)\n\n[1] 3 1 4 1\n\n\nIt is also possible to coerce character vectors to factors. For example, we can coerce education_chr to a factor like so:\n\n# Coerce education_chr to a factor\neducation_chr_f &lt;- factor(\n  x      = education_chr,\n  levels = c(\n    \"Less than high school\", \"High school graduate\", \"Some college\", \n    \"College graduate\"\n  )\n)\neducation_chr_f\n\n[1] Some college          Less than high school College graduate     \n[4] Less than high school\n4 Levels: Less than high school High school graduate ... College graduate\n\n\n👆 Here’s what we did above:\n\nWe coerced a character vector (education_chr) to a factor using the factor() function.\nBecause the levels are character strings, there was no need to pass any values to the labels argument this time. Keep in mind, though, that the order of the values passed to the levels argument matters. It will be the order that the factor levels will be displayed in our analyses.\n\nYou might reasonably wonder why we would want to convert character vectors to factors, but we will save that discussion for the chapter on [Numerical Descriptions of Categorical Variables].",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#data-frames",
    "href": "chapters/lets_get_programming/lets_get_programming.html#data-frames",
    "title": "5  Let’s Get Programming",
    "section": "5.3 Data frames",
    "text": "5.3 Data frames\nVectors are useful for storing a single characteristic where all the data is of the same type. However, in epidemiology, we typically want to store information about many different characteristics of whatever we happen to be studying. For example, we didn’t just want the names of the people in our class, we also wanted the heights. Of course, we can also store the heights in a vector like so:\n\nheights &lt;- c(68, 63, 71, 72)\nheights\n\n[1] 68 63 71 72\n\n\nBut this vector, in and of itself, doesn’t tell us which height goes with which person. When we want to create relationships between our vectors, we can use them to build a data frame. For example:\n\n# Create a vector of names\nnames &lt;- c(\"John\", \"Sally\", \"Brad\", \"Anne\")\n# Create a vector of heights\nheights &lt;- c(68, 63, 71, 72)\n# Combine them into a data frame\nclass &lt;- data.frame(names, heights)\n# Print the data frame to the screen\nclass\n\n  names heights\n1  John      68\n2 Sally      63\n3  Brad      71\n4  Anne      72\n\n\n👆Here’s what we did above:\n\nWe created a data frame with the data.frame() function.\n\nThe first argument we passed to the data.frame() function was a vector of names that we previously created.\nThe second argument we passed to the data.frame() function was a vector of heights that we previously created.\n\nWe assigned that data frame to the word class using the &lt;- function.\n\nR now recognizes class as an object that we can do things with.\nR programmers may refer to this class object as “the class object” or “the class data frame”. For our purposes, these all mean the same thing. We could also call it a data set, but that term isn’t used much in R circles.\n\nWe printed the contents of the class object to the screen by typing the word “class”.\n\nR returns (shows us) the data frame on the computer screen.\n\n\nTry copying and pasting the code above into the RStudio console on your computer. You should notice the class data frame appear in your global environment. You may also notice that the global environment pane gives you some additional information about this data frame to the right of its name. Specifically, you should see 4 obs. of 2 variables. This is R telling us that class has four rows or observations (4 obs.) and two columns or variables (2 variables). If you click the little blue arrow to the left of the data frame’s name, you will see information about the individual vectors that make up the data frame.\nAs a shortcut, instead of creating individual vectors and then combining them into a data frame as we’ve done above, most R programmers will create the vectors (columns) directly inside of the data frame function like this:\n\n# Create the class data frame\nclass &lt;- data.frame(\n  names   = c(\"John\", \"Sally\", \"Brad\", \"Anne\"),\n  heights = c(68, 63, 71, 72)\n) # Closing parenthesis down here.\n\n# Print the data frame to the screen\nclass\n\n  names heights\n1  John      68\n2 Sally      63\n3  Brad      71\n4  Anne      72\n\n\nAs you can see, both methods produce the exact same result. The second method, however, requires a little less typing and results in fewer objects cluttering up your global environment. What we mean by that is that the names and heights vectors won’t exist independently in your global environment. Rather, they will only exist as columns of the class data frame.\nYou may have also noticed that when we created the names and heights vectors (columns) directly inside of the data.frame() function we used the equal sign (=) to assign values instead of the assignment arrow (&lt;-). This is just one of those quirky R exceptions we talked about in the chapter on speaking R’s language. In fact, = and &lt;- can be used interchangeably in R. It is only by convention that we usually use &lt;- for assigning values, but use = for assigning values to columns in data frames. we don’t know why this is the convention. If it were up to me, we wouldn’t do this. We would just pick = or &lt;- and use it in all cases where we want to assign values. But, it isn’t up to me and we gave up on trying to fight it a long time ago. Your R programming life will be easier if you just learn to assign values this way – even if it’s dumb. 🤷\n\n\n\n\n\n\nWarning\n\n\n\nBy definition, all columns in a data frame must have the same length (i.e., number of rows). That means that each vector you create when building your data frame must have the same number of values in it. For example, the class data frame above has four names and four heights. If we had only entered three heights, we would have gotten the following error: Error in data.frame(names = c(\"John\", \"Sally\", \"Brad\", \"Anne\"), heights = c(68,  : arguments imply differing number of rows: 4, 3",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#tibbles",
    "href": "chapters/lets_get_programming/lets_get_programming.html#tibbles",
    "title": "5  Let’s Get Programming",
    "section": "5.4 Tibbles",
    "text": "5.4 Tibbles\nTibbles are a data structure that come from another tidyverse package – the tibble package. Tibbles are data frames and serve the same purpose in R that data frames serve; however, they are enhanced in several ways. 💪 You are welcome to look over the tibble documentation or the tibbles chapter in R for Data Science if you are interested in learning about all the differences between tibbles and data frames. For our purposes, there are really only a couple things we want you to know about tibbles right now.\nFirst, tibbles are a part of the tibble package – NOT base R. Therefore, we have to install and load either the tibble package or the dplyr package (which loads the tibble package for us behind the scenes) before we can create tibbles. We typically just load the dplyr package.\n\n# Install the dplyr package. YOU ONLY NEED TO DO THIS ONE TIME.\ninstall.packages(\"dplyr\")\n\n\n# Load the dplyr package. YOU NEED TO DO THIS EVERY TIME YOU START A NEW R SESSION.\nlibrary(dplyr)\n\nSecond, we can create tibbles using one of three functions: as_tibble(), tibble(), or tribble(). I’ll show you some examples shortly.\nThird, try not to be confused by the terminology. Remember, tibbles are data frames. They are just enhanced data frames.\n\n5.4.1 The as_tibble function\nWe use the as_tibble() function to turn an already existing basic data frame into a tibble. For example:\n\n# Create a data frame\nmy_df &lt;- data.frame(\n  name = c(\"john\", \"alexis\", \"Steph\", \"Quiera\"),\n  age  = c(24, 44, 26, 25)\n)\n\n# Print my_df to the screen\nmy_df\n\n    name age\n1   john  24\n2 alexis  44\n3  Steph  26\n4 Quiera  25\n\n\n\n# View the class of my_df\nclass(my_df)\n\n[1] \"data.frame\"\n\n\n👆Here’s what we did above:\n\nWe used the data.frame() function to create a new data frame called my_df.\nWe used the class() function to view my_df’s class (i.e., what kind of object it is).\n\nThe result returned by the class() function tells us that my_df is a data frame.\n\n\n\n# Use as_tibble() to turn my_df into a tibble\nmy_df &lt;- as_tibble(my_df)\n\n# Print my_df to the screen\nmy_df\n\n# A tibble: 4 × 2\n  name     age\n  &lt;chr&gt;  &lt;dbl&gt;\n1 john      24\n2 alexis    44\n3 Steph     26\n4 Quiera    25\n\n\n\n# View the class of my_df\nclass(my_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n👆Here’s what we did above:\n\nWe used the as_tibble() function to turn my_df into a tibble.\nWe used the class() function to view my_df’s class (i.e., what kind of object it is).\n\nThe result returned by the class() function tells us that my_df is still a data frame, but it is also a tibble. That’s what “tbl_df” and “tbl” mean.\n\n\n\n\n5.4.2 The tibble function\nWe can use the tibble() function in place of the data.frame() function when we want to create a tibble from scratch. For example:\n\n# Create a data frame\nmy_df &lt;- tibble(\n  name = c(\"john\", \"alexis\", \"Steph\", \"Quiera\"),\n  age  = c(24, 44, 26, 25)\n)\n\n# Print my_df to the screen\nmy_df\n\n# A tibble: 4 × 2\n  name     age\n  &lt;chr&gt;  &lt;dbl&gt;\n1 john      24\n2 alexis    44\n3 Steph     26\n4 Quiera    25\n\n\n\n# View the class of my_df\nclass(my_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n👆Here’s what we did above:\n\nWe used the tibble() function to create a new tibble called my_df.\nWe used the class() function to view my_df’s class (i.e., what kind of object it is).\n\nThe result returned by the class() function tells us that my_df is still a data frame, but it is also a tibble. That’s what “tbl_df” and “tbl” mean.\n\n\n\n\n5.4.3 The tribble function\nAlternatively, we can use the tribble() function in place of the data.frame() function when we want to create a tibble from scratch. For example:\n\n# Create a data frame\nmy_df &lt;- tribble(\n  ~name,    ~age,\n  \"john\",   24, \n  \"alexis\", 44, \n  \"Steph\",  26,\n  \"Quiera\", 25\n)\n\n# Print my_df to the screen\nmy_df\n\n# A tibble: 4 × 2\n  name     age\n  &lt;chr&gt;  &lt;dbl&gt;\n1 john      24\n2 alexis    44\n3 Steph     26\n4 Quiera    25\n\n\n\n# View the class of my_df\nclass(my_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n👆Here’s what we did above:\n\nWe used the tribble() function to create a new tibble called my_df.\nWe used the class() function to view my_df’s class (i.e., what kind of object it is).\n\nThe result returned by the class() function tells us that my_df is still a data frame, but it is also a tibble. That’s what “tbl_df” and “tbl” mean.\n\nThere is absolutely no difference between the tibble we created above with the tibble() function and the tibble we created above with the tribble() function. The only difference between the two functions is the syntax we used to pass the column names and data values to each function.\n\nWhen we use the tibble() function, we pass the data values to the function horizontally as vectors. This is the same syntax that the data.frame() function expects us to use.\nWhen we use the tribble() function, we pass the data values to the function vertically instead. The only reason this function exists is because it can sometimes be more convenient to type in our data values this way. That’s it.\nRemember to type a tilde (“~”) in front of your column names when using the tribble() function. For example, type ~name instead of name. That’s how R knows you’re giving it a column name instead of a data value.\n\n\n\n\n5.4.4 Why use tibbles\nAt this point, some students wonder, “If tibbles are just data frames, why use them? Why not just use the data.frame() function?” That’s a fair question. As we have said multiple times already, tibbles are enhanced. However, we don’t believe that going into detail about those enhancements is going to be useful to most of you at this point – and may even be confusing. But, we will show you one quick example that’s pretty self-explanatory.\nLet’s say that we are given some data that contains four people’s age in years. We want to create a data frame from that data. However, let’s say that we also want a column in our new data frame that contains those same ages in months. Well, we could do the math ourselves. We could just multiply each age in years by 12 (for the sake of simplicity, assume that everyone’s age in years is gathered on their birthday). But, we’d rather have R do the math for us. We can do so by asking R to multiply each value of the the column called age_years by 12. Take a look:\n\n# Create a data frame using the data.frame() function\nmy_df &lt;- data.frame(\n  name       = c(\"john\", \"alexis\", \"Steph\", \"Quiera\"),\n  age_years  = c(24, 44, 26, 25),\n  age_months = age_years * 12\n)\n\nError: object 'age_years' not found\n\n\nUh, oh! We got an error! This error says that the column age_years can’t be found. How can that be? We are clearly passing the column name age_years to the data.frame() function in the code chunk above. Unfortunately, the data.frame() function doesn’t allow us to create and refer to a column name in the same function call. So, we would need to break this task up into two steps if we wanted to use the data.frame() function. Here’s one way we could do this:\n\n# Create a data frame using the data.frame() function\nmy_df &lt;- data.frame(\n  name       = c(\"john\", \"alexis\", \"Steph\", \"Quiera\"),\n  age_years  = c(24, 44, 26, 25)\n)\n\n# Add the age in months column to my_df\nmy_df &lt;- my_df %&gt;% mutate(age_months = age_years * 12)\n\n# Print my_df to the screen\nmy_df\n\n    name age_years age_months\n1   john        24        288\n2 alexis        44        528\n3  Steph        26        312\n4 Quiera        25        300\n\n\nAlternatively, we can use the tibble() function to get the result we want in just one step like so:\n\n# Create a data frame using the tibble() function\nmy_df &lt;- tibble(\n  name       = c(\"john\", \"alexis\", \"Steph\", \"Quiera\"),\n  age_years  = c(24, 44, 26, 25),\n  age_months = age_years * 12\n)\n\n# Print my_df to the screen\nmy_df\n\n# A tibble: 4 × 3\n  name   age_years age_months\n  &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 john          24        288\n2 alexis        44        528\n3 Steph         26        312\n4 Quiera        25        300\n\n\nIn summary, tibbles are data frames. For the most part, we will use the terms “tibble” and “data frame” interchangeably for the rest of the book. However, remember that tibbles are enhanced data frames. Therefore, there are some things that we will do with tibbles that we can’t do with basic data frames.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#missing-data",
    "href": "chapters/lets_get_programming/lets_get_programming.html#missing-data",
    "title": "5  Let’s Get Programming",
    "section": "5.5 Missing data",
    "text": "5.5 Missing data\nAs indicated in the warning box at the end of the data frames section of this chapter, all columns in our data frames have to have the same length. So what do we do when we are truly missing information in some of our observations? For example, how do we create the class data frame if we are missing Anne’s height for some reason?\nIn R, we represent missing data with an NA. For example:\n\n# Create the class data frame\ndata.frame(\n  names   = c(\"John\", \"Sally\", \"Brad\", \"Anne\"),\n  heights = c(68, 63, 71, NA) # Now we are missing Anne's height\n)\n\n  names heights\n1  John      68\n2 Sally      63\n3  Brad      71\n4  Anne      NA\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you capitalize NA and don’t use any spaces or quotation marks. Also, make sure you use NA instead of writing \"Missing\" or something like that.\n\n\nBy default, R considers NA to be a logical-type value (as opposed to character or numeric). for example:\n\ntypeof(NA)\n\n[1] \"logical\"\n\n\nHowever, you can tell R to make NA a different type by using one of the more specific forms of NA. For example:\n\ntypeof(NA_character_)\n\n[1] \"character\"\n\n\n\ntypeof(NA_integer_)\n\n[1] \"integer\"\n\n\n\ntypeof(NA_real_)\n\n[1] \"double\"\n\n\nMost of the time, you won’t have to worry about doing this because R will take care of converting NA for you. What do we mean by that? Well, remember that every vector can have only one type. So, when you add an NA (logical by default) to a vector with double values as we did above (i.e., c(68, 63, 71, NA)), that would cause you to have three double values and one logical value in the same vector, which is not allowed. Therefore, R will automatically convert the NA to NA_real_ for you behind the scenes.\nThis is a concept known as “type coercion” and you can read more about it here if you are interested. As we said, most of the time you don’t have to worry about type coercion – it will happen automatically. But, sometimes it doesn’t and it will cause R to give you an error. we mostly encounter this when using the if_else() and case_when() functions, which we will discuss later.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#our-first-analysis",
    "href": "chapters/lets_get_programming/lets_get_programming.html#our-first-analysis",
    "title": "5  Let’s Get Programming",
    "section": "5.6 Our first analysis",
    "text": "5.6 Our first analysis\nCongratulations on your new R programming skills. 🎉 You can now create vectors and data frames. This is no small thing. Basically, everything else we do in this book will start with vectors and data frames.\nHaving said that, just creating data frames may not seem super exciting. So, let’s round out this chapter with a basic descriptive analysis of the data we simulated. Specifically, let’s find the average height of the class.\nYou will find that in R there are almost always many different ways to accomplish a given task. Sometimes, choosing one over another is simply a matter of preference. Other times, one method is clearly more efficient and/or accurate than another. This is a point that will come up over and over in this book. Let’s use our desire to find the mean height of the class as an example.\n\n5.6.1 Manual calculation of the mean\nFor starters, we can add up all the heights and divide by the total number of heights to find the mean.\n\n(68 + 63 + 71 + 72) / 4\n\n[1] 68.5\n\n\n👆Here’s what we did above:\n\nWe used the addition operator (+) to add up all the heights.\nWe used the division operator (/) to divide the sum of all the heights by 4 - the number of individual heights we added together.\nWe used parentheses to enforce the correct order of operations (i.e., make R do addition before division).\n\nThis works, but why might it not be the best approach? Well, for starters, manually typing in the heights is error prone. We can easily accidently press the wrong key. Luckily, we already have the heights stored as a column in the class data frame. We can access or refer to a single column in a data frame using the dollar sign notation.\n\n\n5.6.2 Dollar sign notation\n\nclass$heights\n\n[1] 68 63 71 72\n\n\n👆Here’s what we did above:\n\nWe used the dollar sign notation to access the heights column in the class data frame.\n\nDollar sign notation is just the data frame name, followed by the dollar sign, followed by the column name.\n\n\n\n\n5.6.3 Bracket notation\nFurther, we can use bracket notation to access each value in a vector. we think it’s easier to demonstrate bracket notation than it is to describe it. For example, we could access the third value in the names vector like this:\n\n# Create the heights vector\nheights &lt;- c(68, 63, 71, 72)\n\n# Bracket notation\n# Access the third element in the heights vector with bracket notation\nheights[3]\n\n[1] 71\n\n\nRemember, that data frame columns are also vectors. So, we can combine the dollar sign notation and bracket notation, to access each individual value of the height column in the class data frame. This will help us get around the problem of typing each individual height value. For example:\n\n# First way to calculate the mean\n# (68 + 63 + 71 + 72) / 4\n\n# Second way. Use dollar sign notation and bracket notation so that we don't \n# have to type individual heights\n(class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4\n\n[1] 68.5\n\n\n\n\n5.6.4 The sum function\nThe second method is better in the sense that we no longer have to worry about mistyping the heights. However, who wants to type class$heights[...] over and over? What if we had a hundred numbers? What if we had a thousand numbers? This wouldn’t work. Luckily, there is a function that adds all the numbers contained in a numeric vector – the sum() function. Let’s take a look:\n\n# Create the heights vector\nheights &lt;- c(68, 63, 71, 72)\n\n# Add together all the individual heights with the sum function\nsum(heights)\n\n[1] 274\n\n\nRemember, that data frame columns are also vectors. So, we can combine the dollar sign notation and sum() function, to add up all the individual heights in the heights column of the class data frame. It looks like this:\n\n# First way to calculate the mean\n# (68 + 63 + 71 + 72) / 4\n\n# Second way. Use dollar sign notation and bracket notation so that we don't \n# have to type individual heights\n# (class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4\n\n# Third way. Use dollar sign notation and sum function so that we don't have \n# to type as much\nsum(class$heights) / 4\n\n[1] 68.5\n\n\n👆Here’s what we did above:\n\nWe passed the numeric vector heights from the class data frame to the sum() function using dollar sign notation.\nThe sum() function returned the total value of all the heights added together.\nWe divided the total value of the heights by four – the number of individual heights.\n\n\n\n5.6.5 Nesting functions\n!! Before we move on, we want to point out something that is actually kind of a big deal. In the third method above, we didn’t manually add up all the individual heights - R did this calculation for us. Further, we didn’t store the sum of the individual heights somewhere and then divide that stored value by 4. Heck, we didn’t even see what the sum of the individual heights were. Instead, the returned value from the sum function (274) was used directly in the next calculation (/ 4) by R without us seeing the result. In other words, (68 + 63 + 71 + 72) / 4, 274 / 4, and sum(class$heights) / 4 are all exactly the same thing to R. However, the third method (sum(class$heights) / 4) is much more scalable (i.e., adding a lot more numbers doesn’t make this any harder to do) and much less error prone. Just to be clear, the BIG DEAL is that we now know that the values returned by functions can be directly passed to other functions in exactly the same way as if we typed the values ourselves.\nThis concept, functions passing values to other functions is known as nesting functions. It’s called nesting functions because we can put functions inside of other functions.\n“But, Brad, there’s only one function in the command sum(class$heights) / 4 – the sum() function.” Really? Is there? Remember when we said that operators are also functions in R? Well, the division operator is a function. And, like all functions it can be written with parentheses like this:\n\n# Writing the division operator as a function with parentheses\n`/`(8, 4)\n\n[1] 2\n\n\n👆Here’s what we did above:\n\nWe wrote the division operator in its more function-looking form.\n\nBecause the division operator isn’t a letter, we had to wrap it in backticks (`).\nThe backtick key is on the top left corner of your keyboard near the escape key (esc).\nThe first argument we passed to the division function was the dividend (The number we want to divide).\nThe second argument we passed to the division function was the divisor (The number we want to divide by).\n\n\nSo, the following two commands mean exactly the same thing to R:\n\n8 / 4\n\n\n`/`(8, 4)\n\nAnd if we use this second form of the division operator, we can clearly see that one function is nested inside another function.\n\n`/`(sum(class$heights), 4)\n\n[1] 68.5\n\n\n👆Here’s what we did above:\n\nWe calculated the mean height of the class.\n\nThe first argument we passed to the division function was the returned value from the sum() function.\nThe second argument we passed to the division function was the divisor (4).\n\n\nThis is kind of mind-blowing stuff the first time you encounter it. 🤯 we wouldn’t blame you if you are feeling overwhelmed or confused. The main points to take away from this section are:\n\nEverything we do in R, we will do with functions. Even operators are functions, and they can be written in a form that looks function-like; however, we will almost never actually write them in that way.\nFunctions can be nested. This is huge because it allows us to directly pass returned values to other functions. Nesting functions in this way allows us to do very complex operations in a scalable way and without storing a bunch of unneeded values that are created in the intermediate steps of the operation.\nThe downside of nesting functions is that it can make our code difficult to read - especially when we nest many functions. Fortunately, we will learn to use the pipe operator (%&gt;%) in the workflow basics part of this book. Once you get used to pipes, they will make nested functions much easier to read.\n\nNow, let’s get back to our analysis…\n\n\n5.6.6 The length function\nWe think most of us would agree that the third method we learned for calculating the mean height is preferable to the first two methods for most situations. However, the third method still requires us to know how many individual heights are in the heights column (i.e., 4). Luckily, there is a function that tells us how many individual values are contained in a vector – the length() function. Let’s take a look:\n\n# Create the heights vector\nheights &lt;- c(68, 63, 71, 72)\n\n# Return the number of individual values in heights\nlength(heights)\n\n[1] 4\n\n\nRemember, that data frame columns are also vectors. So, we can combine the dollar sign notation and length() function to automatically calculate the number of values in the heights column of the class data frame. It looks like this:\n\n# First way to calculate the mean\n# (68 + 63 + 71 + 72) / 4\n\n# Second way. Use dollar sign notation and bracket notation so that we don't \n# have to type individual heights\n# (class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4\n\n# Third way. Use dollar sign notation and sum function so that we don't have \n# to type as much\n# sum(class$heights) / 4\n\n# Fourth way. Use dollar sign notation with the sum function and the length \n# function\nsum(class$heights) / length(class$heights)\n\n[1] 68.5\n\n\n👆Here’s what we did above:\n\nWe passed the numeric vector heights from the class data frame to the sum() function using dollar sign notation.\nThe sum() function returned the total value of all the heights added together.\nWe passed the numeric vector heights from the class data frame to the length() function using dollar sign notation.\nThe length() function returned the total number of values in the heights column.\nWe divided the total value of the heights by the total number of values in the heights column.\n\n\n\n5.6.7 The mean function\nThe fourth method above is definitely the best method yet. However, this need to find the mean value of a numeric vector is so common that someone had the sense to create a function that takes care of all the above steps for us – the mean() function. And as you probably saw coming, we can use the mean function like so:\n\n# First way to calculate the mean\n# (68 + 63 + 71 + 72) / 4\n\n# Second way. Use dollar sign notation and bracket notation so that we don't \n# have to type individual heights\n# (class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4\n\n# Third way. Use dollar sign notation and sum function so that we don't have \n# to type as much\n# sum(class$heights) / 4\n\n# Fourth way. Use dollar sign notation with the sum function and the length \n# function\n# sum(class$heights) / length(class$heights)\n\n# Fifth way. Use dollar sign notation with the mean function\nmean(class$heights)\n\n[1] 68.5\n\n\nCongratulations again! You completed your first analysis using R!",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#some-common-errors",
    "href": "chapters/lets_get_programming/lets_get_programming.html#some-common-errors",
    "title": "5  Let’s Get Programming",
    "section": "5.7 Some common errors",
    "text": "5.7 Some common errors\nBefore we move on, we want to briefly discuss a couple common errors that will frustrate many of you early in your R journey. You may have noticed that we went out of our way to differentiate between the heights vector and the heights column in the class data frame. As annoying as that may have been, we did it for a reason. The heights vector and the heights column in the class data frame are two separate things to the R interpreter, and you have to be very specific about which one you are referring to. To make this more concrete, let’s add a weight column to our class data frame.\n\nclass$weight &lt;- c(160, 170, 180, 190)\n\n👆Here’s what we did above:\n\nWe created a new column in our data frame – weight – using dollar sign notation.\n\nNow, let’s find the mean weight of the students in our class.\n\nmean(weight)\n\nError: object 'weight' not found\n\n\nUh, oh! What happened? Why is R saying that weight doesn’t exist? We clearly created it above, right? Wrong. We didn’t create an object called weight in the code chunk above. We created a column called weight in the object called class in the code chunk above. Those are different things to R. If we want to get the mean of weight we have to tell R that weight is a column in class like so:\n\nmean(class$weight)\n\n[1] 175\n\n\nA related issue can arise when you have an object and a column with the same name but different values. For example:\n\n# An object called scores\nscores &lt;- c(5, 9, 3)\n\n# A colummn in the class data frame called scores\nclass$scores &lt;- c(95, 97, 93, 100)\n\nIf you ask R for the mean of scores, R will give you an answer.\n\nmean(scores)\n\n[1] 5.666667\n\n\nHowever, if you wanted the mean of the scores column in the class data frame, this won’t be the correct answer. Hopefully, you already know how to get the correct answer, which is:\n\nmean(class$scores)\n\n[1] 96.25\n\n\nAgain, the scores object and the scores column of the class object are different things to R.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/lets_get_programming/lets_get_programming.html#summary",
    "href": "chapters/lets_get_programming/lets_get_programming.html#summary",
    "title": "5  Let’s Get Programming",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nWow! We covered a lot in this first part of the book on getting started with R and RStudio. Don’t feel bad if your head is swimming. It’s a lot to take-in. However, you should feel proud of the fact that you can already do some legitimately useful things with R. Namely, simulate and analyze data. In the next part of this book, we are going to discuss some tools and best practices that will make it easier and more efficient for you to write and share your R code. After that, we will move on to tackling more advanced programming and data analysis challenges.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Let’s Get Programming</span>"
    ]
  },
  {
    "objectID": "chapters/asking_questions/asking_questions.html",
    "href": "chapters/asking_questions/asking_questions.html",
    "title": "6  Asking Questions",
    "section": "",
    "text": "6.1 When should we seek help?\nSooner or later, all of us will inevitably have questions while writing R programs. This is true for novice R users and experienced R veterans alike. Getting useful answers to programming questions can be really complicated under the best conditions (i.e., where someone with experience can physically sit down next to you to interactively work through your code with you). In reality, getting answers to our coding questions is often further complicated by the fact that we don’t have access to an experienced R programmer who can sit down next to us and help us debug our code. Therefore, this chapter will provide us with some guidance for seeking R programming help remotely. We’re not going to lie, this will likely be a frustrating process at times, but we will get through it!\nAn example\nBecause we like to start with the end in mind,  click here  for an example of a real post that we created on Stack Overflow. We will refer back to this post below.\nImagine yourself sitting in front of your computer on a Wednesday afternoon. You are working on a project that requires the analysis of some data. You know that you need to clean up your data a little bit before you can do your analysis. For example, maybe you need to drop all the rows from your data that have a missing value for a set of variables. Before you drop them, you want to take a look at which rows meet this criterion and what information would potentially be lost in the process of dropping those rows. In other words, you just want to view the rows of your data that have a missing value for any variable. Sounds simple enough! However, you start typing out the code to make this happen and that’s when you start to run into problems. At this point, the problem you encounter will typically come in one of a few different flavors.\nIn any of these cases, you will need to figure out what your next step will be. We believe that there is typically a lot of value in starting out by attempting to solve the problem on your own without directly asking others for help. Doing so will often lead you to a deeper understanding of the solution than you would obtain by simply being given the answer. Further, finding the solution on your own helps you develop problem-solving skills that will be useful for the next coding problem you encounter – even if the details of that problem are completely different than the details of your current problem. Having said that, finding a solution on your own does not mean attempting to do so in a vacuum without the use of any resources (e.g., textbooks, existing code, or the internet). By all means, use available resources (we suggest some good ones below)!\nOn the other hand, we – the authors – have found ourselves stubbornly hacking away on our own solution to a coding problem long after doing so ceased being productive on many occasions. We don’t recommend doing this either. We hope that the guidance in this chapter will provide you with some tools for effectively and efficiently seeking help from the broader R programming community once you’ve made a sincere effort to solve the problem on your own.\nBut, how long should you attempt to solve the problem on your own before reaching out for help? As far as we know, there are no hard-and-fast rules about how long you should wait before seeking help with coding problems from others. In reality, the ideal amount of time to wait is probably dependent on a host of factors including the nature of the problem, your level of experience, project deadlines, all of your little personal idiosyncrasies, and a whole host of other factors. Therefore, the best guidance we can provide is pretty vague. In general, it isn’t ideal to reach out to the R programming community for help as soon as you encounter a problem, nor is it typically ideal to spend many hours attempting to solve a coding problem that could be solved in few minutes if you were to post a well-written question on Stack Overflow or the RStudio Community (more on these below).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asking Questions</span>"
    ]
  },
  {
    "objectID": "chapters/asking_questions/asking_questions.html#when-should-we-seek-help",
    "href": "chapters/asking_questions/asking_questions.html#when-should-we-seek-help",
    "title": "6  Asking Questions",
    "section": "",
    "text": "As you sit down to write the code, you realize that you don’t really even know where to start.\nYou happily start typing out the code that you believe should work, but when you run the code you get an [error][errors] message.\nYou happily start typing out the code that you believe should work, but when you run the code you don’t get the result you were expecting.\nYou happily start typing out the code that you believe should work and it does! However, you notice that your solution seems clunky, inefficient, or otherwise less than ideal.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asking Questions</span>"
    ]
  },
  {
    "objectID": "chapters/asking_questions/asking_questions.html#where-should-we-seek-help",
    "href": "chapters/asking_questions/asking_questions.html#where-should-we-seek-help",
    "title": "6  Asking Questions",
    "section": "6.2 Where should we seek help?",
    "text": "6.2 Where should we seek help?\nWhere should you turn once you’ve determined that it is time to seek help for your coding problem? We suggest that you simply start with Google. Very often, a quick Google search will give you the results you need to help you solve your problem. However, Google search results won’t always have the answer you are looking for.\nIf you’ve done a Google search and you still can’t figure out how to solve your coding problem, we recommend posting a question on one of the following two websites:\n\nStack Overflow (https://stackoverflow.com/). This is a great website where programmers who use many different languages help each other solve programming problems. This website is free, but you will need to create an account.\nRStudio Community (https://community.rstudio.com/). Another great discussion-board-type website from the people who created a lot of the software we will use in this book. This website is also free, but also requires you to create an account.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease remember to cross-link your posts if you happen to create them on both Stack Overflow and RStudio Community. When we say “cross-link” we mean that you should add a hyperlink to your RStudio Community post on your Stack Overflow post and a link to your Stack Overflow post on your RStudio Community post.\n\n\nNext, let’s learn how to make a post.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asking Questions</span>"
    ]
  },
  {
    "objectID": "chapters/asking_questions/asking_questions.html#how-should-we-seek-help",
    "href": "chapters/asking_questions/asking_questions.html#how-should-we-seek-help",
    "title": "6  Asking Questions",
    "section": "6.3 How should we seek help?",
    "text": "6.3 How should we seek help?\nAt this point, you’ve run into a problem, you’ve spent a little time trying to work out a solution in your head, you’ve searched Google for a solution to the problem, and you’ve still come up short. So, you decide to ask the R programming community for some help using Stack Overflow. But, how do you do that?\n\n\n\n\n\n\nNote\n\n\n\nWe’ve decided to show you haw to create a post on Stack Overflow in this section, but the process for creating a post in the RStudio Community is very similar. Further, an RStudio Community tutorial is available here: https://community.rstudio.com/t/example-question-answer-topic-thread/70762.\n\n\n\n6.3.1 Creating a post on Stack Overflow\nThe first thing you need to do is navigate to the Stack Overflow website. The homepage will look something like the screenshot below.\n\n\n\n\n\n\n\n\n\nNext, you will click the blue “Ask Question” button. Doing so will take you to a screen like the following.\n\n\n\n\n\n\n\n\n\nAs you can see, you need to give your post a title, you need to post the actual question in the body section of the form, and then you can (and should) tag your post. “A tag is simply a word or a phrase that describes the topic of the question.”1 For our R-related questions we will want to use the “r” tag. Other examples of tags you may use often if you continue your R programming journey may include “dplyr” and “ggplot2”. When you have completed the form, you simply click the blue “Review your question” button towards the bottom-left corner of the screen.\n\n6.3.1.1 Inserting R code\nTo insert R code into your post (i.e., in the body), you will need to create code blocks. Then, you will type your R code inside of the code blocks. You can create code blocks using back-ticks ( ` ). The back-tick key is the upper-left key of most keyboards – right below the escape key. On our keyboard, the back-tick and the tilde ( ~ ) share the same key. We will learn more about code blocks in the chapter on using [Quarto/]. For now, let’s just take a look at an example of creating a code block in the screenshot below. This screenshot comes from the  example Stack Overflow post  introduced at the beginning of the chapter.\n\n\n\n\n\n\n\n\n\nAs you can see, we placed three back-ticks on their own line before our R code and three back-ticks on their own line after our R code. Alternatively, we could have used our mouse to highlight our R code and then clicked the code format button, which is highlighted in the screenshot above and looks like an empty pair of curly braces ( {} ).\n\n\n6.3.1.2 Reviewing the post\nAfter you create your post and click the “Review your question” button, you will have an opportunity to check your post for a couple of potential issues.\n\nDuplicates. You want to try your best to make sure your question isn’t a duplicate question. Meaning, you want to make sure that someone else hasn’t already asked the same question or a question that is very similar. As you are typing your post title, Stack Overflow will show you a list of potentially similar questions. It will show you that list again as you are reviewing your post. You should take a moment to look through that list and make sure you question isn’t going to be a duplicate. If it does end up being a duplicate, Stack Overflow moderators may tag it as such and close it.\nTypos and errors. Of course, you also want to check your post for standard typos, grammatical errors, and coding errors. However, you can always edit your post later if an error does slip through. You just need to click the edit text at the bottom of your post. A screenshot from the example post is shown in the screenshot below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Creating better posts and asking better questions\nThere are no bad R programming questions, but there are definitely ways to ask those questions that will be better received than others. And better received questions will typically result in faster responses and more useful answers. It’s important that you ask your questions in a way that will allow the reader to understand what you are trying to accomplish, what you’ve already tried, and what results you are getting. Further, unless it’s something extremely straight forward, you should always provide a little chunk of data that recreates the problem you are experiencing. These are known as reproducible examples This is so important that there is an R package that does nothing but help you create reproducible examples – Reprex.\nAdditionally, Stack Overflow and the RStudio community both publish guidelines for posting good questions.\n\nStack Overflow guide to asking questions: https://stackoverflow.com/help/how-to-ask\nRStudio Community Tips for writing R-related questions: https://community.rstudio.com/t/faq-tips-for-writing-r-related-questions/6824\n\nYou should definitely pause here an take a few minutes to read through these guidelines. If not now, come back and read them before you post your first question on either website. Below, we show you a few example posts and highlight some of the most important characteristics of quality posts.\n\n6.3.2.1 Example posts\nHere are a few examples of highly viewed posts on Stack Overflow and the RStudio community. Feel free to look them over. Notice what was good about these posts and what could have been better. The specifics of these questions are totally irrelevant. Instead, look for the elements that make posts easy to understand and respond to.\n\nStack Overflow: How to join (merge) data frames (inner, outer, left, right)\nRStudio Community: Error: Aesthetics must be either length 1 or the same as the data (2): fill\nStack Overflow: How should I deal with “package ‘xxx’ is not available (for R version x.y.z)” warning?\nRStudio Community: Could anybody help me! Cannot add ggproto objects together\n\n\n\n6.3.2.2 Question title\nWhen creating your posts, you want to make sure they have succinct, yet descriptive, titles. Stack overflow suggests that you pretend you are talking to a busy colleague and have to summarize your issue in a single sentence.2 The RStudio Community tips for writing questions further suggests that you be specific and use keywords.3 Finally, if you are really struggling, it may be helpful to write your title last.2 In our opinion, the titles from the first 3 examples above are pretty good. The fourth has some room for improvement.\n\n\n6.3.2.3 Explanation of the issue\nMake sure your posts have a brief, yet clear, explanation of what you are trying to accomplish. For example, “Sometimes I want to view all rows in a data frame that will be dropped if I drop all rows that have a missing value for any variable. In this case, I’m specifically interested in how to do this with dplyr 1.0’s across() function used inside of the filter() verb.”\nIn addition, you may want to add what you’ve already tried, what result you are getting, and what result you are expecting. This information can help others better understand your problem and understand if the solution they offer you does what you are actually trying to do.\nFinally, if you’ve already come across other posts or resources that were similar to the problem you are having, but not quite similar enough for you to solve your problem, it can be helpful to provide links to those as well. The author of example 3 above (i.e., How should I deal with “package ‘xxx’ is not available (for R version x.y.z)” warning?) does a very thorough job of linking to other posts.\n\n\n6.3.2.4 Reproducible example\nMake sure your question/post includes a small, reproducible data set that helps others recreate your problem. This is so important, and so often overlooked by students in our courses. Notice that we did NOT say to post the actual data you are working on for your project. Typically, the actual data sets that we work with will have many more rows and columns than are needed to recreate the problem. All of this extra data just makes the problem harder to clearly see. And more importantly, the real data we often work with contains protected health information (PHI) that should NEVER be openly published on the internet.\nHere is an example of a small, reproducible data set that we created for the  example Stack Overflow post  introduced at the beginning of the chapter. It only has 5 data rows and 3 columns, but any solution that solves the problem for this small data set will likely solve the problem in our actual data set as well.\n\n# Load the dplyr package.\nlibrary(dplyr)\n\n# Simulate a small, reproducible example of the problem.\ndf &lt;- tribble(\n  ~id, ~x, ~y,\n  1, 1, 0,\n  2, 1, 1,\n  3, NA, 1,\n  4, 0, 0,\n  5, 1, NA\n)\n\nSometimes you can add reproducible data to your post without simulating your own data. When you download R, it comes with some built in data sets that all other R users have access to as well. You can see an full list of those data sets by typing the following command in your R console:\n\ndata()\n\nThere are two data sets in particular, mtcars and iris, that seemed to be used often in programming examples and question posts. You can add those data sets to your global environment and start experimenting with them using the following code.\n\n# Add the mtcars data frame your global environment\ndata(mtcars)\n\n# Add the iris data frame to your global environment\ndata(iris)\n\nIn general, you are safe to post a question on Stack Overflow or the RStudio Community using either of these data frames in your example code – assuming you are able to recreate the issue you are trying to solve using these data frames.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asking Questions</span>"
    ]
  },
  {
    "objectID": "chapters/asking_questions/asking_questions.html#helping-others",
    "href": "chapters/asking_questions/asking_questions.html#helping-others",
    "title": "6  Asking Questions",
    "section": "6.4 Helping others",
    "text": "6.4 Helping others\nEventually, you may get to a point where you are able to help others with their R coding issues. In fact, spending a little time each day looking through posts and seeing if you can provide answers (whether you officially post them or not) is one way to improve your R coding skills. For some of us, this is even a fun way to pass time! 🤓\nIn the same way that there ways to improve the quality and usefulness of your question posts, there are also ways to improve the quality and usefulness of your replies to question posts. Stack Overflow also provides a guide for writing quality answers, which is available here: https://stackoverflow.com/help/how-to-answer. In our opinion, the most important part is to be patient, kind, and respond with a genuine desire to be helpful.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asking Questions</span>"
    ]
  },
  {
    "objectID": "chapters/asking_questions/asking_questions.html#summary",
    "href": "chapters/asking_questions/asking_questions.html#summary",
    "title": "6  Asking Questions",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nIn this chapter we discussed when and how to ask for help with R coding problems that will inevitably occur. In short,\n\nTry solving the problem on your own first, but don’t spend an entire day beating your head against the wall.\nStart with Google.\nIf you can’t find a solution on Google, create a post on Stack Overflow or the RStudio Community.\nUse best practices to create a high quality posts on Stack Overflow or the RStudio Community. Specifically:\n\nWrite succinct, yet descriptive, titles.\nWrite a a brief, yet clear, explanation of what you are trying to accomplish. Add what you’ve already tried, what result you are getting, and what result you are expecting.\nTry to always include a reproducable example of the problem you are encountering in the form of data.\n\nBe patient, kind, and genuine when posting or responding to posts.\n\n\n\n\n\n1. Stack Overflow. What are tags, and how should I use them? Published online January 2022.\n\n\n2. Stack Overflow. How do I ask a good question? Published online January 2022.\n\n\n3. RStudio. FAQ: Tips for writing r-related questions. Published online September 2021.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asking Questions</span>"
    ]
  },
  {
    "objectID": "chapters/r_scripts/r_scripts.html",
    "href": "chapters/r_scripts/r_scripts.html",
    "title": "7  R Scripts",
    "section": "",
    "text": "7.1 Creating R scripts\nUp to this point, we’ve only showed you how to submit your R code to R in the console. Figure 7.1\nSubmitting code directly to the console in this way works well for quick little tasks and snippets of code. But, writing longer R programs this way has some drawbacks that are probably already obvious to you. Namely, your code isn’t saved anywhere. And, because it isn’t saved anywhere, you can’t modify it, use it again later, or share it with others.\nTechnically, the statements above are not entirely true. When you submit code to the console, it is copied to RStudio’s History pane and from there you can save, modify, and share with others (see figure Figure 7.2. But, this method is much less convenient, and provides you with far fewer whistles and bells than the other methods we’ll discuss in this book.\nThose of you who have worked with other statistical programs before may be familiar with the idea of writing, modifying, saving, and sharing code scripts. SAS calls these code scripts “SAS programs”, Stata calls them “DO files”, and SPSS calls them “SPSS syntax files”. If you haven’t created code scripts before, don’t worry. There really isn’t much to it.\nIn R, the most basic type of code script is simply called an R script. An R script is just a plain text file that contains R code and comments. R script files end with the file extension .R.\nBefore we dive into giving you any more details about R scripts, we want to say that we’re actually going to discourage you from using them for most of what we do in this book. Instead, we’re going to encourage you to use Quarto files for the majority of your interactive coding, and for preparing your final products for end users. The next chapter is all about Quarto files. However, we’re starting with R scripts because:\nWith all that said, the screenshot below is of an example R script:\nClick here to download the R script\nAs you can see, I’ve called out a couple key elements of the R script to discuss. Figure 7.3\nFirst, instead of just jumping into writing R code, lines 1-5 contain a header that we’ve created with comments. Because we’ve created it with comments, the R interpreter will ignore it. But, it will help other people you collaborate with (including future you) figure out what this script does. Therefore, we suggest that your header includes at least the following elements:\nSecond, you may notice that we also used comments to create something we’re calling decorations on lines 1, 5, and 17. Like all comments, they are ignored by the R interpreter. But, they help create visual separation between distinct sections of your R code, which makes your code easier for humans to read. We tend to use the equal sign (# ====) for separating major sections and the dash (# ----) for separating minor sections; although, “major” and “minor” are admittedly subjective.\nwe haven’t explicitly highlighted it in the screenshot above, but it’s probably worth pointing out the use of line breaks (i.e., returns) in the code as well. This is much easier to read…\nthan this…\nThird, it’s considered a best practice to keep each line of code to 80 characters (including spaces) or less. There’s a little box at the bottom left corner of your R script that will tell you what row your cursor is currently in and how many characters into that row your cursor is currently at (starting at 1, not 0).\nFor example, 20:3 corresponds to having your cursor between the “e” and the “a” in mean(mtcars$mpg) in the example script above. Figure 7.4\nFourth, it’s also considered a best practice to load any packages that your R code will use at the very top of your R script (lines 7 & 8). Figure 7.3 Doing so will make it much easier for others (including future you) to see what packages your R code needs to work properly right from the start.\nTo create your own R scripts, click on the icon shown below Figure 7.5 and you will get a dropdown box with a list of files you can create. @ref(fig:new-r-script2)\nFigure 7.5: Click the new source file icon.\nClick the very first option – R Script.\nFigure 7.6: New source file options.\nWhen you do, a new untitled R Script will appear in the source pane.\nA blank R script in the source pane.\nAnd that’s pretty much it. Everything else in figure Figure 7.3 is just R code and comments about the R code. But, you can now easily save, modify, and share this code with others. In the next chapter, we are going to learn how to write R code in Quarto files, where we can add a ton of whistles and bells to this simple R script.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Scripts</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html",
    "href": "chapters/quarto_files/quarto_files.html",
    "title": "8  Quarto Files",
    "section": "",
    "text": "8.1 What is Quarto?\nIn the R Scripts chapter, you learned how to create R scripts – plain text files that contain R code and comments. These R scripts are kind of a big deal because they give us a simple and effective tool for saving, modifying, and sharing our R code. If it weren’t for the existence of Quarto files, we would probably do all of the coding in this book using R scripts. However, Quarto files do exist and they are AWESOME! So, we’re going to suggest that you use them instead of R scripts the majority of the time.\nIt’s actually kind of difficult for us to describe what a Quarto file is if you’ve never seen or heard of one before. Therefore, we’re going to start with an example and work backwards from there. Figure 8.1 below is a Quarto file. It includes the exact same R code and comments as the example we saw in Figure 7.3 in the previous chapter.\nClick here to download the Quarto file\nNotice that the results are embedded directly in the Quarto file immediately below the R code (e.g., between lines 21 and 22)!\nOnce rendered, the Quarto file creates the HTML file you see below in Figure 8.2. HTML files are what websites are made out of, and we’ll walk you through how to create them from Quarto files later in this chapter.\nClick here to download the rendered HTML file.\nNotice how everything is nicely formatted and easy to read!\nWhen you create Quarto files on your computer, as in Figure 8.3, the rendered HTML file is saved in the same folder by default.\nIn Figure 8.3 above, the HTML file is highlighted with a red box and ends with the .html file extension. The Quarto file is below the HTML file and ends with the .qmd file extension. Both of these files can be modified, saved, and shared with others.\nThere are literally entire websites and books about Quarto. Therefore, we’re only going to hit some of the highlights in this chapter. As a starting point, you can think of Quarto files as being a mix of R scripts, the R console, and a Microsoft Word or Google Doc document. We say this because:\nEven when we don’t share our Quarto files with anyone else, we find that the added functionality described above really helps us organize our data analysis more effectively and helps us understand what we were doing if we come back to the analysis at some point in the future.\nBut, Quarto_really_ shines when we do want to share our analysis or results with others. To get an idea of what we’re talking about, please take a look at the Quarto gallery and view some of the amazing things you can do with Quarto. As you can see there, Quarto files mix R code with other kinds of text and media to create documents, websites, presentations, and more. In fact, the book you are reading right now is created with Quarto files!",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html#what-is-quarto",
    "href": "chapters/quarto_files/quarto_files.html#what-is-quarto",
    "title": "8  Quarto Files",
    "section": "",
    "text": "The R code that you would otherwise write in R scripts is written in R code chunks when you use Quarto files. In Figure 8.1 there are R code chunks at lines 10 to 12, 14 to 16, 18 to 21, 27 to 29, and 33 to 35.\nInstead of having to flip back and forth between your source pane and your console (or viewer) pane in RStudio, the results from your R code are embedded directly in the Quarto file – directly below the code that generated them. In Figure 8.1 there are embedded results between lines 21 and 22, between lines 29 and 30, and between lines 35 and 36 (not fully visible).\nWhen creating a document in Microsoft Word or Google Docs, you may format text headings to help organize your document, you may format your text to emphasize certain words, you may add tables to help organize concepts or data, you may add links to other resources, and you may add pictures or charts to help you clearly communicate ideas to yourself or others. Similarly, Quarto files allow you to surround your R code with formatted text, tables, links, pictures, and charts directly in your document.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html#why-use-quarto",
    "href": "chapters/quarto_files/quarto_files.html#why-use-quarto",
    "title": "8  Quarto Files",
    "section": "8.2 Why use Quarto?",
    "text": "8.2 Why use Quarto?\nAt this point, you may be thinking “Ok, that Quarto gallery has some cool stuff, but it also looks complicated. Why shouldn’t I just use a basic R script for the little R program I’m writing?” If that’s what you’re thinking, you have a valid point. Quarto files are slightly more complicated than basic R scripts. However, after reading the sections below, we think you will find that getting started with Quarto doesn’t have to be super complicated and the benefits provided make the initial investment in learning Quarto worth your time.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html#create-a-quarto-file",
    "href": "chapters/quarto_files/quarto_files.html#create-a-quarto-file",
    "title": "8  Quarto Files",
    "section": "8.3 Create a Quarto file",
    "text": "8.3 Create a Quarto file\nRStudio makes it very easy to create your own Quarto file, of which there are several types. In this chapter, we’re going to show you how to create a Quarto file that can be rendered to an HTML file and viewed in your web browser.\nThe process is actually really similar to the process we used to create an R script. Start by clicking on the icon shown below in Figure 8.4.\n\n\n\n\n\n\n\n\nFigure 8.4: Click the new file icon.\n\n\n\n\n\nAs before, we’ll be presented with a dropdown box that lists a bunch of different file types for us to choose from. This time, we’ll click Quarto Document instead of R script. Figure 8.5\n\n\n\n\n\n\n\n\nFigure 8.5: New source file options.\n\n\n\n\n\nNext, a dialogue box will pop up with some options for us. For now, we will just give our Quarto document a super creative title – “Text Quarto” – and make sure the default HTML format is selected. Finally, we will click the Create button in the bottom right-hand corner of the dialogue box.\n\n\n\n\n\n\n\n\nFigure 8.6: New Quarto document options.\n\n\n\n\n\nA new Quarto file will appear in the RStudio source pane after we click the Create button. This Quarto file includes some example text and code meant to help us get started. We are typically going to erase all the example stuff and write our own text and code, but Figure 8.7 highlights some key components of Quarto files for now.\n\n\n\n\n\n\n\n\nFigure 8.7: The ‘Test Quarto’ file in the RStudio source pane.\n\n\n\n\n\nFirst, notice lines 1 through 6 in the example above. These lines make up something called the YAML header (pronounced yamel). It isn’t important for us to know what YAML means, but we do need to know that this is one of the defining features of Quarto files. We’ll talk more about the details of the YAML header soon.\nSecond, notice lines 16 through 18. These lines make up something called an R code chunk. Code chunks in Quarto files always start with three backticks ( ` ) and a pair of curly braces ({}), and they always end with three more backticks. We know that this code chunk contains R code because of the “r” inside of the curly braces. We can also create code chunks that will run other languages (e.g., python), but we won’t do that in this book. You can think of each R code chunk as a mini R script. We’ll talk more about the details of code chunks soon.\nThird, all of the other text is called Markdown. In Figure 8.7 above, the markdown text is just filler text with some basic instructions for users. In a real project we would use formatted text like this to add context around our code. For now, you can think of this as being very similar to the comments we wrote in our R scripts, but markdown allows us to do lots of cool things that the comments in our R scripts aren’t able to do. For example, line 6 has a link to a website embedded in it, line 8 includes a heading (i.e., ## Quarto), and line 14 includes text that is being formatted (the orange text surrounded by two asterisks). In this case, the text is being bolded.\nAnd that is all we have to do to create a basic Quarto file. Next, we’re going to give you a few more details about each of the key components of the Quarto file that we briefly introduced above.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html#yaml-headers",
    "href": "chapters/quarto_files/quarto_files.html#yaml-headers",
    "title": "8  Quarto Files",
    "section": "8.4 YAML headers",
    "text": "8.4 YAML headers\nThe YAML header is unlike anything we’ve seen before. The YAML header always begins and ends with dash-dash-dash (---) typed on its own line (1 & 6 in Figure 8.7). The code written inside the YAML header generally falls into two categories:\n\nValues to be rendered in the Quarto file. For example, in Figure 8.7 we told Quarto to title our document “Test Quarto”. The title is added to the file by adding the title keyword, followed by a colon (:), followed by a character string wrapped in quotes. Examples of other values we could have added include author and date.\nInstructions that tell Quarto how to process the file. What do we mean by that? Well, remember the Quarto gallery you saw earlier? That gallery includes Word documents, PDF documents, websites, and more. But all of those different document types started as Quarto file similar to the one in Figure 8.7. Quarto will create a PDF document, a Word document, or a website from the Quarto file based, in part, on the instructions we give it inside the YAML header. For example, the YAML header in Figure 8.7 tells Quarto to create an HTML file from our Quarto file. This output type is selected by adding the format keyword, followed by a colon (:), followed by the html keyword. Further, we added the embed-resources: true option to our HTML format. Including that option makes it possible for us to send a single HTML file to others with all the supporting files embedded.\n\nWhat does an HTML file look like? Well, if you hit the Render button in RStudio:\n\n\n\n\n\n\n\n\nFigure 8.8: RStudio’s render button. Only visible when a Quarto file is open.\n\n\n\n\n\nR will ask you to save your Quarto file. After you save it, R will automatically create (or render) a new HTML file and save it in the same location where your Quarto file is saved. Additionally, a little browser window, like Figure 8.9 will pop up and give you a preview of what the rendered HTML file looks like.\n\n\n\n\n\n\n\n\nFigure 8.9: An HTML file created using a Quarto file.\n\n\n\n\n\nNotice all the formatting that was applied when R rendered the HTML file. For example, the title – “Test Quarto” – is in big bold letters at the top of the screen, The headings – Quarto and Running code – are also written in a large bold font with a faint line underneath them, the link to the Quarto website is now blue and clickable, and the word “Render” is written in bold font.\nWe can imagine that this section may seem a little confusing to some readers right now. If so, don’t worry. You don’t really need to understand the YAML header at this point. Remember, when you create a new Quarto file in the manner we described above, the YAML header is already there. You will probably want to change the title, but that may be the only change you make for now.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html#r-code-chunks",
    "href": "chapters/quarto_files/quarto_files.html#r-code-chunks",
    "title": "8  Quarto Files",
    "section": "8.5 R code chunks",
    "text": "8.5 R code chunks\nAs we said above, R code chunks always start out with three backticks ( ` ) and a pair of curly braces ({}) with an “r” in them ({r}), and they always end with three more backticks. Typing that over and over can be tedious, so RStudio provides a keyboard shortcut for inserting R code chunks into our Quarto files.\nOn MacOS type option + command + i.\nOn Windows type control + alt + i\nInside the code chunk, we can type anything that we would otherwise type in the console or in an R script – including comments. We can then click the little green arrow in the top right corner of the code chunk to submit it to R and see the result (see the play button in Figure 8.7).\nAlternatively, we can run the code in the code chunk by typing shift + command + return on MacOS or shift + control + enter on Windows. If we want to submit a small section of code in a code chunk, as opposed to all of the code in the code chunk, we can use our mouse to highlight just the section of code we want to run and type control + return on MacOS or control + enter on Windows. There are also options to run all code chunks in the Quarto file, all code chunks above the current code chunk, and all code chunks below the current chunk. You can access these, and other, run options using the Run button in the top right-hand corner of the Quarto file in RStudio (see Figure 8.10 below).\n\n\n\n\n\n\n\n\nFigure 8.10: The run button in RStudio.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html#markdown",
    "href": "chapters/quarto_files/quarto_files.html#markdown",
    "title": "8  Quarto Files",
    "section": "8.6 Markdown",
    "text": "8.6 Markdown\nMany readers have probably heard of HTML and CSS before. HTML stands for hypertext markup language and CSS stands for cascading style sheets. Together, HTML and CSS are used to create and style every website you’ve ever seen. HTML files created from our Quarto files are no different. They will open in any web browser and behave just like any other website. Therefore, we can manipulate and style them using HTML and CSS just like any other website. However, it takes most people a lot of time and effort to learn HTML and CSS. So, markdown was created as an easier-to-use alternative. Think of it as HTML and CSS lite. It can’t fully replace HTML and CSS, but it is much easier to learn, and you can use it to do many of the main things you might want to do with HTML and CSS. For example, Figure 8.7 and Figure 8.9 we saw that wrapping our text with two asterisks (**) bolds it.\nThere are a ton of other things we can do with markdown, and we recommend checking out Quarto’s markdown basics website to learn more. The website covers a lot and may feel overwhelming at first. So, we suggest just play around with some of the formatting options and get a feel for what they do. Having said that, it’s totally fine if you don’t try to tackle learning markdown syntax right now. You don’t really need markdown to follow along with the rest of the book. However, we still suggest using Quarto files for writing, saving, modifying, and sharing your R code.\n\n8.6.1 Markdown headings\nWhile we are discussing markdown, we would like to call special attention to markdown headings. We briefly glazed over them above, but we find that beginning R users typically benefit from a slightly more detailed discussion. Think back to the ## Quarto on line 8 of Figure 8.7. This markdown created a heading – text that stands out and breaks our document up into sections. We can create headings by beginning a line in our Quarto document with one or more hash symbols (#), followed by a space, and then our heading text. Headings can be nested underneath each other in the same way you might nest topics in a bulleted list. For example:\n\nAnimals\n\nDog\n\nLab\nYorkie\n\nCat\n\nPlants\n\nFlowers\nTrees\n\nOak\n\n\n\nNesting list items this way organizes our list and conveys information that would otherwise require explicitly writing out more text. For example, that a lab is a type of dog and that dogs are a type of animal. Thoughtfully nesting our headings in our Quarto files can have similar benefits. So, how do we nest our headings? Great question! Quarto and RStudio will automatically nest them based on the number of hash symbols we use (between 1 and 6). In the example above, ## Quarto it is a second-level heading. We know this because the line begins with two hash symbols. Figure 8.11 below shows how we might organize a Quarto file for a data analysis project into nested sections using markdown headings.\nA really important benefit of organizing our Quarto file this way is that it allows us to use RStudio’s document outline pane to quickly navigate around our Quarto file. In this trivial example, it isn’t such a big deal. But it can be a huge time saver in a Quarto file with hundreds, or thousands, of lines of code.\n\n\n\n\n\n\n\n\nFigure 8.11: A Quarto file with nested headings.\n\n\n\n\n\nAs a final note on markdown headings, we find that new R users sometimes mix up comments and headings. This is a really understandable mistake to make because both start with the hash symbol. So, how do you know when typing a hash symbol will create a comment and when it will create a heading?\n\nThe hash symbol always creates comments in R scripts. R scripts don’t understand markdown. Therefore, they don’t have markdown headings. R scripts only understand comments, which begin with a hash symbol, and R code.\nThe hash symbol always creates markdown headings in Quarto files when typed outside of an R code chunk. Remember, everything in between the R code chunks in our Quarto files is considered markdown by Quarto, and hash symbols create headings in the markdown language.\nThe hash symbol always creates comments in Quarto files when typed inside of an R code chunk. Remember, we can think of each R code chunk as a mini R script, and in R scripts, hash symbols create comments.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/quarto_files/quarto_files.html#summary",
    "href": "chapters/quarto_files/quarto_files.html#summary",
    "title": "8  Quarto Files",
    "section": "8.7 Summary",
    "text": "8.7 Summary\nQuarto files bring together R code, formatted text, and media in a single file. We can use them to make our lives easier when working on small projects that are just for us, and we can use them to create large complex documents, websites, and applications that are intended for much larger audiences. RStudio makes it easy for us to create and render Quarto files into many different document types, and learning a little bit of markdown can help us format those documents really nicely. We believe that Quarto files are a great default file type to use for most projects and we encourage readers to review the Quarto website for more details (and inspiration)!",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Quarto Files</span>"
    ]
  },
  {
    "objectID": "chapters/r_projects/projects.html",
    "href": "chapters/r_projects/projects.html",
    "title": "9  R Projects",
    "section": "",
    "text": "In previous chapters of this book, we learned how to use R Scripts and Quarto Files to create, modify, save, and share our R code and results. However, in most real-world projects we will actually create multiple different R scripts and/or Quarto files. Further, we will often have other files (e.g., images or data) that we want to store alongside our R code files. Over time, keeping up with all of these files can become cumbersome. R projects are a great tool for helping us organize and manage collections of files. Another really important advantage to organizing our files into R projects is that they allow us to use relative file paths instead of absolute file paths, which we will [discuss in detail later][File paths].\nRStudio makes creating R projects really simple. For starters, let’s take a look at the top right corner of our RStudio application window. Currently, we see an R project icon that looks like little blue 3-dimensional box with an “R” in the middle. To the right of the R project icon, we see words Project: (None). RStudio is telling us that our current session is not associated with an R project.\n\n\n\n\n\n\n\n\n\nTo create a new R project, we just need to click the drop-down arrow next to the words Project: (None) to open the projects menu. Then, we will click the New Project... option.\n\n\n\n\n\n\n\n\n\nDoing so will open the new project wizard. For now, we will select the New Directory option. We will discuss the other options later in the book.\n\n\n\n\n\n\n\n\n\nNext, we will click the New Project option.\n\n\n\n\n\n\n\n\n\nIn the next window, we will have to make some choices and enter some information. The fist thing we will have to do is name our project. We do so by entering a value in the Directory name: box. Often, we can name our R project directory to match the name of the larger project we are working on in a pretty natural way. If not, the name we choose for our project directory should essentially follow the same guidelines that we use for [object (variable) names][Object (variable) names], which we will learn about soon. In this example, we went with the very creative my_first_project project name.😆\nWhen we create our R project in a moment, RStudio will create a folder on our computer where we can keep all of the files we need for our project. That folder will be named using the name we entered in the Directory name: box in the previous step. So, the next thing we need to do is tell R where on our computer to put the folder. We do so by clicking the Browse... button and selecting a location. For this example, we chose to create the project on our computer’s desktop.\nFinally, we just click the Create Project button near the bottom-right corner of the New Project Wizard.\n\n\n\n\n\n\n\n\n\nDoing so will create our new R project in the location we selected in the Create project as subdirectory of: text box in the new project wizard. In the screenshot below, we can see that a folder was created on our computer’s desktop called my_first_project. Additionally, there is one file inside of that folder named my_first_project that ends with the file extension .Rproj (see red arrow 2 in the figure below).\n\n\n\n\n\n\n\n\n\nThis file is called an R project file. Every time we create an R project, RStudio will create an R project file and add it to our project directory (i.e., the folder) for us. This file helps RStudio track and organize our R project.\nTo easiest way to open the R project we just created is to double click the R project file – my_first_project.Rproj. Doing so will open a new RStudio session along with all of the R code files we had open last time we were working on our R project. Because this is our first time opening our example R project, we won’t see any R code files.\nAlternatively, we can open our R project by once again clicking the R project icon in the upper right corner of an open RStudio session and then clicking the Open Project... option. This will open a file selection window where we can select our R project directory and open it.\n\n\n\n\n\n\n\n\n\nFinally, we will know that RStudio understands that we are working in the context of our project because the words Project: (None) that we previously saw in the top right corner of the RStudio window will be replaced with the project name. In this case, my_first_project.\n\n\n\n\n\n\n\n\n\nNow that we’ve created our R project, there’s nothing special we need to do to add other files to it. We only need save files and folders for our project as we typically would. We just need to make sure that we save them in our project directory (i.e., the folder). RStudio will take care of the rest.\nR projects are a great tool for organizing our R code and other complimentary files. Should we use them every single time we use R? Probably not. So, when should we use them? Well, the best – albeit somewhat unhelpful – answer is probably to use them whenever they are useful. However, at this point in your R journey you may not have enough experience to know when they will be useful and when they won’t. Therefore, we are going to suggest that create an R project for your project if (1) your project will have more than one file and/or (2) more than one person will be working on the R code in your project. As we alluded to earlier, organizing our files into R projects allows us to use relative file paths instead of absolute file paths, which will make it much easier for us to collaborate with others. [File paths] will be discussed in detail later.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R Projects</span>"
    ]
  },
  {
    "objectID": "chapters/best_practices/best_practices.html",
    "href": "chapters/best_practices/best_practices.html",
    "title": "10  Coding Best Practices",
    "section": "",
    "text": "10.1 General principles\nAt this point in the book, we’ve talked a little bit about what R is. We’ve also talked about the RStudio IDE and took a quick tour around its four main panes. Finally, we wrote our first little R program, which simulated and analyzed some data about a hypothetical class. Writing and executing this R program officially made you an R programmer. 🏆\nHowever, you should know that not all R code is equally “good” – even when it’s equally valid. What do we mean by that? Well, we already discussed the R interpreter and R syntax in the chapter on speaking R’s language. Any code that uses R syntax that the R interpreter can understand is valid R code. But, is the R interpreter the only one reading your R code? No way! In epidemiology, we collaborate with others all the time! That collaboration is going to be much more efficient and enjoyable when there is good communication – including R code that is easy to read and understand. Further, you will often need to read and/or reuse code you wrote weeks, months, or years after you wrote it. You may be amazed at how quickly you forget what you did and/or why you did it that way. Therefore, in addition to writing valid R code, this chapter is about writing “good” R code – code that easily and efficiently communicates ideas to humans.\nOf course, “good code” is inevitably somewhat subjective. Reasonable people can have a difference of opinion about the best way to write code that is easy to read and understand. Additionally, reasonable people can have a difference of opinion about when code is “good enough.” For these reasons, we’re going to offer several “suggestions” about writing good R code below, but only two general principles, which we believe most R programmers would agree with.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/best_practices/best_practices.html#general-principles",
    "href": "chapters/best_practices/best_practices.html#general-principles",
    "title": "10  Coding Best Practices",
    "section": "",
    "text": "Comment your code. Whether you intend to share your code with other people or not, make sure to write lots of comments about what you are trying to accomplish in each section of your code and why.\nUse a style consistently. We’re going to suggest several guidelines for styling your R code below, but you may find that you prefer to style your R code in a different way. Whether you adopt our suggested style or not, please find or create a style that works for you and your collaborators and use it consistently.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/best_practices/best_practices.html#code-comments",
    "href": "chapters/best_practices/best_practices.html#code-comments",
    "title": "10  Coding Best Practices",
    "section": "10.2 Code comments",
    "text": "10.2 Code comments\nThere isn’t a lot of specific advice that we can give here because comments are so idiosyncratic to the task at hand. So, we think the best we can do at this point is to offer a few examples for you to think about.\n\n10.2.1 Defining key variables\nAs we will discuss below, variables should have names that are concise, yet informative. However, the data you receive in the real world will not always include informative variable names. Even when someone has given the variables informative names, there may still be contextual information about the variables that is important to understand for data management and analysis. Some data sets will come with something called a codebook or data dictionary. These are text files that contain information about the data set that are intended to provide you with some of that more detailed information. For example, the survey questions that were used to capture the values in each variable or what category each value in a categorical variable represents. However, real data sets don’t always come with a data dictionary, and even when they do, it can be convenient to have some of that contextual information close at hand, right next to your code. Therefore, we will sometimes comment our code with information about variables that are important for the analysis at hand. Here is an example from an administrative data set we ww using for an analysis:\n\n* **Case number definition**\n\n    - Case / investigation number.\n\n* **Intake stage definition**\n\n    - An ID number assigned to the Intake. Each Intake (Report) has its \n      own number. A case may have more than one intake. For example, case # 12345 \n      has two intakes associated with it, 9 days apart, each with their own ID \n      number. Each of the two intakes associated with this case have multiple \n      allegations.\n\n* **Intake start definition**\n\n    - An intake is the submission or receipt of a report - a phone call or \n      web-based. The Intake Start Date refers to the date the staff member \n      opens a new record to begin recording the report.\n\n\n\n10.2.2 What this code is trying to accomplish\nSometimes, it is obvious what a section of code literally does. but not so obvious why you’re doing it. We often try to write some comments around our code about what it’s trying to ultimately accomplish and why. For example:\n\n## Standardize character strings\n\n# Because we will merge this data with other data sets in the future based on \n# character strings (e.g., name), we need to go ahead and standardize their \n# formats here. This will prevent mismatches during the merges. Specifically, \n# we:\n\n# 1. Transform all characters to lower case   \n# 2. Remove any special characters (e.g., hyphens, periods)   \n# 3. Remove trailing spaces (e.g., \"John Smith \")   \n# 4. Remove double spaces (e.g., \"John  Smith\")  \n\nvars &lt;- quos(full_name, first_name, middle_name, last_name, county, address, city)\n\nclient_data &lt;- client_data %&gt;% \n  mutate_at(vars(!!! vars), tolower) %&gt;% \n  mutate_at(vars(!!! vars), stringr::str_replace_all, \"[^a-zA-Z\\\\d\\\\s]\", \" \") %&gt;%\n  mutate_at(vars(!!! vars), stringr::str_replace, \"[[:blank:]]$\", \"\") %&gt;% \n  mutate_at(vars(!!! vars), stringr::str_replace_all, \"[[:blank:]]{2,}\", \" \")\n\nrm(vars)\n\n\n\n10.2.3 Why we chose this particular strategy\nIn addition to writing comments about why we did something, we sometimes write comments about why we did it instead of something else. Doing this can save you from having to relearn lessons you’ve already learned through trial and error but forgot. For example:\n\n### Create exact match dummy variables\n\n* We reshape the data from long to wide to create these variables because it significantly decreases computation time compared to doing this as a group_by operation on the long data.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/best_practices/best_practices.html#style-guidelines",
    "href": "chapters/best_practices/best_practices.html#style-guidelines",
    "title": "10  Coding Best Practices",
    "section": "10.3 Style guidelines",
    "text": "10.3 Style guidelines\nUsInG c_o_n_s_i_s_t_e_n_t STYLE i.s. import-ant!\n\nGood coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. As with styles of punctuation, there are many possible variations… Good style is important because while your code only has one author, it’ll usually have multiple readers. This is especially true when you’re writing code with others. In that case, it’s a good idea to agree on a common style up-front. Since no style is strictly better than another, working with others may mean that you’ll need to sacrifice some preferred aspects of your style.1\n\nBelow, we outline the style that we and our collaborators typically use when writing R code for a research project. It generally follows the Tidyverse style guide, which we strongly suggest you read. Outside of our class, you don’t have to use our style, but you really should find or create a style that works for you and your collaborators and use it consistently.\n\n10.3.1 Comments\nPlease put a space in between the pound/hash sign and the rest of your text when writing comments. For example, # here is my comment instead of #here is my comment. It just makes the comment easier to read.\n\n\n10.3.2 Object (variable) names\nIn addition to the object naming guidance given in the Tidyverse style guide, We suggest the following object naming conventions.\n\n\n10.3.3 Use names that are informative\nUsing names that are informative and easy to remember will make life easier for everyone who uses your data – including you!\n\n# Uninformative names - Don't do this\nx1\nvar1\n\n# Informative names\nemployed\nmarried\neducation\n\n\n10.3.3.1 Use names that are concise\nYou want names to be informative, but you don’t want them to be overly verbose. Really long names create more work for you and more opportunities for typos. In fact, we recommend using a single word when you can.\n\n# Write out entire name of the study the data comes from - Don't do this\nwomens_health_initiative\n\n# Write out an acronym for the study the data comes from - assuming everyone \n# will be familiar with this acronym - Do this\nwhi\n\n\n\n10.3.3.2 Use all lowercase letters\nRemember, R is case-sensitive, which means that myStudyData and mystudydata are different things to R. Capitalizing letters in your file name just creates additional details to remember and potentially mess up. Just keep it simple and stick with lowercase letters.\n\n# All upper case - so aggressive - Don't use\nMYSTUDYDATA\n\n# Camel case - Don't use\nmyStudyData\n\n# All lowercase - Use\nmy_study_data\n\n\n\n10.3.3.3 Separate multiple words with underscores.\nSometimes you really just need to use multiple words to name your object. In those cases, we suggested separating words with an underscore.\n\n# Multiple words running together - Hard to read - Don't use\nmycancerdata\n\n# Camel case - easier to read, but more to remember and mess up - Don't use\nmyCancerData\n\n# Separate with periods - easier to read, but doesn't translate well to many \n# other languages. For example, SAS won't accept variable names with \n# periods - Don't use\nmy.cancer.data\n\n# Separate with underscores - Use\nmy_cancer_data\n\n\n\n10.3.3.4 Prefix the names of similar variables\nWhen you have multiple related variables, it’s good practice to start their variable names with the same word. It makes these related variables easier to find and work with in the future if we need to do something with all of them at once. We can sort our variable names alphabetically to easily find find them. Additionally, we can use variable selectors like starts_with(\"name\") to perform some operation on all of them at once.\n\n# Don't use\nfirst_name\nlast_name\nmiddle_name\n\n# Use\nname_first\nname_last\nname_middle\n\n# Don't use\nstreet\ncity\nstate\n\n# Use\naddress_street\naddress_city\naddress_state\n\n\n\n\n10.3.4 File Names\nAll the variable naming suggestons above also apply to file names. However, we make a few additional suggestions specific to file names below.\n\n10.3.4.1 Managing multiple files in projects\nWhen you are doing data management and analysis for real-world projects you will typically need to break the code up into multiple files. If you don’t, the code often becomes really difficult to read and manage. Having said that, finding the code you are looking for when there are 10, 20, or more separate files isn’t much fun either. Therefore, we suggest the following (or similar) file naming conventions be used in your projects.\n\nSeparate data cleaning and data analysis into separate files (typically, .R or .Rmd).\n\nData cleaning files should be prefixed with the word “data” and named as follows\n\ndata_[order number]_[purpose]\n\n\n\n\n# Examples\ndata_01_import.Rmd\ndata_02_clean.Rmd\ndata_03_process_for_regression.Rmd\n\n\nAnalysis files that do not directly create a table or figure should be prefixed with the word “analysis” and named as follows\n\nanalysis_[order number]_[brief summary of content]\n\n\n\n# Examples\nanalysis_01_exploratory.Rmd\nanalysis_02_regression.Rmd\n\n\nAnalysis files that DO directly create a table or figure should be prefixed with the word “table” or “fig” respectively and named as follows\n\ntable_[brief summary of content] or\n\nfig_[brief summary of content]\n\n\n\n# Examples\ntable_network_characteristics.Rmd\nfig_reporting_patterns.Rmd\n\n\n\n\n\n\n\nNote\n\n\n\nWe sometimes do data manipulation (create variables, subset data, reshape data) in an analysis file if that analysis (or table or chart) is the only analysis that uses the modified data. Otherwise, we do the modifications in a separate data cleaning file.\n\n\n\nImages\n\nShould typically be exported as png (especially when they are intended for use HTML files).\n\nShould typically be saved in a separate “img” folder under the project home directory.\n\nShould be given a descriptive name.\n\nExample: histogram_heights.png, NOT fig_02.png.\n\nWe have found that the following image sizes typically work pretty well for our projects.\n\n1920 x 1080 for HTML\n\n770 x 360 for Word\n\n\nWord and PDF output files\n\nWe typically save them in a separate “docs” folder under the project home directory.\nWhenever possible, we try to set the Word or PDF file name to match the name of the R file that it was created in.\n\nExample: first_quarter_report.Rmd creates docs/first_quarter_report.pdf\n\n\nExported data files (i.e., RDS, RData, CSV, Excel, etc.)\n\nWe typically save them in a separate “data” folder under the project home directory.\nWhenever possible, we try to set the Word or PDF file name to match the name of the R file that it was created in.\n\nExample: data_03_texas_only.Rmd creates data/data_03_texas_only.csv\n\n\n\n\n\n\n\n1. Wickham H. Style guide. In: Advanced R.; 2019.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Coding Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/using_pipes/using_pipes.html",
    "href": "chapters/using_pipes/using_pipes.html",
    "title": "11  Using Pipes",
    "section": "",
    "text": "11.1 What are pipes?\n🤔 What are pipes? This |&gt; is the pipe operator. As of version 4.1, the pipe operator is part of base R. Prior to version 4.1, the pipe operator was only available from the magrittr. The pipe imported from the magrittr package looked like %&gt;% and you may still come across it in R code – including in this book.\n🤔 What does the pipe operator do? In our opinion, the pipe operator makes your R code much easier to read and understand.\n🤔 How does it do that? It makes your R code easier to read and understand by allowing you to view your nested functions in the order you want them to execute, as opposed to viewing them literally nested inside of each other.\nYou were first introduced to nesting functions in the Let’s get programming chapter. Recall that functions return values, and the R language allows us to directly pass those returned values into other functions for further calculations. We referred to this as nesting functions and said it was a big deal because it allows us to do very complex operations in a scalable way, without storing a bunch of unneeded intermediate objects in our global environment.\nIn that chapter, we also discussed a potential downside of nesting functions. Namely, our R code can become really difficult to read when we start nesting lots of functions inside one another.\nPipes allow us to retain the benefits of nesting functions without making our code really difficult to read. At this point, we think it’s best to show you an example. In the code below we want to generate a sequence of numbers, then we want to calculate the log of each of the numbers, and then find the mean of the logged values.\n# Performing an operation using a series of steps.\nmy_numbers &lt;- seq(from = 2, to = 100, by = 2)\nmy_numbers_logged &lt;- log(my_numbers)\nmean_my_numbers_logged &lt;- mean(my_numbers_logged)\nmean_my_numbers_logged\n\n[1] 3.662703\n👆 Here’s what we did above:\nThe obvious first question here is, “why would I ever want to do that?” Good question! You probably won’t ever want to do what we just did in the code chunk above, but we haven’t learned many functions for working with real data yet and we don’t want to distract you with a bunch of new functions right now. Instead, we want to demonstrate what pipes do. So, we’re stuck with this silly example.\n👍 What’s nice about the code above? We would argue that it is pretty easy to read because each line does one thing and it follows a series of steps in logical order. First, create the numbers. Second, log the numbers. Third, get the mean of the logged numbers.\n👎 What could be better about the code above? All we really wanted was the mean value of the logged numbers (i.e., mean_my_numbers_logged); however, on our way to getting mean_my_numbers_logged we also created two other objects that we don’t care about – my_numbers and my_numbers_logged. It took us time to do the extra typing required to create those objects, and those objects are now cluttering up our global environment. It may not seem like that big of a deal here, but in a real data analysis project these things can really add up.\nNext, let’s try nesting these functions instead:\n# Performing an operation using nested functions.\nmean_my_numbers_logged &lt;- mean(log(seq(from = 2, to = 100, by = 2)))\nmean_my_numbers_logged\n\n[1] 3.662703\n👆Here’s what we did above:\n👍 What’s nice about the code above? It is certainly more efficient than the sequential step method we used at first. We went from using 4 lines of code to using 2 lines of code, and we didn’t generate any unneeded objects.\n👎 What could be better about the code above? Many people would say that this code is harder to read than than the the sequential step method we used at first. This is primarily due to the fact that each line no longer does one thing, and the code no longer follows a sequence of steps from start to finish. For example, the final operation we want to do is calculate the mean, but the mean() function is the first function we see when we read the code.\nFinally, let’s try see what this code looks like when we use pipes:\n# Performing an operation using pipes.\nmean_my_numbers_logged &lt;- seq(from = 2, to = 100, by = 2) |&gt; \n  log() |&gt; \n  mean()\nmean_my_numbers_logged\n\n[1] 3.662703\n👆Here’s what we did above:\n👏 As you can see, by using pipes we were able to retain the benefits of performing the operation in a series of steps (i.e., each line of code does one thing and they follow in sequential order) and the benefits of nesting functions (i.e., more efficient code).\nThe utility of the pipe operator may not be immediately apparent to you based on this very simple example. So, next we’re going to show you a little snippet of code from one of our research projects. In the code chunk that follows, the operation we’re trying to perform on the data is written in two different ways – without pipes and with pipes. It’s very unlikely that you will know what this code does, but that isn’t really the point. Just try to get a sense of which version is easier for you to read.\n# Nest functions without pipes\nresponses &lt;- select(ungroup(filter(group_by(filter(merged_data, !is.na(incident_number)), incident_number), row_number() == 1)), date_entered, detect_data, validation)\n\n# Nest functions with pipes\nresponses &lt;- merged_data |&gt; \n  filter(!is.na(incident_number)) |&gt; \n  group_by(incident_number) |&gt; \n  filter(row_number() == 1) |&gt; \n  ungroup() |&gt; \n  select(date_entered, detect_data, validation)\nWhat do you think? Even without knowing what this code does, do you feel like one version is easier to read than the other?",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using Pipes</span>"
    ]
  },
  {
    "objectID": "chapters/using_pipes/using_pipes.html#what-are-pipes",
    "href": "chapters/using_pipes/using_pipes.html#what-are-pipes",
    "title": "11  Using Pipes",
    "section": "",
    "text": "We created a vector of numbers called my_numbers using the seq() function.\n\nThen we used the log() function to create a new vector of numbers called my_numbers_logged, which contains the log values of the numbers in my_numbers.\n\nThen we used the mean() function to create a new vector called mean_my_numbers_logged, which contains the mean of the log values in my_numbers_logged.\n\nFinally, we printed the value of mean_my_numbers_logged to the screen to view.\n\n\n\n\n\n\n\n\nWe created a vector of numbers called mean_my_numbers_logged by nesting the seq() function inside of the log() function and nesting the log() function inside of the mean() function.\nThen, we printed the value of mean_my_numbers_logged to the screen to view.\n\n\n\n\n\n\n\nWe created a vector of numbers called mean_my_numbers_logged by passing the result of the seq() function directly to the log() function using the pipe operator, and passing the result of the the log() function directly to the mean() function using the pipe operator.\nThen, we printed the value of mean_my_numbers_logged to the screen to view.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using Pipes</span>"
    ]
  },
  {
    "objectID": "chapters/using_pipes/using_pipes.html#how-do-pipes-work",
    "href": "chapters/using_pipes/using_pipes.html#how-do-pipes-work",
    "title": "11  Using Pipes",
    "section": "11.2 How do pipes work?",
    "text": "11.2 How do pipes work?\nPerhaps we’ve convinced you that pipes are generally useful. But, it may not be totally obvious to you how to use them. They are actually really simple. Start by thinking about pipes as having a left side and a right side.\n\n\n\n\n\n\n\n\nFigure 11.1: Pipes have a left side and a right side.\n\n\n\n\n\nThe thing on the right side of the pipe operator should always be a function.\n\n\n\n\n\n\n\n\nFigure 11.2: A function should always be to the right of the pipe operator.\n\n\n\n\n\nThe thing on the left side of the pipe operator can be a function or an object.\n\n\n\n\n\n\n\n\nFigure 11.3: A function or an object can be to the left of the pipe operator.\n\n\n\n\n\nAll the pipe operator does is take the thing on the left side and pass it to the first argument of the function on the right side.\n\n\n\n\n\n\n\n\nFigure 11.4: Pipe the left side to the first argument of the function on the right side.\n\n\n\n\n\nIt’s a really simple concept, but it can also cause people a lot of confusion at first. So, let’s take look at a couple more concrete examples.\nBelow we pass a vector of numbers to the to the mean() function, which returns the mean value of those numbers to us.\n\nmean(c(2, 4, 6, 8))\n\n[1] 5\n\n\nWe can also use a pipe to pass that vector of numbers to the mean() function.\n\nc(2, 4, 6, 8) |&gt; mean()\n\n[1] 5\n\n\nSo, the R interpreter took the thing on the left side of the pipe operator, stuck it into the first argument of the function on the right side of the pipe operator, and then executed the function. In this case, the mean() function doesn’t require any other arguments, so we don’t have to write anything else inside of the mean() function’s parentheses. When we see c(2, 4, 6, 8) |&gt; mean(), R sees mean(c(2, 4, 6, 8))\nHere’s one more example. Pretty soon we will learn how to use the filter() function from the dplyr package to keep only a subset of rows from our data frame. Let’s start by simulating some data:\n\n# Simulate some data\nheight_and_weight &lt;- tibble(\n  id     = c(\"001\", \"002\", \"003\", \"004\", \"005\"),\n  sex    = c(\"Male\", \"Male\", \"Female\", \"Female\", \"Male\"),\n  ht_in  = c(71, 69, 64, 65, 73),\n  wt_lbs = c(190, 176, 130, 154, 173)\n)\n\nheight_and_weight\n\n# A tibble: 5 × 4\n  id    sex    ht_in wt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 001   Male      71    190\n2 002   Male      69    176\n3 003   Female    64    130\n4 004   Female    65    154\n5 005   Male      73    173\n\n\nIn order to work, the filter() function requires us to pass two values to it. The first value is the name of the data frame object with the rows we want to subset. The second is the condition used to subset the rows. Let’s say that we want to do a subgroup analysis using only the females in our data frame. We could use the filter() function like so:\n\n# First value = data frame name (height_and_weight)\n# Second value = condition for keeping rows (when the value of sex is Female)\nfilter(height_and_weight, sex == \"Female\")\n\n# A tibble: 2 × 4\n  id    sex    ht_in wt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 003   Female    64    130\n2 004   Female    65    154\n\n\n👆Here’s what we did above:\n\nWe kept only the rows from the data frame called height_and_weight that had a value of Female for the variable called sex using dplyr’s filter() function.\n\nWe can also use a pipe to pass the height_and_weight data frame to the filter() function.\n\n# First value = data frame name (height_and_weight)\n# Second value = condition for keeping rows (when the value of sex is Female)\nheight_and_weight |&gt; filter(sex == \"Female\")\n\n# A tibble: 2 × 4\n  id    sex    ht_in wt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 003   Female    64    130\n2 004   Female    65    154\n\n\nAs you can see, we get the exact same result. So, the R interpreter took the thing on the left side of the pipe operator, stuck it into the first argument of the function on the right side of the pipe operator, and then executed the function. In this case, the filter() function needs a value supplied to two arguments in order to work. So, we wrote sex == \"Female\" inside of the filter() function’s parentheses. When we see height_and_weight |&gt; filter(sex == \"Female\"), R sees filter(height_and_weight, sex == \"Female\").\n\n\n\n\n\n\nNote\n\n\n\nThis pattern – a data frame piped into a function, which is usually then piped into one or more additional functions is something that you will see over and over in this book.\n\n\nDon’t worry too much about how the filter() function works. That isn’t the point here. The two main takeaways so far are:\n\nPipes make your code easier to read once you get used to them.\nThe R interpreter knows how to automatically take whatever is on the left side of the pipe operator and make it the value that gets passed to the first argument of the function on the right side of the pipe operator.\n\n\n11.2.1 Keyboard shortcut\nTyping |&gt; over and over can be tedious! Thankfully, RStudio provides a keyboard shortcut for inserting the pipe operator into your R code.\nOn Mac type shift + command + m.\nOn Windows type shift + control + m\nIt may not seem totally intuitive at first, but this shortcut is really handy once you get used to it.\n\n\n11.2.2 Pipe style\nAs with all the code we write, style is an important consideration. We generally agree with the recommendations given in the Tidyverse style guide. In particular:\n\nWe tend to use pipes in such a way that each line of code does one, and only one, thing.\nIf a line of code contains a pipe operator, the pipe operator should generally be the last thing typed on the line.\nThe pipe operator should always have a space in front of it.\nIf the pipe operator isn’t the last thing typed on the line, then it should be have a space after it too.\n“If the function you’re piping into has named arguments (like mutate() or summarize()), put each argument on a new line. If the function doesn’t have named arguments (like select() or filter()), keep everything on one line unless it doesn’t fit, in which case you should put each argument on its own line.”1\n“After the first step of the pipeline, indent each line by two spaces. RStudio will automatically put the spaces in for you after a line break following a |&gt; . If you’re putting each argument on its own line, indent by an extra two spaces. Make sure ) is on its own line, and un-indented to match the horizontal position of the function name.”1\n\nEach of these recommendations are demonstrated in the code below.\n\n# Do this...\nfemale_height_and_weight &lt;- height_and_weight |&gt; # Line 1\n  filter(sex == \"Female\") |&gt;                     # Line 2\n  summarise(                                     # Line 3\n    mean_ht = mean(ht_in),                       # Line 4\n    sd_ht   = sd(ht_in)                          # Line 5\n  ) |&gt;                                           # Line 6\n  print()                                        # Line 7\n\n# A tibble: 1 × 2\n  mean_ht sd_ht\n    &lt;dbl&gt; &lt;dbl&gt;\n1    64.5 0.707\n\n\nIn the code above, we would first like you to notice that each line of code does one, and only one, thing. Line 1 only assigns the result of the code pipeline to a new object – female_height_and_weight, line 2 only keeps the rows in the data frame we want – rows for females, line 3 only opens the summarise() function, line 4 only calculates the mean of the ht_in column, line 5 only calculates the standard deviation of the ht_in column, line 6 only closes the summarise() function, and line 7 only prints the result to the screen.\nSecond, we’d like you to notice that each line containing a pipe operator (i.e., lines 1, 2, and 6) ends with the pipe operator, and the pipe operators all have a space in front of them.\nThird, we’d like you to notice that each named argument in the summarise() function is written on its own line (i.e., lines 4 and 5).\nFinally, we’d like you notice that each step of the pipeline is indented two spaces (i.e., lines 2, 3, 6, and 7), lines 4 and 5 are indented an additional two spaces because they contain named arguments to the summarise() function, and that the summarise() function’s closing parenthesis is on its own line (i.e., line 6), horizontally aligned with the “s” in “summarise(”.\nNow compare that with the code in the code chunk below.\n\n# Avoid this...\nfemale_height_and_weight &lt;- height_and_weight |&gt; filter(sex == \"Female\") |&gt; \n  summarise(mean_ht = mean(ht_in), sd_ht = sd(ht_in)) |&gt; print()  \n\n# A tibble: 1 × 2\n  mean_ht sd_ht\n    &lt;dbl&gt; &lt;dbl&gt;\n1    64.5 0.707\n\n\nAlthough we get the same result as before, most people would agree that the code is harder to quickly glance at and read. Further, most people would also agree that it would be more difficult to add or rearrange steps when the code is written that way. As previously stated, there is a certain amount of subjectivity in what constitutes “good” style. But, we will once again reiterate that it is important to adopt some style and use it consistently. If you are a beginning R programmer, why not adopt the tried-and-true styles suggested here and adjust later if you have a compelling reason to do so?",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using Pipes</span>"
    ]
  },
  {
    "objectID": "chapters/using_pipes/using_pipes.html#final-thought-on-pipes",
    "href": "chapters/using_pipes/using_pipes.html#final-thought-on-pipes",
    "title": "11  Using Pipes",
    "section": "11.3 Final thought on pipes",
    "text": "11.3 Final thought on pipes\nWe think it’s important to note that not everyone in the R programming community is a fan of using pipes. We hope that we’ve made a compelling case for why we use pipes, but we acknowledge that it is ultimately a preference, and that using pipes is not the best choice in all circumstances. Whether or not you choose to use the pipe operator is up to you; however, we will be using them extensively throughout the remainder of this book.\n\n\n\n\n1. Wickham H, Çetinkaya-Rundel M, Grolemund G. Workflow: Code style. In: R for Data Science. second.; 2023.",
    "crumbs": [
      "Coding Tools and Best Practices",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using Pipes</span>"
    ]
  },
  {
    "objectID": "chapters/data_transfer/data_transfer.html",
    "href": "chapters/data_transfer/data_transfer.html",
    "title": "12  Introduction to Data Transfer",
    "section": "",
    "text": "In previous chapters, we learned how to write our own simple R programs by directly creating data frames in RStudio with the data.frame() function, the tibble() function, and the tribble() function. We consider this to be a really fundamental skill to master because it allows us to simulate data and it allows us to get data into R regardless of what format that data is stored in (assuming we can “see” the stored data). In other words, if nothing else, we can always resort to creating data frames this way.\nIn practice, however, this is not how people generally exchange data. You might recall that in Section 2.2.1 Transferring data We briefly mentioned the need to get data into R that others have stored in various different file types. These file types are also sometimes referred to as file formats. Common examples encountered in epidemiology include database files, spreadsheets, text files, SAS data sets, and Stata data sets.\n\n\n\n\n\n\n\n\n\nFurther, the data frames we’ve created so far don’t currently live in our global environment from one programming session to the next. We haven’t yet learned how to efficiently store our data long-term. We think the limitations of having to manually create a data frame every time we start a new programming session are probably becoming obvious to you at this point.\nIn this part of the book, we will learn to import data stored in various different file types into R for data management and analysis, we will learn to store R data frames in a more permanent way so that we can come back later to modify or analyze them, and we will learn to export data so that we may efficiently share it with others.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Data Transfer</span>"
    ]
  },
  {
    "objectID": "chapters/file_paths/file_paths.html",
    "href": "chapters/file_paths/file_paths.html",
    "title": "13  File Paths",
    "section": "",
    "text": "13.1 Finding file paths\nIn this part of the book, we will need to work with file paths. File paths are nothing more than directions that tell R where to find, or place, data on our computer. In our experience, however, some students are a little bit confused about file paths at first. So, in this chapter we will briefly introduce what file paths are and how to find the path to a specific file on our computer.\nLet’s say that we want you to go to the store and buy a loaf of bread.\nWhen we say, “go to the store”, this is really a shorthand way of telling you a much more detailed set of directions.\nNot only do you need to do all of the steps in the directions above, but you also need to use the exact sequence above in order to arrive at the desired destination.\nFile paths aren’t so different. If we want R to “go get” the file called my_study_data.csv, we have to give it directions to where that file is located. But the file’s location is not a geographic location that involves making left and right turns. Rather, it is a location in your computer’s file system that involves moving deeper into folders that are nested inside one another.\nFor example, let’s say that we have a folder on our desktop called “NTRHD” for “North Texas Regional Health Department.\nAnd, my_study_data.csv is inside the NTRHD folder.\nWe can give R directions to that data using the following path:\n/Users/bradcannell/Desktop/NTRHD/my_study_data.csv (On Mac)\nOR\nC:/Users/bradcannell/Desktop/NTRHD/my_study_data.csv (On Windows)\nThese directions may be read in a more human-like way by replacing the slashes with “and then”. For example, /Users/bradcannell/Desktop/NTRHD/my_study_data.csv can be read as “starting at the computer’s home directory, go into files that are accessible to the username bradcannell, and then go into the folder called Desktop, and then go into the folder called NTRHD, and then get the file called my_study_data.csv.”\nSelf Quiz:\nLet’s say that we move my_study_data.csv to a different folder on our desktop called research. What file path would we need to give R to tell it how to find the data?\n/Users/bradcannell/Desktop/research/my_study_data.csv (On Mac)\nOR\nC:/Users/bradcannell/Desktop/research/my_study_data.csv (On Windows)\nNow let’s say that we created a new folder inside of the research folder on our desktop called my studies. Now what file path would we need to give R to tell it how to find the data?\n/Users/bradcannell/Desktop/research/my studies/my_study_data.csv (On Mac)\nOR\nC:/Users/bradcannell/Desktop/research/my studies/my_study_data.csv (On Windows)\nNow that we know how file paths are constructed, we can always type them manually. However, typing file paths manually is tedious and error prone. Luckily, both Windows and MacOS have shortcuts that allow us to easily copy and paste file paths into R.\nOn a Mac, we right-click on the file we want the path for and a drop-down menu will appear. Then, click the Get Info menu option.\nNow, we just copy the file path in the Where section of the get info window and paste it into our R code.\nAlternatively, as shown below, we can right click on the file we want the path for to open the same drop-down menu shown above. But, if we hold down the alt/option key the Copy menu option changes to Copy ... as Pathname. We can then left-click that option to copy the path and paste it into our R code.\nFigure 13.1: A gif about file paths.\nA similar method exists in Windows as well. First, we hold down the shift key and right click on the file we want the path for. Then, we click Copy as path in the drop-down menu that appears and paste the file path into our R code.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>File Paths</span>"
    ]
  },
  {
    "objectID": "chapters/file_paths/file_paths.html#relative-file-paths",
    "href": "chapters/file_paths/file_paths.html#relative-file-paths",
    "title": "13  File Paths",
    "section": "13.2 Relative file paths",
    "text": "13.2 Relative file paths\nAll of the file paths we’ve seen so far in this chapter are absolute file paths (as opposed to relative file paths). In this case, absolute just means that the file path begins with the computer’s home directory. Remember, that the home directory in the examples above was /Users/bradcannell. When we are collaborating with other people, or sometimes even when we use more than one computer to work on our projects by ourselves, this can problematic. Pause here for a moment and think about why that might be…\nUsing absolute file paths can be problematic because the home directory can be different on every computer we use and is almost certainly different on one of our collaborator’s computers. Let’s take a look at an example. In the screenshot below, we are importing an Excel spreadsheet called form_20.xlsx into R as an R data frame named df. Don’t worry about the import code itself. We will learn more about [importing Microsoft Excel spreadsheets][Importing Microsoft Excel spreadsheets] soon. For now, just look at the file path we are passing to the read_excel() function. By doing so, we are telling R where to go find the Excel file that we want to import. In this case, are we giving R an absolute or relative file path?\n\n\n\n\n\n\n\n\n\nWe are giving R an absolute file path. We know this because it starts with the home directory – /Users/bradcannell. Does our code work?\nYes! Our code does work. We can tell because there are no errors on the screen and the df object we created looks as we expect it to when we print it to the screen. Great!!\nNow, let’s say that our research assistant – Arthur Epi – is going to help us analyze this data as well. So, we share this code file with him. What do you think will happen when he runs the code on his computer?\n\n\n\n\n\n\n\n\n\nWhen Arthur tries to import this file on his computer using our code, he gets an error. The error tells him that the path /Users/bradcannell/Dropbox/02 Teaching/R4Epi Textbook/my_first_project/data/form_20.xlsx doesn’t exist. And on Arthur’s computer it doesn’t! The file form_20.xlsx exists, but not at the location /Users/bradcannell/Dropbox/02 Teaching/R4Epi Textbook/my_first_project/data/. This is because Arthur’s home directory is /Users/arthurepi not /Users/bradcannell. The directions are totally different!\nTo make this point clearer, let’s return to our directions to the store example from earlier in the chapter. In that example, we only gave one list of directions to the store.\n\n\n\n\n\n\n\n\n\nNotice that these directions assume that we are starting from our house. As long as we leave from our house, they work great! But what happens if we are at someone else’s house and we ask you to go to the store and buy a loaf of bread? You’d walk out the front door and immediately discover that the directions don’t make any sense! You’d think, “Camp Bowie Blvd.? Where is that? I don’t see that street anywhere!”\nDid the store disappear? No, of course not! The store is still there. It’s just that our directions to the store assume that we are starting from our house. If these directions were a file path, they would be an absolute file path. They start all the way from our home and only work from our home.\nSo, could Arthur just change the absolute file path to work on his computer? Sure! He could do that, but then the file path wouldn’t work on Brad’s computer anymore. So, could there just be two code chunks in the file – one for Brad’s computer and one for Arthur’s computer? Sure! We could do that, but then one code chunk or the other will always throw an error on someone’s computer. That will mean that we won’t ever be able to just run our R code in its entirety. We’ll have to run it chunk-by-chunk to make sure we skip the chunk that throws an error. And this problem would just be multiplied if we are working with 5, 10, or 15 other collaborators instead of just 1. So, is there a better solution?\nYes! A better solution is to use a relative file path. Returning to our directions to the store example, it would be like giving directions to the store from a common starting point that everyone knows.\n\n\n\n\n\n\n\n\n\nNotice that the directions are now from a common location, which isn’t somebody’s “home”. Instead, it’s the corner of Camp Bowie Blvd. and Hulen St. You could even say that the directions are now relative to a common starting place. Now, we can give these directions to anyone and they can use them as long as they can find the corner of Camp Bowie and Hulen! Relative file paths work in much the same way. We tell RStudio to anchor itself at a common location that exists on everyone’s computer and then all the directions are relative to that location. But, how can we do that? What location do all of our collaborators have on all of their computers?\nThe answer is our R project’s directory (i.e., folder)! In order to effectively use relative file paths in R, we start by creating an R project. If you don’t remember how to create R projects, this would be a good time to go back and review the [R projects] chapter.\nIn the screenshot below, we can see that our RStudio session is open in the context of our R project called my_first_project.\n\n\n\n\n\n\n\n\n\nIn that context, R starts looking for files in our R project folder – no matter where we put the R project folder on our computer.\nFor example, in the next screenshot, we can see that the R project folder we previously created) (arrow 1), which is called my_first_project, is located on a computer’s desktop. One way we can tell that it’s an R project is because it contains an R project file (arrow 2). We can also see that our R project now contains a folder, which contains an Excel file called form_20.xlsx (arrow 3). Finally, we can see that we we’ve added a new Quarto/ file called test_relative_links.Rmd (arrow 4). That file contains the code we wrote to import form_20.xlsx as an R data frame.\n\n\n\n\n\n\n\n\n\nBecause we are using an R project, we can tell R where to find form_20.xlsx using a relative file path. That is, we can give R directions that begin at the R project’s directory. Remember, that just means the folder containing the R project file. In this case, my_first_project. Pause here for a minute. With that starting point in mind, how would you tell R to find form_20.xlsx?\nWell, you would say, “go into the folder called data, and then get the file called form_20.xlsx.” Written as a file path, what would that look like?\nIt would look like data/form_20.xlsx. Let’s give it a try!\n\n\n\n\n\n\n\n\n\nIt works! We can tell because there are no errors on the screen and the df object we created looks as we expect it to when we print it to the screen.\nNow, let’s try it on Arthur’s computer and see what happens.\n\n\n\n\n\n\n\n\n\nAs you can see, the absolute path still doesn’t work on Arthur’s computer, but the relative path does! It may not be obvious to you now, but this makes collaborating so much easier!\nLet’s quickly recap what we needed to do to be able to use relative file paths.\n\nWe need to create an R project.\nWe needed to save our R code and our data inside of the R project directory.\nWe needed to share the R project folder with our collaborators. This part wasn’t shown, but it was implied. We could have shared our R project by email. We could have shared our R project by using a shared cloud-based file storage service like Dropbox, Google Drive, or OneDrive. Better yet, we could have shared our R project using a GitHub repository, which we will discuss later in the book.\nWe replaced all absolute file paths in our code with relative file paths. In general, we should always use relative file paths if at all possible. It makes our code easier to read and maintain, and it makes life so much easier for us when we collaborate with others!\n\nNow that we know what file paths are and how to find them, let’s use them to import and export data to and from R.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>File Paths</span>"
    ]
  },
  {
    "objectID": "chapters/importing_plain_text/importing_plain_text.html",
    "href": "chapters/importing_plain_text/importing_plain_text.html",
    "title": "14  Importing Plain Text Files",
    "section": "",
    "text": "14.1 Packages for importing data\nWe previously learned how to manually create a data frame in RStudio with the data.frame() function, the tibble() function, or the tribble() function. This will get the job done, but it’s not always very practical – particularly when you have larger data sets.\nAdditionally, others will usually share data with you that is already stored in a file of some sort. For our purposes, any file containing data that is not an R data frame is referred to as raw data. In my experience, raw data is most commonly shared as CSV (comma separated values) files or as Microsoft Excel files. CSV files will end with the .csv file extension and Excel files end with the .xls or .xlsx file extensions. But remember, generally speaking R can only manipulate and analyze data that has been imported into R’s global environment. In this lesson, you will learn how to take data stored in several different common types of files import them into R for use.\nThere are many different file types that one can use to store data. In this book, we will divide those file types into two categories: plain text files and binary files. Plain text files are simple files that you (a human) can directly read using only your operating system’s plain text editor (i.e., Notepad on Windows or TextEdit on Mac). These files usually end with the .txt file extension – one exception being the .csv extension. Specifically, in this chapter we will learn to import the following variations of plain text files:\nLater, we will discuss importing binary files. For now, you can think of binary files as more complex file types that can’t generally be read by humans without the use of special software. Some examples include Microsoft Excel spreadsheets, SAS data sets, and Stata data sets.\nBase R contains several functions that can be used to import plain text files; however, I’m going to use the readr package to import data in the examples that follow. Compared to base R functions for importing plain text files, readr:\nIf you would like to follow along, I suggest that you go ahead and install and load readr now.\nlibrary(readr)",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importing Plain Text Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_plain_text/importing_plain_text.html#packages-for-importing-data",
    "href": "chapters/importing_plain_text/importing_plain_text.html#packages-for-importing-data",
    "title": "14  Importing Plain Text Files",
    "section": "",
    "text": "Is roughly 10 times faster.\nDoesn’t convert character variables to factors by default.\nBehaves more consistently across operating systems and geographic locations.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importing Plain Text Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_plain_text/importing_plain_text.html#importing-space-delimited-files",
    "href": "chapters/importing_plain_text/importing_plain_text.html#importing-space-delimited-files",
    "title": "14  Importing Plain Text Files",
    "section": "14.2 Importing space delimited files",
    "text": "14.2 Importing space delimited files\nWe will start by importing data with values are separated by a single space. Not necessarily because this is the most common format you will encounter; in my experience it is not. But it’s about as simple as it gets, and other types of data are often considered special cases of files separated with a single space. So, it seems like a good place to start.\n\n\n\n\n\n\nNote\n\n\n\nIn programming lingo, it is common to use the word delimited interchangeably with the word separated. For example, you might say “values separated by a single space” or you might say “a file with space delimited values.”\n\n\n\n\n\n\n\n\n\n\n\nFor our first example we will import a text file with values separated by a single space. The contents of the file are the now familiar height and weight data.\nYou may click here to download this file to your computer.\n\nsingle_space &lt;- read_delim(\n  file = \"single_delimited.txt\",\n  delim = \" \"\n)\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (3): id, sex, ht_in\ndbl (1): wgt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsingle_space\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n1 001   Male   71        190\n2 002   Male   .         176\n3 003   Female 64        130\n4 004   Female 65        154\n\n\n👆Here’s what we did above:\n\nWe used readr’s read_delim() function to import a data set with values that are delimited by a single space. Those values were imported as a data frame, and we assigned that data frame to the R object called single_space.\nYou can type ?read_delim into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the read_delim() function is the file argument. The value passed to the file argument should be a file path that tells R where to find the data set on your computer.\nThe second argument to the read_delim() function is the delim argument. The value passed to the delim argument tells R what character separates each value in the data set. In this case, a single space separates the values. Note that we had to wrap the single space in quotation marks.\nThe readr package imported the data and printed a message giving us some information about how it interpreted column names and column types. In programming lingo, deciding how to interpret the data that is being imported is called parsing the data.\n\nBy default, readr will assume that the first row of data contains variable names and will try to use them as column names in the data frame it creates. In this case, that was a good assumption. We want the columns to be named id, sex, ht_in, and wgt_lbs. Later, we will learn how to override this default behavior.\nBy default, readr will try to guess what type of data (e.g., numbers, character strings, dates, etc.) each column contains. It will guess based on analyzing the contents of the first 1,000 rows of the data. In this case, readr’s guess was not entirely correct (or at least not what we wanted). readr correctly guessed that the variables id and sex should be character variables, but incorrectly guessed that ht_in should be a character variable as well. Below, we will learn how to fix this issue.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to always include the file extension in your file paths. For example, using “/single_delimited” instead of “/single_delimited.txt” above (i.e., no .txt) would have resulted in an error telling you that the filed does not exist.\n\n\n\n14.2.1 Specifying missing data values\nIn the previous example, readr guessed that the variable ht_in was a character variable. Take another look at the data and see if you can figure out why?\n\n\n\n\n\n\n\n\n\nDid you see the period in the third value of the third row? The period is there because this value is missing, and a period is commonly used to represent missing data. However, R represents missing data with the special NA value – not a period. So, the period is just a regular character value to R. When R reads the values in the ht_in column, it decides that it can easily turn the numbers into character values, but it doesn’t know how to turn the period into a number. So, the column is parsed as a character vector.\nBut as we said, this is not what we want. So, how do we fix it? Well, in this case, we will simply need to tell R that missing values are represented with a period in the data we are importing. We do that by passing that information to the na argument of the read_delim() function:\n\nsingle_space &lt;- read_delim(\n  file = \"single_delimited.txt\",\n  delim = \" \",\n  na = \".\"\n)\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): id, sex\ndbl (2): ht_in, wgt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsingle_space\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      NA     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nBy default, the value passed to the na argument of the read_delim() function is c(\"\", \"NA\"). This means that R looks for nothing (i.e., a value should be there but isn’t - this really doesn’t make sense when the delimiter is a single space) or an NA.\nWe told R to look for a period to represent missing data instead of a nothing or an NA by passing the period character to the na argument.\nIt’s important to note that changing the value of the na argument does not change the way R represents missing data in the data frame that is created. It only tells R how to identify missing values in the raw data that we are importing. In the R data frame that is created, missing data will still be represented with the special NA value.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importing Plain Text Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_plain_text/importing_plain_text.html#importing-tab-delimited-files",
    "href": "chapters/importing_plain_text/importing_plain_text.html#importing-tab-delimited-files",
    "title": "14  Importing Plain Text Files",
    "section": "14.3 Importing tab delimited files",
    "text": "14.3 Importing tab delimited files\nSometimes you will encounter plain text files that contain values separated by tab characters instead of a single space. Files like these may be called tab separated value or tsv files, or they may be called tab-delimited files.\n\n\n\n\n\n\n\n\n\nTo import tab separated value files in R, we use a variation of the same program we just saw. We just need to tell R that now the values in the data will be delimited by tabs instead of a single space.\nYou may click here to download this file to your computer.\n\ntab &lt;- read_delim(\n  file = \"tab.txt\",\n  delim = \"\\t\"\n)\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): id, sex\ndbl (2): ht_in, wgt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ntab\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nWe used readr’s read_delim() function to import a data set with values that are delimited by tabs. Those values were imported as a data frame, and we assigned that data frame to the R object called tab.\nTo tell R that the values are now separated by tabs, we changed the value we passed to the delim argument to \"\\t\". This is a special symbol that means “tab” to R.\n\nI don’t personally receive tab separated values files very often. But, apparently, they are common enough to warrant a shortcut function in the readr package. That is, instead of using the read_delim() function with the value of the delim argument set to \"\\t\", we can simply pass our file path to the read_tsv() function. Under the hood, the read_tsv() function does exactly the same thing as the read_delim() function with the value of the delim argument set to \"\\t\".\n\ntab &lt;- read_tsv(\"tab.txt\")\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): id, sex\ndbl (2): ht_in, wgt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ntab\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importing Plain Text Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_plain_text/importing_plain_text.html#importing-fixed-width-format-files",
    "href": "chapters/importing_plain_text/importing_plain_text.html#importing-fixed-width-format-files",
    "title": "14  Importing Plain Text Files",
    "section": "14.4 Importing fixed width format files",
    "text": "14.4 Importing fixed width format files\nYet another type of plain text file we will discuss is called a fixed width format or fwf file. Again, these files aren’t super common in my experience, but they can be sort of tricky when you do encounter them. Take a look at this example:\n\n\n\n\n\n\n\n\n\nAs you can see, a hallmark of fixed width format files is inconsistent spacing between values. For example, there is only one single space between the values 004 and Female in the fourth row. But, there are multiple spaces between the values 65 and 154. Therefore, we can’t tell R to look for a single space or tab to separate values. So, how do we tell R which characters (including spaces) go with which variable? Well, if you look closely you will notice that all variable values start in the same column. If you are wondering what I mean, try to imagine a number line along the top of the data:\n\n\n\n\n\n\n\n\n\nThis number line creates a sequence of columns across your data, with each column being 1 character wide. Notice that spaces are also considered a character with width just like any other. We can use these columns to tell R exactly which columns contain the values for each variable.\nYou may click here to download this file to your computer.\nNow, in this case we can just use readr’s read_table() function to import this data:\n\nfixed &lt;- read_table(\"fixed_width.txt\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  id = col_character(),\n  sex = col_character(),\n  ht_in = col_double(),\n  wgt_lbs = col_double()\n)\n\n\nWarning: 1 parsing failure.\nrow col  expected    actual              file\n  1  -- 4 columns 5 columns 'fixed_width.txt'\n\n\n\nfixed\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nWe used readr’s read_table() function to import data from a fixed width format file. Those values were imported as a data frame, and we assigned that data frame to the R object called fixed.\nYou can type ?read_table into your R console to view the help documentation for this function and follow along with the explanation below.\nBy default, the read_table() function looks for values to be separated by one or more columns of space.\n\nHowever, how could you import this data if there weren’t always spaces in between data values. For example:\n\n\n\n\n\n\n\n\n\nIn this case, the read_table() function does not give us the result we want.\n\nfixed &lt;- read_table(\"fixed_width_no_space.txt\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  id = col_character(),\n  sex = col_double(),\n  ht_inwgt_lbs = col_double()\n)\n\n\nWarning: 3 parsing failures.\nrow col  expected    actual                       file\n  1  -- 3 columns 4 columns 'fixed_width_no_space.txt'\n  3  -- 3 columns 2 columns 'fixed_width_no_space.txt'\n  4  -- 3 columns 2 columns 'fixed_width_no_space.txt'\n\n\n\nfixed\n\n# A tibble: 4 × 3\n  id            sex ht_inwgt_lbs\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 001Male        71          190\n2 002Male        69          176\n3 003Female64   130           NA\n4 004Female65   154           NA\n\n\nInstead, it parses the entire data set as a single character column. It does this because it can’t tell where the values for one variable stop and the values for the next variable start. However, because all the variables start in the same column, we can tell R how to parse the data correctly. We can actually do this in a couple different ways:\nYou may click here to download this file to your computer.\n\n14.4.1 Vector of column widths\nOne way to import this data is to tell R how many columns wide each variable is in the raw data. We do that like so:\n\nfixed &lt;- read_fwf(\n  file = \"fixed_width_no_space.txt\",\n  col_positions = fwf_widths(\n    widths    = c(3, 6, 5, 3),\n    col_names = c(\"id\", \"sex\", \"ht_in\", \"wgt_lbs\")\n  ),\n  skip = 1\n)\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (2): id, sex\ndbl (2): ht_in, wgt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nfixed\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nWe used readr’s read_fwf() function to import data from a fixed width format file. Those values were imported as a data frame, and we assigned that data frame to the R object called fixed.\nYou can type ?read_fwf into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the read_fwf() function is the file argument. The value passed to the file argument should be file path that tells R where to find the data set on your computer.\nThe second argument to the read_fwf() function is the the col_positions argument. The value passed to this argument tells R the width (i.e., number of columns) that belong to each variable in the raw data set. This information is actually passed to the col_positions argument directly from the fwf_widths() function. This is an example of nesting functions.\n\nThe first argument to the fwf_widths() function is the widths argument. The value passed to the widths argument should be a numeric vector of column widths. The column width of each variable should be calculated as the number of columns that contain the values for that variable. For example, take another look at the data with the imaginary number line:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll of the values for the variable id can be located within the first 3 columns of data. All of the values for the variable sex can be located within the next 6 columns of data. All of the values for the variable ht_in can be located within the next 5 columns of data. And, all of the values for the variable wgt_lbs can be located within the next 3 columns of data. Therefore, we pass the vector c(3, 6, 5, 3) to the widths argument.\n\n\nThe second argument to the fwf_widths() function is the col_names argument. The value passed to the col_names argument should be a character vector of column names.\n\n\n\n\nThe third argument of the read_fwf() function that we passed a value to is the skip argument. The value passed to the skip argument tells R how many rows to ignore before looking for data values in the raw data. In this case, we passed a value of one, which told R to ignore the first row of the raw data. We did this because the first row of the raw data contained variable names instead of data values, and we already gave R variable names in the col_names argument to the fwf_widths() function.\n\n\n\n14.4.2 Paired vector of start and end positions\nAnother way to import this data is to tell R how which columns each variable starts and stops at in the raw data. We do that like so:\n\nfixed &lt;- read_fwf(\n  file = \"fixed_width_no_space.txt\",\n  col_positions = fwf_positions(\n    start     = c(1, 4, 10, 15),\n    end       = c(3, 9, 11, 17),\n    col_names = c(\"id\", \"sex\", \"ht_in\", \"wgt_lbs\")\n  ),\n  skip = 1\n)\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (2): id, sex\ndbl (2): ht_in, wgt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nfixed\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nThis time, we passed column positions to the col_positions argument of read_fwf() directly from the fwf_positions() function.\n\nThe first argument to the fwf_positions() function is the start argument. The value passed to the start argument should be a numeric vector containing the first column that contains a value for each variable. For example, take another look at the data with the imaginary number line:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first column that contains part of the value for the variable id can be located in column 1 of data. The first column that contains part of the value for the variable sex can be located in column 4 of data. The first column that contains part of the value for the variable ht_in can be located in column 10 of data. And, the first column that contains part of the value for the variable wgt_lbs can be located in column 15 of data. Therefore, we pass the vector c(1, 4, 10, 15) to the start argument.\n\n\nThe second argument to the fwf_positions() function is the end argument. The value passed to the end argument should be a numeric vector containing the last column that contains a value for each variable. The last column that contains part of the value for the variable id can be located in column 3 of data. The last column that contains part of the value for the variable sex can be located in column 9 of data. The last column that contains part of the value for the variable ht_in can be located in column 11 of data. And, the last column that contains part of the value for the variable wgt_lbs can be located in column 17 of data. Therefore, we pass the vector c(3, 9, 11, 17) to the end argument.\n\n\nThe third argument to the fwf_positions() function is the col_names argument. The value passed to the col_names argument should be a character vector of column names.\n\n\n\n\n\n14.4.3 Using named arguments\nAs a shortcut, either of the methods above can be written using named vectors. All this means is that we basically combine the widths and col_names arguments to pass a vector of column widths, or we combine the start, end, and col_names arguments to pass a vector of start and end positions. For example:\nColumn widths:\n\nread_fwf(\n  file = \"fixed_width_no_space.txt\",\n  col_positions = fwf_cols(\n    id      = 3,\n    sex     = 6,\n    ht_in   = 5,\n    wgt_lbs = 3\n  ),\n  skip = 1\n)\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\nColumn positions:\n\nread_fwf(\n  file = \"fixed_width_no_space.txt\",\n  col_positions = fwf_cols(\n    id      = c(1, 3),\n    sex     = c(4, 9),\n    ht_in   = c(10, 11),\n    wgt_lbs = c(15, 17)\n  ),\n  skip = 1\n)\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (2): id, sex\ndbl (2): ht_in, wgt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importing Plain Text Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_plain_text/importing_plain_text.html#importing-comma-separated-values-files",
    "href": "chapters/importing_plain_text/importing_plain_text.html#importing-comma-separated-values-files",
    "title": "14  Importing Plain Text Files",
    "section": "14.5 Importing comma separated values files",
    "text": "14.5 Importing comma separated values files\nThe final type of plain text file that we will discuss is by far the most common type used in my experience. I’m talking about the comma separated values or csv file. Unlike space and tab separated values files, csv file names end with the .csv file extension. Although, csv files are plain text files that can be opened in plain text editors such as Notepad for Windows or TextEdit for Mac, many people view csv files in spreadsheet applications like Microsoft Excel, Numbers for Mac, or Google Sheets.\n\n\n\n\n\nA csv file viewed in a plain text editor.\n\n\n\n\n\n\n\n\n\nA csv file viewed in Microsoft Excel.\n\n\n\n\nImporting standard csv files into R with the readr package is easy and uses a syntax that is very similar to read_delim() and read_tsv(). In fact, in many cases we only have to pass the path to the csv file to the read_csv() function like so:\nYou may click here to download this file to your computer.\n\ncsv &lt;- read_csv(\"comma.csv\")\n\nRows: 4 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (3): id, ht_in, wt_lbs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ncsv\n\n# A tibble: 4 × 4\n     id sex    ht_in wt_lbs\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 Male      71    190\n2     2 Male      69    176\n3     3 Female    64    130\n4     4 Female    65    154\n\n\n👆Here’s what we did above:\n\nWe used readr’s read_csv() function to import a data set with values that are delimited by commas. Those values were imported as a data frame, and we assigned that data frame to the R object called csv.\nYou can type ?read_csv into your R console to view the help documentation for this function and follow along with the explanation below.\nLike read_tsv(), R is basically executing the read_delim() function with the value of the delim argument set to \",\" under the hood. You could also use the read_delim() function with the value of the delim argument set to \",\" if you wanted to.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importing Plain Text Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_plain_text/importing_plain_text.html#additional-arguments",
    "href": "chapters/importing_plain_text/importing_plain_text.html#additional-arguments",
    "title": "14  Importing Plain Text Files",
    "section": "14.6 Additional arguments",
    "text": "14.6 Additional arguments\nFor the most part, the data we imported in all of the examples above was relatively well behaved. What I mean by that is that the data basically “looked” like each of the read_ functions were expecting it to “look”. Therefore, we didn’t have to adjust many of the various read_ functions’ default values. The exception was changing the default value of the na argument to the read_delim() function. However, all of the read_ functions above have additional arguments that you may need to tweak on occasion. The two that I tend to adjust most often are the col_names and col_types arguments. It’s impossible for me to think of every scenario where you may need to do this, but I’ll walk through a basic example below, which should be sufficient for you to get the idea.\nTake a look at this csv file for a few seconds. It started as the same exact height and weight data we’ve been using, but I made a few changes. See if you can spot them all.\n\n\n\n\n\n\n\n\n\nWhen people record data in Microsoft Excel, they do all kinds of crazy things. In the screenshot above, I’ve included just a few examples of things I see all the time. For example:\n\nRow one contains generic variable names that don’t really serve much of a purpose.\nRow two is a blank line. I’m not sure why it’s there. Maybe the study staff finds it aesthetically pleasing?\nRow three contains some variable descriptions. These are actually useful, but they aren’t currently formatted in a way that makes for good variable names.\nRow 7, column D is a missing value. However, someone wrote the word “Missing” instead of leaving the cell blank.\nColumn E also contains some notes for the data collection staff that aren’t really part of the data.\n\nAll of the issues listed above are things we will have to deal with before we can analyze our data. Now, in this small data set we could just fix these issues directly in Microsoft Excel and then import the altered data into R with a simple call to read_csv() without adjusting any options. However, that this is generally a really bad idea.\n\n\n\n\n\n\nWarning\n\n\n\n\nI suggest that you don’t EVER alter your raw data. All kinds of crazy things happen with data and data files. If you keep your raw data untouched and in a safe place, worst case scenario you can always come back to it and start over. If you start messing with the raw data, then you may lose the ability to recover what it looked like in its original form forever. If you import the data into R before altering it then your raw data stays preserved\nIf you are going to make alterations in Excel prior to importing the data, I strongly suggest making a copy of the raw data first. Then, alter the copy before importing into R. But, even this can be a bad idea.\nIf you make alterations to the data in Excel then there is generally no record of those alterations. For example, let’s say you click in a cell and delete a value (maybe even by accident), and then send me the csv file. I will have no way of knowing that a value was deleted. When you alter the data directly in Excel (or any program that doesn’t require writing code), it can be really difficult for others (including future you) to know what was done to the data. You may be able manually compare the altered data to the original data if you have access to both, but who wants to do that – especially if the file is large? However, if you import the data into R as-is and programmatically make alterations with R code, then your R code will, by definition, serve a record of all alterations that were made.\nOften data is updated. You could spend a significant amount of time altering your data in Excel only to be sent an updated file next week. Often, the manual alterations you made in one Excel file are not transferable to another. However, if all alterations are made in R, then you can often just run the exact same code again on the updated data.\n\n\n\nSo, let’s walk through addressing these issues together. We’ll start by taking a look at our results with all of read_csv’s arguments left at their default values.\nYou may click here to download this file to your computer.\n\ncsv &lt;- read_csv(\"comma_complex.csv\")\n\nNew names:\nRows: 6 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(5): Var1...1, Var1...2, Var3, Var4, Notes\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Var1` -&gt; `Var1...1`\n• `Var1` -&gt; `Var1...2`\n\n\n\ncsv\n\n# A tibble: 6 × 5\n  Var1...1 Var1...2        Var3                   Var4                     Notes\n  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;                  &lt;chr&gt;                    &lt;chr&gt;\n1 &lt;NA&gt;     &lt;NA&gt;            &lt;NA&gt;                   &lt;NA&gt;                     &lt;NA&gt; \n2 Study ID Participant Sex Paticipant Height (in) Participant Weight (lbs) &lt;NA&gt; \n3 1        Male            71                     190                      &lt;NA&gt; \n4 2        Male            &lt;NA&gt;                   176                      &lt;NA&gt; \n5 3        Female          64                     130                      &lt;NA&gt; \n6 4        Female          65                     Missing                  Call…\n\n\nThat is obviously not what we wanted. So, let’s start adjusting some of read_csv()’s defaults – staring with the column names.\n\ncsv &lt;- read_csv(\n  file = \"comma_complex.csv\",\n  col_names = c(\"id\", \"sex\", \"ht_in\", \"wgt_lbs\")\n)\n\nRows: 7 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): id, sex, ht_in, wgt_lbs, X5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n# A tibble: 7 × 5\n  id       sex             ht_in                  wgt_lbs                  X5   \n  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;                  &lt;chr&gt;                    &lt;chr&gt;\n1 Var1     Var1            Var3                   Var4                     Notes\n2 &lt;NA&gt;     &lt;NA&gt;            &lt;NA&gt;                   &lt;NA&gt;                     &lt;NA&gt; \n3 Study ID Participant Sex Paticipant Height (in) Participant Weight (lbs) &lt;NA&gt; \n4 1        Male            71                     190                      &lt;NA&gt; \n5 2        Male            &lt;NA&gt;                   176                      &lt;NA&gt; \n6 3        Female          64                     130                      &lt;NA&gt; \n7 4        Female          65                     Missing                  Call…\n\n\n👆Here’s what we did above:\n\nWe passed a character vector of variable names to the col_names argument. Doing so told R to use the words in the character vector as column names instead of the values in the first row of the raw data (the default).\nBecause the character vector of names only contained 4 values, the last column was dropped from the data. R gives us a warning message to let us know. Specially, for each row it says that it was expecting 4 columns (because we gave it 4 column names), but actually found 5 columns. We’ll get rid of this message next.\n\n\ncsv &lt;- read_csv(\n  file = \"comma_complex.csv\",\n  col_names = c(\"id\", \"sex\", \"ht_in\", \"wgt_lbs\"),\n  col_types = cols(\n    col_character(),\n    col_character(),\n    col_integer(),\n    col_integer(),\n    col_skip()\n  )\n)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\ncsv\n\n# A tibble: 7 × 4\n  id       sex             ht_in wgt_lbs\n  &lt;chr&gt;    &lt;chr&gt;           &lt;int&gt;   &lt;int&gt;\n1 Var1     Var1               NA      NA\n2 &lt;NA&gt;     &lt;NA&gt;               NA      NA\n3 Study ID Participant Sex    NA      NA\n4 1        Male               71     190\n5 2        Male               NA     176\n6 3        Female             64     130\n7 4        Female             65      NA\n\n\n👆Here’s what we did above:\n\nWe told R explicitly what type of values we wanted each column to contain. We did so by nesting a col_ function for each column type inside the col() function, which is passed directly to the col-types argument.\nYou can type ?readr::cols into your R console to view the help documentation for this function and follow along with the explanation below.\nNotice various column types (e.g., col_character()) are functions, and that they are nested inside of the cols() function. Because they are functions, you must include the parentheses. That’s just how the readr package is designed.\nNotice that the last column type we passed to the col_types argument was col_skip(). This tells R to ignore the 5th column in the raw data (5th because it’s the 5th column type we listed). Doing this will get rid of the warning we saw earlier.\nYou can type ?readr::cols into your R console to see all available column types.\nBecause we told R explicitly what type of values we wanted each column to contain, R had to drop any values that couldn’t be coerced to the type we requested. More specifically, they were coerced to missing (NA). For example, the value Var3 that was previously in the first row of the ht_in column. It was coerced to NA because R does not know (nor do I) how to turn the character string “Var3” into an integer. R gives us a warning message about this.\n\nNext, let’s go ahead and tell R to ignore the first three rows of the csv file. They don’t contain anything that is of use to us at this point.\n\ncsv &lt;- read_csv(\n  file = \"comma_complex.csv\",\n  col_names = c(\"id\", \"sex\", \"ht_in\", \"wgt_lbs\"),\n  col_types = cols(\n    col_character(),\n    col_character(),\n    col_integer(),\n    col_integer(),\n    col_skip()\n  ),\n  skip = 3\n)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\ncsv\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;   &lt;int&gt;\n1 1     Male      71     190\n2 2     Male      NA     176\n3 3     Female    64     130\n4 4     Female    65      NA\n\n\n👆Here’s what we did above:\n\nWe told R to ignore the first three rows of the csv file by passing the value 3 to the skip argument.\nThe remaining warning above is R telling us that it still had to convert the word “Missing” to an NA in the 4th row of the wgt_lbs column because it didn’t know how to turn the word “Missing” into an integer. This is actually exactly what we wanted to happen, but we can get rid of the warning by explicitly adding the word “Missing” to the list of values R looks for in the na argument.\n\n\ncsv &lt;- read_csv(\n  file = \"comma_complex.csv\",\n  col_names = c(\"id\", \"sex\", \"ht_in\", \"wgt_lbs\"),\n  col_types = cols(\n    col_character(),\n    col_character(),\n    col_integer(),\n    col_integer(),\n    col_skip()\n  ),\n  skip = 3,\n  na = c(\"\", \"NA\", \"Missing\")\n)\n\n\ncsv\n\n# A tibble: 4 × 4\n  id    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;   &lt;int&gt;\n1 1     Male      71     190\n2 2     Male      NA     176\n3 3     Female    64     130\n4 4     Female    65      NA\n\n\nWow! This was kind of a long chapter! 🤯 But, you should now have the foundation you need to start importing data in R instead of creating data frames manually. At least as it pertains to data that is stored in plain text files. Next, we will learn how to import data that is stored in binary files. Most of the concepts we learned in this chapter will apply, but we will get to use a couple new packages 📦.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importing Plain Text Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_binary_files/importing_binary_files.html",
    "href": "chapters/importing_binary_files/importing_binary_files.html",
    "title": "15  Importing Binary Files",
    "section": "",
    "text": "15.1 Packages for importing data\nIn the last chapter we learned that there are many different file types that one can use to store data. We also learned how to use the readr package to import several different variations of plain text files into R.\nIn this chapter, we will focus on data stored in binary files. Again, you can think of binary files as being more complex than plain text files and accessing the information in binary files requires the use of special software. Some examples of binary files that we have frequently seen used in epidemiology include Microsoft Excel spreadsheets, SAS data sets, and Stata data sets. Below, we will learn how to import all three file types into R.\nTechnically, base R does not contain any functions that can be used to import the binary file types discussed above. However, the foreign package contains functions that may be used to import SAS data sets and Stata data sets, and is installed by default when you install R on your computer. Having said that, we aren’t going to use the foreign package in this chapter. Instead, we’re going to use the following packages to import data in the examples below. If you haven’t done so already, we suggest that you go ahead and install these packages now.\nlibrary(readxl)\nlibrary(haven)",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Binary Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_binary_files/importing_binary_files.html#packages-for-importing-data",
    "href": "chapters/importing_binary_files/importing_binary_files.html#packages-for-importing-data",
    "title": "15  Importing Binary Files",
    "section": "",
    "text": "readxl. We will use the readxl package to import Microsoft Excel files.\nhaven. We will use the haven package to import SAS and Stata data sets.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Binary Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_binary_files/importing_binary_files.html#importing-microsoft-excel-spreadsheets",
    "href": "chapters/importing_binary_files/importing_binary_files.html#importing-microsoft-excel-spreadsheets",
    "title": "15  Importing Binary Files",
    "section": "15.2 Importing Microsoft Excel spreadsheets",
    "text": "15.2 Importing Microsoft Excel spreadsheets\nWe probably sent data in Microsoft Excel files more than any other file format. Fortunately, the readxl package makes it really easy to import Excel spreadsheets into R. And, because that package is maintained by the same people who create the readr package that you have already seen, we think it’s likely that the readxl package will feel somewhat familiar right from the start.\nWe would be surprised if any of you had never seen an Excel spreadsheet before – they are pretty ubiquitous in the modern world – but we’ll go ahead and show a screenshot of our height and weight data in Excel for the sake of completeness.\n\n\n\n\n\n\n\n\n\nAll we have to do to import this spreadsheet into R as a data frame is passing the path to the excel file to the path argument of the read_excel() function.\nYou may click here to download this file to your computer.\n\nexcel &lt;- read_excel(\"excel.xlsx\")\n\n\nexcel\n\n# A tibble: 4 × 4\n  ID    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nWe used readxl’s read_excel() function to import a Microsoft Excel spreadsheet. That spreadsheet was imported as a data frame and we assigned that data frame to the R object called excel.\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to always include the file extension in your file paths. For example, using “/excel” instead of “/excel.xlsx” above (i.e., no .xlsx) would have resulted in an error telling you that the filed does not exist.\n\n\nFortunately for us, just passing the Excel file to the read_excel() function like this will usually “just work.” But, let’s go ahead and simulate another situation that is slightly more complex. Once again, we’ve received data from a team that is using Microsoft Excel to capture some study data.\n\n\n\n\n\n\n\n\n\nAs you can see, this data looks very similar to the csv file we previously imported. However, it looks like the study team has done a little more formatting this time. Additionally, they’ve added a couple of columns we haven’t seen before – date of birth and annual household income.\nAs a final little wrinkle, the data for this study is actually the second sheet in this Excel file (also called a workbook). The study team used the first sheet in the workbook as a data dictionary that looks like this:\n\n\n\n\n\n\n\n\n\nOnce again, we will have to deal with some of the formatting that was done in Excel before we can analyze our data in R.\nYou may click here to download this file to your computer.\nWe’ll start by taking a look at the result we get when we try to pass this file to the read_excel() function without changing any of read_excel()’s default values.\n\nexcel &lt;- read_excel(\"excel_complex.xlsx\")\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n\n\n\nexcel\n\n# A tibble: 8 × 3\n  `Height and Weight Study\\r\\nData Dictionary` ...2                        ...3 \n  &lt;chr&gt;                                        &lt;chr&gt;                       &lt;chr&gt;\n1 &lt;NA&gt;                                         &lt;NA&gt;                        &lt;NA&gt; \n2 Variable                                     Definition                  Type \n3 Study ID                                     Randomly assigned particip… Cont…\n4 Assigned Sex at Birth                        Sex the participant was as… Dich…\n5 Height (inches)                              Participant's height in in… Cont…\n6 Weight (lbs)                                 Participant's weight in po… Cont…\n7 Date of Birth                                Participant's date of birth Date \n8 Annual Household Income                      Participant's annual house… Cont…\n\n\nAnd, as we’re sure you saw coming, this isn’t the result we wanted. However, we can get the result we wanted by making a few tweaks to the default values of the sheet, col_names, col_types, skip, and na arguments of the read_excel() function.\n\nexcel &lt;- read_excel(\n  path = \"excel_complex.xlsx\",\n  sheet = \"Study Phase 1\",\n  col_names = c(\"id\", \"sex\", \"ht_in\", \"wgt_lbs\", \"dob\", \"income\"),\n  col_types = c(\n    \"text\",\n    \"text\",\n    \"numeric\",\n    \"numeric\",\n    \"date\",\n    \"numeric\",\n    \"skip\"\n  ),\n  skip = 3,\n  na = c(\"\", \"NA\", \"Missing\")\n)\n\n\nexcel\n\n# A tibble: 4 × 6\n  id    sex    ht_in wgt_lbs dob                 income\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dttm&gt;               &lt;dbl&gt;\n1 001   Male      71     190 1981-05-20 00:00:00  46000\n2 002   Male      NA     176 1990-08-16 00:00:00  67000\n3 003   Female    64     130 1980-02-21 00:00:00  49000\n4 004   Female    65      NA 1983-04-12 00:00:00  89000\n\n\nAs we said, the readr package and readxl package were developed by the same people. So, the code above looks similar to the code we used to import the csv file in the previous chapter. Therefore, we’re not going to walk through this code step-by-step. Rather, we’re just going to highlight some of the slight differences.\n\nYou can type ?read_excel into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the read_excel() function is the path argument. It serves the same purpose as the file argument to read_csv() – it just has a different name.\nThe sheet argument to the read_excel() function tells R which sheet of the Excel workbook contains the data you want to import. In this case, the study team named that sheet “Study Phase 1”. We could have also passed the value 2 to the sheet argument because “Study Phase 1” is the second sheet in the workbook. However, we suggest using the sheet name. That way, if the study team sends you a new Excel file next week with different ordering, you are less likely to accidentally import the wrong data.\nThe value we pass to the col_types argument is now a vector of character strings instead of a list of functions nested in the col() function.\n\nThe values that the col_types function will accept are \"skip\" for telling R to ignore a column in the spreadsheet, \"guess\" for telling R to guess the variable type, \"logical\" for logical (TRUE/FALSE) variables, “numeric” for numeric variables, \"date\" for date variables, \"text\" for character variables, and \"list\" for everything else.\nNotice that we told R to import income as a numeric variable. This caused the commas and dollar signs to be dropped. We did this because keeping the commas and dollar signs would have required us to make income a character variable (numeric variables can only include numbers). If we had imported income as a character variable, we would have lost the ability to perform mathematical operations on it. Remember, it makes no sense to “add” two words together. Later, we will show you how to add dollar signs and commas back to the numeric values if you want to display them in your final results.\n\nWe used the col_names, skip, and na arguments in exactly the same way we used them in the read_csv function.\n\nYou should be able to import most of the data stored in Excel spreadsheets with just the few options that we discussed above. However, there may be times were importing spreadsheets is even more complicated. If you find yourself in that position, we suggest that you first check out the readxl website here.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Binary Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_binary_files/importing_binary_files.html#importing-data-from-other-statistical-analysis-software",
    "href": "chapters/importing_binary_files/importing_binary_files.html#importing-data-from-other-statistical-analysis-software",
    "title": "15  Importing Binary Files",
    "section": "15.3 Importing data from other statistical analysis software",
    "text": "15.3 Importing data from other statistical analysis software\nMany applications designed for statistical analysis allow you to save data in a binary format. One reason for this is that binary data formats allow you to save metadata alongside your data values. Metadata is data about the data. Using our running example, the data is about the heights, weights, and other characteristics of our study participants. Metadata about this data might include information like when this data set was created, or value labels that make the data easier to read (e.g., the dollar signs in the income variable).\nIn our experience, you are slightly more likely to have problems importing binary files saved from other statistical analysis applications than plain text files. Perhaps because they are more complex, the data just seems to become corrupt and do other weird things more often than is the case with plain text files. However, in our experience, it is also the case that when we are able to import binary files created in other statistical analysis applications, doing so requires less adjusting of default values. In fact, we will usually only need to pass the file path to the correct read_ function.\nBelow, we will see some examples of importing binary files saved in two popular statistical analysis applications – SAS and Stata. We will use the haven package to import both.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Binary Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_binary_files/importing_binary_files.html#importing-sas-data-sets",
    "href": "chapters/importing_binary_files/importing_binary_files.html#importing-sas-data-sets",
    "title": "15  Importing Binary Files",
    "section": "15.4 Importing SAS data sets",
    "text": "15.4 Importing SAS data sets\nSAS actually allows users to save data in more than one type of binary format. Data can be saved as SAS data sets or as SAS Transport files. SAS data set file names end with the .sas7bdat file extension. SAS Transport file file names end with the .xpt file extension.\nIn order to import a SAS data set, we typically only need to pass the correct file path to haven’s read_sas() function.\nYou may click here to download this file to your computer.\n\nsas &lt;- read_sas(\"height_and_weight.sas7bdat\")\n\n\nsas\n\n# A tibble: 4 × 4\n  ID    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nWe used haven’s read_sas() function to import a SAS data set. That data was imported as a data frame and we assigned that data frame to the R object called sas.\n\nIn addition to SAS data sets, data that has been altered in SAS can also be saved as a SAS transport file. Some of the national, population-based public health surveys (e.g., BRFSS and NHANES) make their data publicly available in this format.\nYou can download the 2018 BRFSS data as a SAS Transport file here. About halfway down the webpage, there is a link that says, “2018 BRFSS Data (SAS Transport Format)”.\n\n\n\n\n\n\n\n\n\nClicking that link should download the data to your computer. Notice that the SAS Transport file is actually stored inside a zip file. You can unzip the file first if you would like, but you don’t even have to do that. Amazingly, you can pass the path to the zipped .xpt file directly to the read_xpt() function like so:\n\nbrfss_2018 &lt;- read_xpt(\"LLCP2018XPT.zip\")\n\n\nhead(brfss_2018)\n\n# A tibble: 6 × 275\n  `_STATE` FMONTH IDATE    IMONTH IDAY  IYEAR DISPCODE SEQNO     `_PSU` CTELENM1\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1        1      1 01052018 01     05    2018      1100 20180000… 2.02e9        1\n2        1      1 01122018 01     12    2018      1100 20180000… 2.02e9        1\n3        1      1 01082018 01     08    2018      1100 20180000… 2.02e9        1\n4        1      1 01032018 01     03    2018      1100 20180000… 2.02e9        1\n5        1      1 01122018 01     12    2018      1100 20180000… 2.02e9        1\n6        1      1 01112018 01     11    2018      1100 20180000… 2.02e9        1\n# ℹ 265 more variables: PVTRESD1 &lt;dbl&gt;, COLGHOUS &lt;dbl&gt;, STATERE1 &lt;dbl&gt;,\n#   CELLFON4 &lt;dbl&gt;, LADULT &lt;dbl&gt;, NUMADULT &lt;dbl&gt;, NUMMEN &lt;dbl&gt;, NUMWOMEN &lt;dbl&gt;,\n#   SAFETIME &lt;dbl&gt;, CTELNUM1 &lt;dbl&gt;, CELLFON5 &lt;dbl&gt;, CADULT &lt;dbl&gt;,\n#   PVTRESD3 &lt;dbl&gt;, CCLGHOUS &lt;dbl&gt;, CSTATE1 &lt;dbl&gt;, LANDLINE &lt;dbl&gt;,\n#   HHADULT &lt;dbl&gt;, GENHLTH &lt;dbl&gt;, PHYSHLTH &lt;dbl&gt;, MENTHLTH &lt;dbl&gt;,\n#   POORHLTH &lt;dbl&gt;, HLTHPLN1 &lt;dbl&gt;, PERSDOC2 &lt;dbl&gt;, MEDCOST &lt;dbl&gt;,\n#   CHECKUP1 &lt;dbl&gt;, EXERANY2 &lt;dbl&gt;, SLEPTIM1 &lt;dbl&gt;, CVDINFR4 &lt;dbl&gt;, …\n\n\n👆Here’s what we did above:\n\nWe used haven’s read_xpt() function to import a zipped SAS Transport File. That data was imported as a data frame and we assigned that data frame to the R object called brfss_2018.\nBecause this is a large data frame (437,436 observations and 275 variables), we used the head() function to print only the first 6 rows of the data to the screen.\n\nBut, this demonstration actually gets even cooler. Instead of downloading the SAS Transport file to our computer before importing it, we can actually sometimes import files, including SAS Transport files, directly from the internet.\nFor example, you can download the 2017-2018 NHANES demographic data as a SAS Transport file here\n\n\n\n\n\n\n\n\n\nIf you right-click on the link that says, “DEMO_J Data [XPT - 3.3 MB]”, you will see an option to copy the link address.\n\n\n\n\n\n\n\n\n\nClick “Copy Link Address” and then navigate back to RStudio. Now, all you have to do is paste that link address where you would normally type a file path into the read_xpt() function. When you run the code chunk, the read_xpt() function will import the NHANES data directly from the internet (assuming you are connected to the internet). 😲\n\nnhanes_demo &lt;- read_xpt(\"https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\")\n\n\nhead(nhanes_demo)\n\n# A tibble: 6 × 46\n   SEQN SDDSRVYR RIDSTATR RIAGENDR RIDAGEYR RIDAGEMN RIDRETH1 RIDRETH3 RIDEXMON\n  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 93703       10        2        2        2       NA        5        6        2\n2 93704       10        2        1        2       NA        3        3        1\n3 93705       10        2        2       66       NA        4        4        2\n4 93706       10        2        1       18       NA        5        6        2\n5 93707       10        2        1       13       NA        5        7        2\n6 93708       10        2        2       66       NA        5        6        2\n# ℹ 37 more variables: RIDEXAGM &lt;dbl&gt;, DMQMILIZ &lt;dbl&gt;, DMQADFC &lt;dbl&gt;,\n#   DMDBORN4 &lt;dbl&gt;, DMDCITZN &lt;dbl&gt;, DMDYRSUS &lt;dbl&gt;, DMDEDUC3 &lt;dbl&gt;,\n#   DMDEDUC2 &lt;dbl&gt;, DMDMARTL &lt;dbl&gt;, RIDEXPRG &lt;dbl&gt;, SIALANG &lt;dbl&gt;,\n#   SIAPROXY &lt;dbl&gt;, SIAINTRP &lt;dbl&gt;, FIALANG &lt;dbl&gt;, FIAPROXY &lt;dbl&gt;,\n#   FIAINTRP &lt;dbl&gt;, MIALANG &lt;dbl&gt;, MIAPROXY &lt;dbl&gt;, MIAINTRP &lt;dbl&gt;,\n#   AIALANGA &lt;dbl&gt;, DMDHHSIZ &lt;dbl&gt;, DMDFMSIZ &lt;dbl&gt;, DMDHHSZA &lt;dbl&gt;,\n#   DMDHHSZB &lt;dbl&gt;, DMDHHSZE &lt;dbl&gt;, DMDHRGND &lt;dbl&gt;, DMDHRAGZ &lt;dbl&gt;, …\n\n\n👆Here’s what we did above:\n\nWe used haven’s read_xpt() function to import a SAS Transport File directly from the NHANES website. That data was imported as a data frame and we assigned that data frame to the R object called nhanes_demo.\nBecause this is a large data frame (9,254 observations and 46 variables), we used the head() function to print only the first 6 rows of the data to the screen.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Binary Files</span>"
    ]
  },
  {
    "objectID": "chapters/importing_binary_files/importing_binary_files.html#importing-stata-data-sets",
    "href": "chapters/importing_binary_files/importing_binary_files.html#importing-stata-data-sets",
    "title": "15  Importing Binary Files",
    "section": "15.5 Importing Stata data sets",
    "text": "15.5 Importing Stata data sets\nFinally, we will import a Stata data set (.dta) to round out our discussion of importing data from other statistical analysis software packages. There isn’t much of anything new here – you could probably have even guessed how to do this without us showing you.\nYou may click here to download this file to your computer.\n\nstata &lt;- read_stata(\"height_and_weight.dta\")\n\n\nstata\n\n# A tibble: 4 × 4\n  ID    sex    ht_in wgt_lbs\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 001   Male      71     190\n2 002   Male      69     176\n3 003   Female    64     130\n4 004   Female    65     154\n\n\n👆Here’s what we did above:\n\nWe used haven’s read_stata() function to import a Stata data set. That data was imported as a data frame and we assigned that data frame to the R object called stata.\n\nYou now know how to write code that will allow you to import data stored in all of the file formats that we will use in this book, and the vast majority of formats that you are likely to encounter in your real-world projects. In the next section, We will introduce you to a tool in RStudio that makes importing data even easier.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Binary Files</span>"
    ]
  },
  {
    "objectID": "chapters/rstudio_import_tool/rstudio_import_tool.html",
    "href": "chapters/rstudio_import_tool/rstudio_import_tool.html",
    "title": "16  RStudio’s Data Import Tool",
    "section": "",
    "text": "In previous chapters, we learned how to programmatically import data into R. In this chapter, we will briefly introduce you to RStudio’s data import tool. Conceptually, we won’t be introducing anything you haven’t already seen before. We just want to make you aware of this tool, which can be a welcomed convenience at times.\nFor this example, we will use the import tool to help us import the same height and weight csv file we imported in the chapter on importing plain text files.\nYou may click here to download this file to your compter.\nTo open RStudio’s data import tool, click the Import Dataset dropdown menu near the top of the environment pane.\n\n\n\n\n\n\n\n\n\nNext, because this is a csv file, we will choose the From Text (readr) option from the dropdown menu. The difference between From Text (base) and From Text (readr) is that From Text (readr) will use functions from the readr package to import the data and From Text (base) will use base R functions to import the data.\n\n\n\n\n\n\n\n\n\nAfter you select a file type from the import tool dropdown menu, a separate data import window will open.\n\n\n\n\n\n\n\n\n\nAt this point, you should click the browse button to locate the file you want to import.\n\n\n\n\n\n\n\n\n\nDoing so will open your operating system’s file explorer window. Use that window to find and select the file you want to import. Again, we am using comma.csv for this demonstration.\n\n\n\n\n\n\n\n\n\nAfter selecting you file, there will be some changes in the data import window. Specifically,\n\nThe file path to the raw data you are importing will appear in the File/URL field.\nA preview of how R is currently parsing that data will appear in the Data Preview field.\nSome or all of the import options will become available for you to select or deselect.\nThe underlying code that R is currently using to import this data is displayed in the Code Preview window.\nThe copy to clipboard icon becomes clickable.\n\n\n\n\n\n\n\n\n\n\nImporting this simple data set doesn’t require us to alter many of the import options. However, we do want to point out that you can change the variable type by clicking in the column headers in the Data Preview field. After clicking, a dropdown menu will display that allows you to change variable types. This is equivalent to adjusting the default values passed to the col_types argument of the read_csv() function.\nWe will go ahead and change the ht_in and wgt_lbs variables from type double to type integer using the dropdown menu.\n\n\n\n\n\n\n\n\n\nAt this point, our data is ready for import. You can simply press the Import button in the bottom-right corner of the data import window. However, we are going to suggest that you don’t do that. Instead, we’re going to suggest that you click the clipboard icon to copy the code displayed in the Code Preview window and then click the Cancel button.\nNext, return to your R script or Quarto file and paste the code that was copied to your clipboard. At this point, you can run the code as though you wrote it. More importantly, this code is now a part of the record of how you conducted your data analysis. Further, if someone sends you an updated raw data set, you may only need to update the file path in your code instead of clicking around the data import tool again.\n\n\n\n\n\n\n\n\n\nThat concludes the portion of the book devoted to importing data. In the next chapter, we will discuss strategies for exporting data so that you can store it in a more long-term way and/or share it with others.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>RStudio's Data Import Tool</span>"
    ]
  },
  {
    "objectID": "chapters/exporting_data_to_disk/exporting_data_to_disk.html",
    "href": "chapters/exporting_data_to_disk/exporting_data_to_disk.html",
    "title": "17  Exporting Data",
    "section": "",
    "text": "17.1 Plain text files\nThe data frames we’ve created so far don’t currently live in our global environment from one programming session to the next because we haven’t yet learned how to efficiently store our data long-term. This limitation makes it difficult to share our data with others or even to come back later to modify or analyze our data ourselves. In this chapter, you will learn to export data from R’s memory to a file on your hard drive so that you may efficiently store it or share it with others. In the examples that follow, we’re going to use this simulated data.\n👆 Here’s what we did above:\nMost of readr’s read_ functions that were introduced in the importing plain text files chapter have a write_ counterpart that allow you to export data from R into a plain text file.\nAdditionally, all of havens read_ functions that were introduced in the importing binary files chapter have a write_ counterpart that allow you to export data from R into SAS, Stata, and SPSS binary file formats.\nInterestingly, readxl does not have a write_excel() function for exporting R data frames as .xls or .xlsx files. However, the importance of this is mitigated by the fact that Excel can open .csv files and readr contains a function (write_csv())for exporting data frames in the .csv file format. If you absolutely have to export your data frame as a .xls or .xlsx file, there are other R packages capable of doing so (e.g., xlsx).\nSo, with all these options what format should you choose? our answer to this sort of depends on the answers to two questions. First, will this data be shared with anyone else? Second, will we need any of the metadata that would be lost if we export this data to a plain text file?\nUnless you have a compelling reason to do otherwise, we’re going to suggest that you always export your R data frames as csv files if you plan to share your data with others. The reason is simple. They just work. we can think of many times when someone sent me a SAS or Stata data set and we wasn’t able to import it for some reason or the data didn’t import in the way that we expected it to. we don’t recall ever having that experience with a csv file. Further, every operating system and statistical analysis software application that we’re aware of is able to accept csv files. Perhaps for that reason, they have become the closest thing to a standard for data sharing that exists – at least that we’re aware of.\nExporting an R data frame to a csv file is really easy. The example below shows how to export our simulated demographic data to a csv file on our computer’s desktop:\nreadr::write_csv(demo, \"demo.csv\")\n👆Here’s what we did above:\nEven if you don’t plan on sharing your data, there is another benefit to saving your data as a csv file. That is, it’s easy to open the file and take a quick peek if you need to for some reason. You don’t have to open R and load the file. You can just find the file on your computer, double-click it, and quickly view it in your text editor or spreadsheet application of choice.\nHowever, there is a downside to saving your data frames to a csv file. In general, csv files don’t store any metadata, which can sometimes be a problem (or a least a pain). For example, if you’ve coerced several variables to factors, that information would not be preserved in the csv file. Instead, the factors will be converted to character strings. If you need to preserve metadata, then you may want to save you data frames in a binary format.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exporting Data</span>"
    ]
  },
  {
    "objectID": "chapters/exporting_data_to_disk/exporting_data_to_disk.html#plain-text-files",
    "href": "chapters/exporting_data_to_disk/exporting_data_to_disk.html#plain-text-files",
    "title": "17  Exporting Data",
    "section": "",
    "text": "We used readr’s write_csv() function to export a data frame called demo in our global environment to a csv file on our desktop called demo.csv.\nYou can type ?write_csv into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the write_csv() function is the x argument. The value passed to the x argument should be a data frame that is currently in our global environment.\nThe second argument to the write_csv() function is the path argument. The value passed to the path should be a file path telling R where to create the new csv file.\n\nYou name the csv file directly in the file path. Whatever name you write after the final slash in the file path is what the csv file will be named.\nAs always, make sure you remember to include the file extension in the file path.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exporting Data</span>"
    ]
  },
  {
    "objectID": "chapters/exporting_data_to_disk/exporting_data_to_disk.html#r-binary-files",
    "href": "chapters/exporting_data_to_disk/exporting_data_to_disk.html#r-binary-files",
    "title": "17  Exporting Data",
    "section": "17.2 R binary files",
    "text": "17.2 R binary files\nIn the chapter on importing binary files we mentioned that most statistical analysis software allows you to save your data in a binary file format. The primary advantage to doing so is that potentially useful metadata is stored alongside your analysis data. We were first introduced to factor vectors in [Let’s Get Programming] chapter. There, we saw how coercing some of your variables to factors can be useful. However, doing so requires R to store metadata along with the analysis data. That metadata would be lost if you were to export your data frame to a plain text file. This is an example of a time when we may want to consider exporting our data to a binary file format.\nR actually allows you to save your data in multiple different binary file formats. The two most popular are the .Rdata format and the .Rds format. we’re going to suggest that you use the .Rds format to save your R data frames. Exporting to this format is really easy with the readr package.\nThe example below shows how to export our simulated demographic data to an .Rds file on our computer’s desktop:\n\nreadr::write_rds(demo, \"demo.rds\")\n\n👆Here’s what we did above:\n\nWe used readr’s write_rds() function to export a data frame called demo in our globabl environment to an .Rds file on our desktop called demo.rds.\nYou can type ?write_rds into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the write_rds() function is the x argument. The value passed to the x argument should be a data frame that is currently in our global environment.\nThe second argument to the write_csv() function is the path argument. The value passed to the path should be a file path telling R where to create the new .Rds file.\n\nYou name the .Rds file directly in the file path. Whatever name you write after the final slash in the file path is what the .Rds file will be named.\nAs always, make sure you remember to include the file extension in the file path.\n\n\nTo load the .Rds data back into your global environment, simply pass the path to the .Rds file to readrs read_rds() function:\n\n\ndemo &lt;- readr::read_rds(\"demo.rds\")\n\nThere is a final thought we want to share on exporting data frames. When we got to the end of this chapter, it occurred to me that the way we wrote it may give the impression that that you must choose to export data frames as plain text files or binary files, but not both. That isn’t the case. we frequently export our data as a csv file that we can easily open and view and/or share with others, but also export it to an .Rds file that retains useful metadata we might need the next time we return to our analysis. we suppose there could be times that your files are so large that this is not an efficient strategy, but that is generally not the case in our projects.",
    "crumbs": [
      "Data Transfer",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exporting Data</span>"
    ]
  },
  {
    "objectID": "chapters/intro_descriptive_analysis/intro_descriptive_analysis.html",
    "href": "chapters/intro_descriptive_analysis/intro_descriptive_analysis.html",
    "title": "18  Introduction to Descriptive Analysis",
    "section": "",
    "text": "18.1 What is descriptive analysis and why would we do it?\nSo, we have all this data that tells us all this information about different traits or characteristics of the people for whom the data was collected. For example, if we collected data about the students in this course, we may have information about how tall you are, about what kind of insurance you have, and about what your favorite color is.\nstudent_id\nheight_in\ninsurance\ncolor\n\n\n\n\n1001\n64.96\nprivate\nblue\n\n\n1002\n67.93\nother\nyellow\n\n\n1003\n84.03\nnone\nred\nBut, unless you’re a celebrity, or under investigation for some reason, it’s unlikely that many people outside of your friends and family care to know any of this information about you, per se. Usually they want to know this information about the typical person in the population, or subpopulation, to which you belong. Or, they want to know more about the relationship between people who are like you in some way and some outcome that they are interested in.\nFor example: We typically aren’t interested in knowing that student 1002 (above) is 67.93 inches tall. We are typically more interested in knowing things like the average height of the class – [’r mean(height_in) |&gt; round(2)].\nBefore we can make any inferences or draw any conclusions, we must (or at least should) begin by conducting descriptive analysis of our data. This is also sometimes referred to as exploratory analysis. There are at least three reasons why we want to start with a descriptive analysis:",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/intro_descriptive_analysis/intro_descriptive_analysis.html#what-is-descriptive-analysis-and-why-would-we-do-it",
    "href": "chapters/intro_descriptive_analysis/intro_descriptive_analysis.html#what-is-descriptive-analysis-and-why-would-we-do-it",
    "title": "18  Introduction to Descriptive Analysis",
    "section": "",
    "text": "We can use descriptive analysis to uncover errors in our data.\n\nIt helps us understand the distribution of values in our variables.\n\nDescriptive analysis serve as a starting point for understanding relationships between our variables.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/intro_descriptive_analysis/intro_descriptive_analysis.html#what-kind-of-descriptive-analysis-should-we-perform",
    "href": "chapters/intro_descriptive_analysis/intro_descriptive_analysis.html#what-kind-of-descriptive-analysis-should-we-perform",
    "title": "18  Introduction to Descriptive Analysis",
    "section": "18.2 What kind of descriptive analysis should we perform?",
    "text": "18.2 What kind of descriptive analysis should we perform?\nWhen conducting descriptive analysis, the method you choose will depend on the type of data you’re analyzing. At the most basic level, variables can be described as numerical or categorical.\n\n\n\n\n\n\n\n\n\nNumeric variables can then be further divided into continuous and discrete - the distinction being whether the variable can take on a continuum of values, or only set of certain values.\n\n\n\n\n\n\n\n\n\nCategorical variables can be subdivided into ordinal or nominal variables - depending on whether or not the categories can logically be ordered in a meaningful way.\n\n\n\n\n\n\n\n\n\nFinally, for all types, and subtypes, of variables there are both numerical and graphical methods we can use for descriptive analysis.\n\n\n\n\n\n\n\n\n\nIn the exercises that follow you will be introduced to measures of frequency, measures of central tendency, and measures of dispersion. Then, you’ll learn various methods for estimating and interpreting these measures using R.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction to Descriptive Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html",
    "href": "chapters/categorical_variables/categorical_variables.html",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "",
    "text": "19.1 Factors\nWe’ll begin our discussion of descriptive statistics in the categorical half of our flow chart. Specifically, we’ll start by numerically describing categorical variables. As a reminder, categorical variables are variables whose values fit into categories.\nSome examples of categorical variables commonly seen in public health data are: sex, race or ethnicity, and level of educational attainment.\nNotice that there is no inherent numeric value to any of these categories. Having said that, we can, and often will, assign a numeric value to each category using R.\nThe two most common numerical descriptions of categorical variables are probably the frequency count (you will often hear this referred to as simply the frequency, the count, or the n) and the proportion or percentage (the percentage is just the proportion multiplied by 100).\nThe count is simply the number of observations, in this case people, which fall into each possible category.\nThe proportion is just the count divided by the total number of observations. In this example, 2 people out of 5 people (.40 or 40%) are in the Asian race category.\nThe remainder of this chapter is devoted to learning how to calculate frequency counts and percentages using R.\nWe first learned about factors in the Let’s Get Programming chapter. Before moving on to calculating frequency counts and percentages, we will discuss factors in slightly greater depth here. As a reminder, factors can be useful for representing categorical data in R. To demonstrate, let’s simulate a simple little data frame.\n# Load dplyr for tibble()\nlibrary(dplyr)\ndemo &lt;- tibble(\n  id  = c(\"001\", \"002\", \"003\", \"004\"),\n  age = c(30, 67, 52, 56),\n  edu = c(3, 1, 4, 2)\n)\n👆 Here’s what we did above:\nEach participant in our data frame has a value for edu – 1, 2, 3, or 4. The value they have for that variable corresponds to the highest level of formal education they have completed, which is split up into categories that we defined. We can see which category each person is in by viewing the data.\ndemo\n\n# A tibble: 4 × 3\n  id      age   edu\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 001      30     3\n2 002      67     1\n3 003      52     4\n4 004      56     2\nWe can see that person 001 is in category 3, person 002 is in category 1, and so on. This compact representation of the categories is convenient for data entry and data manipulation, but it also has an obvious limitation – what do these numbers mean? We defined what these values mean for you above, but if you didn’t have that information, or some kind of prior knowledge about the process that was used to gather this data, then you would likely have no idea what these numbers mean.\nNow, we could have solved that problem by making education a character vector from the beginning. For example:\ndemo &lt;- tibble(\n  id       = c(\"001\", \"002\", \"003\", \"004\"),\n  age      = c(30, 67, 52, 56),\n  edu      = c(3, 1, 4, 2),\n  edu_char = c(\n    \"Some college\", \"Less than high school\", \"College graduate\", \n    \"High school graduate\"\n  )\n)\n\ndemo\n\n# A tibble: 4 × 4\n  id      age   edu edu_char             \n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                \n1 001      30     3 Some college         \n2 002      67     1 Less than high school\n3 003      52     4 College graduate     \n4 004      56     2 High school graduate\nBut, this strategy also has a few limitations.\n👎 First, entering data this way requires more typing. Not such a big deal in this case because we only have 4 participants. But, imagine typing out the categories as character strings 10, 20, or 100 times. 😫\n👎 Second, R summarizes character vectors alphabetically by default, which may not be the ideal way to order some categorical variables.\n👎 Third, creating categorical variables in our data frame as character vectors limits us to inputting only observed values for that variable. However, there are cases when other categories are possible and just didn’t apply to anyone in our data. That information may be useful to know.\nAt this point, we’re going to show you how to coerce a variable to a factor in your data frame. Then, we will return to showing you how using factors can overcome some of the limitations outlined above.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html#factors",
    "href": "chapters/categorical_variables/categorical_variables.html#factors",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "",
    "text": "We created a data frame that is meant to simulate some demographic information about 4 hypothetical study participants.\nThe first variable (id) is the participant’s study id.\nThe second variable (age) is the participant’s age at enrollment in the study.\nThe third variable (edu) is the highest level of formal education the participant completed. Where:\n\n1 = Less than high school\n2 = High school graduate\n3 = Some college\n4 = College graduate\n\n\n\n\n\n\n\n\n\n\n\n\n\n19.1.1 Coerce a numeric variable\nThe code below shows one method for coercing a numeric vector into a factor.\n\n# Load dplyr for pipes and mutate()\nlibrary(dplyr)\n\n\ndemo &lt;- demo |&gt; \n  mutate(\n    edu_f = factor(\n      x      = edu,\n      levels = 1:4,\n      labels = c(\n        \"Less than high school\", \"High school graduate\", \"Some college\", \n        \"College graduate\"\n      )\n    )\n  )\n\ndemo\n\n# A tibble: 4 × 5\n  id      age   edu edu_char              edu_f                \n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                 &lt;fct&gt;                \n1 001      30     3 Some college          Some college         \n2 002      67     1 Less than high school Less than high school\n3 003      52     4 College graduate      College graduate     \n4 004      56     2 High school graduate  High school graduate \n\n\n👆Here’s what we did above:\n\nWe used dplyr’s mutate() function to create a new variable (edu_f) in the data frame called demo. The purpose of the mutate() function is to add new variables to data frames. We will discuss mutate() in greater detail later in the book.\n\nYou can type ?mutate into your R console to view the help documentation for this function and follow along with the explanation below.\nWe assigned this new data frame the name demo using the assignment operator (&lt;-).\nBecause we assigned it the name demo, our previous data frame named demo (i.e., the one that didn’t include edu_f) no longer exists in our global environment. If we had wanted to keep that data frame in our global environment, we would have needed to assign our new data frame a different name (e.g., demo_w_factor).\n\nThe first argument to the mutate() function is the .data argument. The value passed to the .data argument should be a data frame that is currently in our global environment. We passed the data frame demo to the .data argument using the pipe operator (|&gt;), which is why demo isn’t written inside mutate’s parentheses.\nThe second argument to the mutate() function is the ... argument. The value passed to the ... argument should be a name value pair. That means, a variable name, followed by an equal sign, followed by the values to be assigned to that variable name (name = value).\n\nThe name we passed to the ... argument was edu_f. This value tells R what to name the new variable we are creating.\n\nIf we had used the name edu instead, then the previous values in the edu variable would have been replaced with the new values. That is sometimes what you want to happen. However, when it comes to creating factors, we typically keep the numeric version of the variable in our data frame (e.g., edu) and add a new factor variable. We just often find that it can be useful to have both versions of the variable hanging around during the analysis process.\nWe also use the _f naming convention in our code. That means that when we create a new factor variable we name it the same thing the original variable was named with the addition of _f (for factor) at the end.\n\nIn this case, the value that will be assigned to the name edu_f will be the values returned by the factor() function. This is an example of nesting functions.\n\nWe used the factor() function to create a factor vector.\n\nYou can type ?factor into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the factor() function is the x argument. The value passed to the x argument should be a vector of data. We passed the edu vector to the x argument.\nThe second argument to the factor() function is the levels argument. This argument tells R the unique values that the new factor variable can take. We used the shorthand 1:4 to tell R that edu_f can take the unique values 1, 2, 3, or 4.\nThe third argument to the factor() function is the labels argument. The value passed to the labels argument should be a character vector of labels (i.e., descriptive text) for each value in the levels argument. The order of the labels in the character vector we pass to the labels argument should match the order of the values passed to the levels argument. For example, the ordering of levels and labels above tells R that 1 should be labeled with “Less than high school”, 2 should be labeled with “High school graduate”, etc.\n\n\nWhen we printed the data frame above, the values in edu_f looked the same as the character strings displayed in edu_char. Notice, however, that the variable type displayed below edu_char in the data frame above is &lt;chr&gt; for character. Alternatively, the variable type displayed below edu_f is &lt;fctr&gt;. Although, labels are used to make factors look like character vectors, they are still integer vectors under the hood. For example:\n\nas.numeric(demo$edu_char)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA NA\n\n\n\nas.numeric(demo$edu_f)\n\n[1] 3 1 4 2\n\n\nThere are two main reasons that you may want to use factors instead of character vectors at times:\n👍 First, R summarizes character vectors alphabetically by default, which may not be the ideal way to order some categorical variables. However, we can explicitly set the order of factor levels. This will be useful to us later when we analyze categorical variables. Here is a glimpse of things to come:\n\ntable(demo$edu_char)\n\n\n     College graduate  High school graduate Less than high school \n                    1                     1                     1 \n         Some college \n                    1 \n\n\n\ntable(demo$edu_f)\n\n\nLess than high school  High school graduate          Some college \n                    1                     1                     1 \n     College graduate \n                    1 \n\n\n👆Here’s what we did above:\n\nYou can type ?base::table into your R console to view the help documentation for this function and follow along with the explanation below.\nWe used the table() function to get a count of the number of times each unique value of edu_char appears in our data frame. In this case, each value appears one time. Notice that the results are returned to us in alphabetical order.\nNext, we used the table() function to get a count of the number of times each unique value of edu_f appears in our data frame. Again, each value appears one time. Notice, however, that this time the results are returned to us in the order that we passed to the levels argument of the factor() function above.\n\n👍 Second, creating categorical variables in our data frame as character vectors limits us to inputting only observed values for that variable. However, there are cases when other categories are possible and just didn’t apply to anyone in our data. That information may be useful to know. Factors allow us to tell R that other values are possible, even when they are unobserved in our data. For example, let’s add a fifth possible category to our education variable – graduate school.\n\ndemo &lt;- demo |&gt; \n  mutate(\n    edu_5cat_f = factor(\n      x      = edu,\n      levels = 1:5,\n      labels = c(\n        \"Less than high school\", \"High school graduate\", \"Some college\", \n        \"College graduate\", \"Graduate school\"\n      )\n    )\n  )\n\ndemo\n\n# A tibble: 4 × 6\n  id      age   edu edu_char              edu_f                 edu_5cat_f      \n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                 &lt;fct&gt;                 &lt;fct&gt;           \n1 001      30     3 Some college          Some college          Some college    \n2 002      67     1 Less than high school Less than high school Less than high …\n3 003      52     4 College graduate      College graduate      College graduate\n4 004      56     2 High school graduate  High school graduate  High school gra…\n\n\nNow, let’s use the table() function once again to count the number of times each unique level of edu_char appears in the data frame and the number of times each unique level of edu_5cat_f appears in the data frame:\n\ntable(demo$edu_char)\n\n\n     College graduate  High school graduate Less than high school \n                    1                     1                     1 \n         Some college \n                    1 \n\n\n\ntable(demo$edu_5cat_f)\n\n\nLess than high school  High school graduate          Some college \n                    1                     1                     1 \n     College graduate       Graduate school \n                    1                     0 \n\n\nNotice that R now tells us that the value Graduate school was possible but was observed zero times in the data.\n\n\n19.1.2 Coerce a character variable\nIt is also possible to coerce character vectors to factors. For example, we can coerce edu_char to a factor like so:\n\ndemo &lt;- demo |&gt; \n  mutate(\n    edu_f_from_char = factor(\n      x      = edu_char,\n      levels = c(\n        \"Less than high school\", \"High school graduate\", \"Some college\", \n        \"College graduate\", \"Graduate school\"\n      )\n    )\n  )\n\ndemo\n\n# A tibble: 4 × 7\n  id      age   edu edu_char              edu_f       edu_5cat_f edu_f_from_char\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                 &lt;fct&gt;       &lt;fct&gt;      &lt;fct&gt;          \n1 001      30     3 Some college          Some colle… Some coll… Some college   \n2 002      67     1 Less than high school Less than … Less than… Less than high…\n3 003      52     4 College graduate      College gr… College g… College gradua…\n4 004      56     2 High school graduate  High schoo… High scho… High school gr…\n\n\n\ntable(demo$edu_f_from_char)\n\n\nLess than high school  High school graduate          Some college \n                    1                     1                     1 \n     College graduate       Graduate school \n                    1                     0 \n\n\n👆Here’s what we did above:\n\nWe coerced a character vector (edu_char) to a factor using the factor() function.\nBecause the levels are character strings, there was no need to pass any values to the labels argument this time. Keep in mind, though, that the order of the values passed to the levels argument matters. It will be the order that the factor levels will be displayed in your analyses.\n\nNow that we know how to use factors, let’s return to our discussion of describing categorical variables.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html#height-and-weight-data",
    "href": "chapters/categorical_variables/categorical_variables.html#height-and-weight-data",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "19.2 Height and Weight Data",
    "text": "19.2 Height and Weight Data\nBelow, we’re going to learn to do descriptive analysis in R by experimenting with some simulated data that contains several people’s sex, height, and weight. You can follow along with this lesson by copying and pasting the code chunks below in your R session.\n\n# Load the dplyr package. We will need several of dplyr's functions in the \n# code below.\nlibrary(dplyr)\n\n\n# Simulate some data\nheight_and_weight_20 &lt;- tibble(\n  id = c(\n    \"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\", \"010\", \"011\", \n    \"012\", \"013\", \"014\", \"015\", \"016\", \"017\", \"018\", \"019\", \"020\"\n  ),\n  sex = c(1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2),\n  sex_f = factor(sex, 1:2, c(\"Male\", \"Female\")),\n  ht_in = c(\n    71, 69, 64, 65, 73, 69, 68, 73, 71, 66, 71, 69, 66, 68, 75, 69, 66, 65, 65, \n    65\n  ),\n  wt_lbs = c(\n    190, 176, 130, 154, 173, 182, 140, 185, 157, 155, 213, 151, 147, 196, 212, \n    190, 194, 176, 176, 102\n  )\n)\n\n\n19.2.1 View the data\nLet’s start our analysis by taking a quick look at our data…\n\nheight_and_weight_20\n\n# A tibble: 20 × 5\n   id      sex sex_f  ht_in wt_lbs\n   &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 001       1 Male      71    190\n 2 002       1 Male      69    176\n 3 003       2 Female    64    130\n 4 004       2 Female    65    154\n 5 005       1 Male      73    173\n 6 006       1 Male      69    182\n 7 007       2 Female    68    140\n 8 008       1 Male      73    185\n 9 009       2 Female    71    157\n10 010       1 Male      66    155\n11 011       1 Male      71    213\n12 012       2 Female    69    151\n13 013       2 Female    66    147\n14 014       2 Female    68    196\n15 015       1 Male      75    212\n16 016       2 Female    69    190\n17 017       2 Female    66    194\n18 018       2 Female    65    176\n19 019       2 Female    65    176\n20 020       2 Female    65    102\n\n\n👆Here’s what we did above:\n\nSimulated some data that we can use to practice categorical data analysis.\nWe viewed the data and found that it has 5 variables (columns) and 20 observations (rows).\nAlso notice that you can use the “Next” button at the bottom right corner of the printed data frame to view rows 11 through 20 if you are viewing this data in RStudio.\n\n\n\n\n\n\nThe “Next” button in RStudio.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html#calculating-frequencies",
    "href": "chapters/categorical_variables/categorical_variables.html#calculating-frequencies",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "19.3 Calculating frequencies",
    "text": "19.3 Calculating frequencies\nNow that we’re able to easily view our data, let’s return to the original purpose of this demonstration – calculating frequencies and proportions. At this point, we suspect that few of you would have any trouble saying that the frequency of females in this data is 12 and the frequency of males in this data is 8. It’s pretty easy to just count the number of females and males in this small data set with only 20 rows. Further, if we asked you what proportion of this sample is female, most of you would still be able to easily say 12/20 = 0.6, or 60%. But, what if we had 100 observations or 1,000,000 observations? You’d get sick of counting pretty quickly. Fortunately, you don’t have to! Let R do it for you! As is almost always the case with R, there are multiple ways we can calculate the statistics that we’re interested in.\n\n19.3.1 The base R table function\nAs we already saw above, we can use the base R table() function like this:\n\ntable(height_and_weight_20$sex)\n\n\n 1  2 \n 8 12 \n\n\nAdditionally, we can use the CrossTable() function from the gmodels package, which gives us a little more information by default.\n\n\n19.3.2 The gmodels CrossTable function\n\n# Like all packages, you will have to install gmodels (install.packages(\"gmodels\")) before you can use the CrossTable() function. \ngmodels::CrossTable(height_and_weight_20$sex)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  20 \n\n \n          |         1 |         2 | \n          |-----------|-----------|\n          |         8 |        12 | \n          |     0.400 |     0.600 | \n          |-----------|-----------|\n\n\n\n \n\n\n\n\n19.3.3 The tidyverse way\nThe final way we’re going to discuss here is the tidyverse way, which is our preference. We will have to write a little additional code, but the end result will be more flexible, more readable, and will return our statistics to us in a data frame that we can save and use for further analysis. Let’s walk through this step by step…\n\n\n\n\n\n\nNote\n\n\n\nYou should already be familiar with the pipe operator (|&gt;), but if it doesn’t look familiar to you, you can learn more about it in Using pipes. Don’t forget, if you are using RStudio, you can use the keyboard shortcut shift + command + m (Mac) or shift + control + m (Windows) to insert the pipe operator.\n\n\nFirst, we don’t want to view the individual values in our data frame. Instead, we want to condense those values into summary statistics. This is a job for the summarise() function.\n\nheight_and_weight_20 |&gt; \n  summarise()\n\n# A tibble: 1 × 0\n\n\nAs you can see, summarise() doesn’t do anything interesting on its own. We need to tell it what kind of summary information we want. We can use the n() function to count rows. By default, it will count all the rows in the data frame. For example:\n\nheight_and_weight_20 |&gt; \n  summarise(n())\n\n# A tibble: 1 × 1\n  `n()`\n  &lt;int&gt;\n1    20\n\n\n👆Here’s what we did above:\n\nWe passed our entire data frame to the summarise() function and asked it to count the number of rows in the data frame.\nThe result we get is a new data frame with 1 column (named n()) and one row with the value 20 (the number of rows in the original data frame).\n\nThis is a great start. However, we really want to count the number of rows that have the value “Female” for sex_f, and then separately count the number of rows that have the value “Male” for sex_f. Said another way, we want to break our data frame up into smaller data frames – one for each value of sex_f – and then count the rows. This is exactly what dplyr’s group_by() function does.\n\nheight_and_weight_20 |&gt;\n  group_by(sex_f) |&gt; \n  summarise(n())\n\n# A tibble: 2 × 2\n  sex_f  `n()`\n  &lt;fct&gt;  &lt;int&gt;\n1 Male       8\n2 Female    12\n\n\nAnd, that’s what we want.\n\n\n\n\n\n\nNote\n\n\n\ndplyr’s group_by() function operationalizes the Split - Apply - Combine strategy for data analysis. That sounds sort of fancy, but all it really means is that we split our data frame up into smaller data frames, apply our calculation separately to each smaller data frame, and then combine those individual results back together as a single result. So, in the example above, the height_and_weight_20 data frame was split into two separate little data frames (i.e., one for females and one for males), then the summarise() and n() functions counted the number of rows in each of the two smaller data frames (i.e., 12 and 8 respectively), and finally combined those individual results into a single data frame, which was printed to the screen for us to view.\n\n\nHowever, it will be awkward to work with a variable named n() (i.e., with parentheses) in the future. Let’s go ahead and assign it a different name. We can assign it any valid name we want. Some names that might make sense are n, frequency, or count. We’re going to go ahead and just name it n without the parentheses.\n\nheight_and_weight_20 |&gt;\n  group_by(sex_f) |&gt; \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  sex_f      n\n  &lt;fct&gt;  &lt;int&gt;\n1 Male       8\n2 Female    12\n\n\n👆Here’s what we did above:\n\nWe added n = to our summarise function (summarise(n = n())) so that our count column in the resulting data frame would be named n instead of n().\n\nFinally, estimating categorical frequencies like this is such a common operation that dplyr has a shortcut for it – count(). We can use the count() function to get the same result that we got above.\n\nheight_and_weight_20 |&gt; \n  count(sex_f)\n\n# A tibble: 2 × 2\n  sex_f      n\n  &lt;fct&gt;  &lt;int&gt;\n1 Male       8\n2 Female    12",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html#calculating-percentages",
    "href": "chapters/categorical_variables/categorical_variables.html#calculating-percentages",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "19.4 Calculating percentages",
    "text": "19.4 Calculating percentages\nIn addition to frequencies, we will often be interested in calculating percentages for categorical variables. As always, there are many ways to accomplish this task in R. From here on out, we’re going to primarily use tidyverse functions.\nIn this case, the proportion of people in our data who are female can be calculated as the number who are female (12) divided by the total number of people in the data (20). Because we already know that there are 20 people in the data, we could calculate proportions like this:\n\nheight_and_weight_20 |&gt;\n  count(sex_f) |&gt; \n  mutate(prop = n / 20)\n\n# A tibble: 2 × 3\n  sex_f      n  prop\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1 Male       8   0.4\n2 Female    12   0.6\n\n\n👆Here’s what we did above:\n\nBecause the count() function returns a data frame just like any other data frame, we can manipulate it in the same ways we can manipulate any other data frame.\nSo, we used dplyr’s mutate() function to create a new variable in the data frame named prop. Again, we could have given it any valid name.\nThen we set the value of prop to be equal to the value of n divided by 20.\n\nThis works, but it would be better to have R calculate the total number of observations for the denominator (20) than for us to manually type it in. In this case, we can do that with the sum() function.\n\nheight_and_weight_20 |&gt; \n  count(sex_f) |&gt; \n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  sex_f      n  prop\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n1 Male       8   0.4\n2 Female    12   0.6\n\n\n👆Here’s what we did above:\n\nInstead of manually typing in the total count for our denominator (20), we had R calculate it for us using the sum() function. The sum() function added together all the values of the variable n (i.e., 12 + 8 = 20).\n\nFinally, we just need to multiply our proportion by 100 to convert it to a percentage.\n\nheight_and_weight_20 |&gt; \n  count(sex_f) |&gt; \n  mutate(percent = n / sum(n) * 100)\n\n# A tibble: 2 × 3\n  sex_f      n percent\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;\n1 Male       8      40\n2 Female    12      60\n\n\n👆Here’s what we did above:\n\nChanged the name of the variable we are creating from prop to percent. But, we could have given it any valid name.\nMultiplied the proportion by 100 to convert it to a percentage.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html#cat-missing-data",
    "href": "chapters/categorical_variables/categorical_variables.html#cat-missing-data",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "19.5 Missing data",
    "text": "19.5 Missing data\nIn the real world, you will frequently encounter data that has missing values. Let’s quickly take a look at an example by adding some missing values to our data frame.\n\nheight_and_weight_20 &lt;- height_and_weight_20 |&gt; \n  mutate(sex_f = replace(sex, c(2, 9), NA)) |&gt; \n  print()\n\n# A tibble: 20 × 5\n   id      sex sex_f ht_in wt_lbs\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 001       1     1    71    190\n 2 002       1    NA    69    176\n 3 003       2     2    64    130\n 4 004       2     2    65    154\n 5 005       1     1    73    173\n 6 006       1     1    69    182\n 7 007       2     2    68    140\n 8 008       1     1    73    185\n 9 009       2    NA    71    157\n10 010       1     1    66    155\n11 011       1     1    71    213\n12 012       2     2    69    151\n13 013       2     2    66    147\n14 014       2     2    68    196\n15 015       1     1    75    212\n16 016       2     2    69    190\n17 017       2     2    66    194\n18 018       2     2    65    176\n19 019       2     2    65    176\n20 020       2     2    65    102\n\n\n👆Here’s what we did above:\n\nReplaced the 2nd and 9th value of sex_f with NA (missing) using the replace() function.\n\nNow let’s see how our code from above handles this\n\nheight_and_weight_20 |&gt; \n  count(sex_f) |&gt; \n  mutate(percent = n / sum(n) * 100)\n\n# A tibble: 3 × 3\n  sex_f     n percent\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     1     7      35\n2     2    11      55\n3    NA     2      10\n\n\nAs you can see, we are now treating missing as if it were a category of sex_f. Sometimes this will be the result you want. However, often you will want the n and percent of non-missing values for your categorical variable. This is sometimes referred to as a complete case analysis. There’s a couple of different ways we can handle this. We will simply filter out rows with a missing value for sex_f with dplyr’s filter() function.\n\nheight_and_weight_20 |&gt; \n  filter(!is.na(sex_f)) |&gt; \n  count(sex_f) |&gt; \n  mutate(percent = n / sum(n) * 100)\n\n# A tibble: 2 × 3\n  sex_f     n percent\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     1     7    38.9\n2     2    11    61.1\n\n\n👆Here’s what we did above:\n\nWe used filter() to keep only the rows that have a non-missing value for sex_f. \n\nIn the R language, we use the is.na() function to tell the R interpreter to identify NA (missing) values in a vector. We cannot use something like sex_f == NA to identify NA values, which is sometimes confusing for people who are coming to R from other statistical languages.\nIn the R language, ! is the NOT operator. It sort of means “do the opposite.”\nSo, filter() tells R which rows of a data frame to keep, and is.na(sex_f) tells R to find rows with an NA value for the variable sex_f. Together, filter(is.na(sex_f)) would tell R to keep rows with an NA value for the variable sex_f. Adding the NOT operator ! tells R to do the opposite – keep rows that do NOT have an NA value for the variable sex_f.\n\nWe used our code from above to calculate the n and percent of non-missing values of sex_f.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html#formatting-results",
    "href": "chapters/categorical_variables/categorical_variables.html#formatting-results",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "19.6 Formatting results",
    "text": "19.6 Formatting results\nNotice that now our percentages are being displayed with 5 digits to the right of the decimal. If we wanted to present our findings somewhere (e.g., a journal article or a report for our employer) we would almost never want to display this many digits. Let’s get R to round these numbers for us.\n\nheight_and_weight_20 |&gt; \n  filter(!is.na(sex_f)) |&gt; \n  count(sex_f) |&gt; \n  mutate(percent = (n / sum(n) * 100) |&gt; round(2))\n\n# A tibble: 2 × 3\n  sex_f     n percent\n  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     1     7    38.9\n2     2    11    61.1\n\n\n👆Here’s what we did above:\n\nWe passed the calculated percentage values (n / sum(n) * 100) to the round() function to round our percentages to 2 decimal places.\n\nNotice that we had to wrap n / sum(n) * 100 in parentheses in order to pass it to the round() function with a pipe.\nWe could have alternatively written our R code this way: mutate(percent = round(n / sum(n) * 100, 2)).",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/categorical_variables/categorical_variables.html#using-freqtables",
    "href": "chapters/categorical_variables/categorical_variables.html#using-freqtables",
    "title": "19  Numerical Descriptions of Categorical Variables",
    "section": "19.7 Using freqtables",
    "text": "19.7 Using freqtables\nIn the sections above, we learned how to use dplyr functions to calculate the frequency and percentage of observations that take on each value of a categorical variable. However, there can be a fair amount of code writing involved when using those methods. The more we have to repeatedly type code, the more tedious and error-prone it becomes. This is an idea we will return to many times in this book. Luckily, the R programming language allows us to write our own functions, which solves both of those problems.\nLater in this book, we will show you how to write your own functions. For the time being, We’re going to suggest that you install and use a package we created called freqtables. The freqtables package is basically an enhanced version of the code we wrote in the sections above. We designed it to help us quickly make tables of descriptive statistics (i.e., counts, percentages, confidence intervals) for categorical variables, and it’s specifically designed to work in a dplyr pipeline.\nLike all packages, you need to first install it…\n\n# You may be asked if you want to update other packages on your computer that\n# freqtables uses. Go ahead and do so.\ninstall.packages(\"freqtables\")\n\nAnd then load it…\n\n# After installing freqtables on your computer, you can load it just like you\n# would any other package.\nlibrary(freqtables)\n\nNow, let’s use the freq_table() function from freqtables package to rerun our analysis from above.\n\nheight_and_weight_20 |&gt;\n  filter(!is.na(sex_f)) |&gt;\n  freq_table(sex_f)\n\n# A tibble: 2 × 9\n  var   cat       n n_total percent    se t_crit   lcl   ucl\n  &lt;chr&gt; &lt;chr&gt; &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 sex_f 1         7      18    38.9  11.8   2.11  18.2  64.5\n2 sex_f 2        11      18    61.1  11.8   2.11  35.5  81.8\n\n\n👆Here’s what we did above:\n\nWe used filter() to keep only the rows that have a non-missing value for sex and passed the data frame on to the freq_table() function using a pipe.\nWe told the freq_table() function to create a univariate frequency table for the variable sex_f. A “univariate frequency table” just means a table (data frame) of useful statistics about a single categorical variable.\nThe univariate frequency table above includes:\n\nvar: The name of the categorical variable (column) we are analyzing.\ncat: Each of the different categories the variable var contains – in this case “Male” and “Female”.\nn: The number of rows where var equals the value in cat. In this case, there are 7 rows where the value of sex_f is Male, and 11 rows where the value of sex_f is Female.\nn_total: The sum of all the n values. This is also to total number of rows in the data frame currently being analyzed.\npercent: The percent of rows where var equals the value in cat.\nse: The standard error of the percent. This value is not terribly useful on its own; however, it’s necessary for calculating the 95% confidence intervals.\nt_crit: The critical value from the t distribution. This value is not terribly useful on its own; however, it’s necessary for calculating the 95% confidence intervals.\nlcl: The lower (95%, by default) confidence limit for the percentage percent.\nucl: The upper (95%, by default) confidence limit for the percentage percent.\n\n\nWe will continue using the freqtables package at various points throughout the book. We will also show you some other cool things we can do with freqtables. For now, all you need to know how to do is use the freq_table() function to calculate frequencies and percentages for single categorical variables.\n🏆 Congratulations! You now know how to use R to do some basic descriptive analysis of individual categorical variables.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Numerical Descriptions of Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html",
    "href": "chapters/central_tendency/central_tendency.html",
    "title": "20  Measures of Central Tendency",
    "section": "",
    "text": "20.1 Calculate the mean\nIn previous sections you’ve seen methods for describing individual categorical variables. Now we’ll switch over to numerically describing numerical variables.\nIn epidemiology, we often want to describe the “typical” person in a population with respect to some characteristic that is recorded as a numerical variable – like height or weight. The most basic, and probably most commonly used, way to do so is with a measure of central tendency.\nIn this chapter we’ll discuss three measures of central tendency:\nNow, this is not a statistics course. But we will briefly discuss these measures and some of their characteristics below to make sure that we’re all on the same page when we discuss the interpretation of our results.\nThe mean\nWhen we talk about the typical, or “average”, value of some variable measured on a continuous scale, we are usually talking about the mean value of that variable. To be even more specific, we are usually talking about the arithmetic mean value. This value has some favorable characteristics that make it a good description of central tendency.\n👍 For starters it’s simple. Most people are familiar with the mean, and at the very least, have some intuitive sense of what it means (no pun intended).\n👍 In addition, there can be only one mean value for any set of values.\nHowever, there are a couple of potentially problematic characteristics of the mean as well:\n👎 It’s susceptible to extreme values in your data. In other words, a couple of people with very atypical values for the characteristic you are interested in can drastically alter the value of the mean, and your estimate for the typical person in your population of interest along with it.\n👎 Additionally, it’s very possible to calculate a mean value that is not actually observed anywhere in your data.\nThe median\nThe median is probably the second most commonly used measure of central tendency. Like the mean, it’s computationally simple and relatively straightforward to understand. 👍 There can be one, and only one, median. 👍 And, its value may also be unobserved in the data.👎\nHowever, unlike the mean, it’s relatively resistant to extreme values. 👍 In fact, when the median is used as the measure of central tendency, it’s often because the person conducting the analysis suspects that extreme values in the data are likely to distort the mean.\nThe mode\nAnd finally, we have the mode, or the value that is most often observed in the data. It doesn’t get much simpler than that. 👍 But, unlike the mean and the median, there can be more than one mode for a given set of values. In fact, there can even be no mode if all the values are observed the exact same number of times.👎\nHowever, if there is a mode, by definition it’s observed in the data.👍\nNow that we are all on the same page with respect to the fundamentals of central tendency, let’s take a look at how to calculate these measures using R.\nCalculating the mean is really straightforward. We can just use base R’s built-in mean() function.\n# Load the dplyr package. We will need several of dplyr's functions in the \n# code below.\nlibrary(dplyr)\n# Simulate some data\nheight_and_weight_20 &lt;- tribble(\n  ~id,   ~sex,     ~ht_in, ~wt_lbs,\n  \"001\", \"Male\",   71,     190,\n  \"002\", \"Male\",   69,     177,\n  \"003\", \"Female\", 64,     130,\n  \"004\", \"Female\", 65,     153,\n  \"005\", NA,       73,     173,\n  \"006\", \"Male\",   69,     182,\n  \"007\", \"Female\", 68,     186,\n  \"008\", NA,       73,     185,\n  \"009\", \"Female\", 71,     157,\n  \"010\", \"Male\",   66,     155,\n  \"011\", \"Male\",   71,     213,\n  \"012\", \"Female\", 69,     151,\n  \"013\", \"Female\", 66,     147,\n  \"014\", \"Female\", 68,     196,\n  \"015\", \"Male\",   75,     212,\n  \"016\", \"Female\", 69,     19000,\n  \"017\", \"Female\", 66,     194,\n  \"018\", \"Female\", 65,     176,\n  \"019\", \"Female\", 65,     176,\n  \"020\", \"Female\", 65,     102\n)\n👆 Here’s what we did above:\nmean(height_and_weight_20$ht_in)\n\n[1] 68.4\n👆 Here’s what we did above:",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#calculate-the-mean",
    "href": "chapters/central_tendency/central_tendency.html#calculate-the-mean",
    "title": "20  Measures of Central Tendency",
    "section": "",
    "text": "We loaded the tibble package so that we could use its tribble() function.\nWe used the tribble() function to simulate some data – heights and weights for 20 hypothetical students.\n\nThe tribble() function creates something called a tibble. A tibble is the tidyverse version of a data frame. In fact, it is a data frame, but with some additional functionality. You can use the link to read more about it if you’d like.\nWe used the tribble() function instead of the data.frame() function to create our data frame above because we can use the tribble() function to create our data frames in rows (like you see above) instead of columns with the c() function.\nUsing the tribble() function to create a data frame isn’t any better or worse than using the data.frame() function. You should just be aware that it exists and is sometimes useful.\n\n\n\n\n\nWe used base R’s mean() function to calculate the mean of the column “ht_in” from the data frame “height_and_weight_20”.\n\nNote: if you just type mean(ht_in) you will get an error. That’s because R will look for an object called “ht_in” in the global environment.\nHowever, we didn’t create an object called “ht_in”. We created an object (in this case a data frame) called “height_and_weight_20”. That object has a column in it called “ht_in”.\nSo, we must specifically tell R to look for the “ht_in” column in the data frame “height_and_weight_20”. Using base R, we can do that in one of two ways: height_and_weight_20$ht_in or height_and_weight_20[[\"ht_in\"]].",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#calculate-the-median",
    "href": "chapters/central_tendency/central_tendency.html#calculate-the-median",
    "title": "20  Measures of Central Tendency",
    "section": "20.2 Calculate the median",
    "text": "20.2 Calculate the median\nSimilar to above, we can use base R’s median() function to calculate the median.\n\nmedian(height_and_weight_20$ht_in)\n\n[1] 68.5\n\n\n👆 Here’s what we did above:\n\nWe used base R’s median() function to calculate the median of the column “ht_in” from the data frame “height_and_weight_20”.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#calculate-the-mode",
    "href": "chapters/central_tendency/central_tendency.html#calculate-the-mode",
    "title": "20  Measures of Central Tendency",
    "section": "20.3 Calculate the mode",
    "text": "20.3 Calculate the mode\nBase R does not have a built-in mode() function. Well, it actually does have a mode() function, but for some reason that function does not return the mode value(s) of a set of numbers. Instead, the mode() function gets or sets the type or storage mode of an object. For example:\n\nmode(height_and_weight_20$ht_in)\n\n[1] \"numeric\"\n\n\nThis is clearly not what we are looking for. So, how do we find the mode value(s)? Well, we are going to have to build our own mode function. Later in the book, we will return to this function and walk through how to build it one step at a time. For now, just copy and paste the code into R on your computer. Keep in mind, as is almost always the case with R, this way of writing this function is only one of multiple possible ways.\n\nmode_val &lt;- function(x) {\n  \n  # Count the number of occurrences for each value of x\n  value_counts &lt;- table(x)\n  \n  # Get the maximum number of times any value is observed\n  max_count &lt;- max(value_counts)\n  \n  # Create and index vector that identifies the positions that correspond to\n  # count values that are the same as the maximum count value: TRUE if so\n  # and false otherwise\n  index &lt;- value_counts == max_count\n  \n  # Use the index vector to get all values that are observed the same number \n  # of times as the maximum number of times that any value is observed\n  unique_values &lt;- names(value_counts)\n  result &lt;- unique_values[index]\n  \n  # If result is the same length as value counts that means that every value\n  # occured the same number of times. If every value occurred the same number\n  # of times, then there is no mode\n  no_mode &lt;- length(value_counts) == length(result)\n  \n  # If there is no mode then change the value of result to NA\n  if (no_mode) {\n    result &lt;- NA\n  }\n  \n  # Return result\n  result\n}\n\n\nmode_val(height_and_weight_20$ht_in)\n\n[1] \"65\" \"69\"\n\n\n👆 Here’s what we did above:\n\nWe created our own function, mode_val(), that takes a vector (or data frame column) as a value to its “x” argument and returns the mode value(s) of that vector.\nWe can also see that the function works as expected when there is more than one mode value. In this case, “65” and “69” each occur 4 times in the column “ht_in”.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#compare-mean-median-and-mode",
    "href": "chapters/central_tendency/central_tendency.html#compare-mean-median-and-mode",
    "title": "20  Measures of Central Tendency",
    "section": "20.4 Compare mean, median, and mode",
    "text": "20.4 Compare mean, median, and mode\n\nNow that you know how to calculate the mean, median, and mode, let’s compare these three measures of central tendency. This is a good opportunity to demonstrate some of the different characteristics of each that we spoke about earlier.\n\nheight_and_weight_20 %&gt;% \n  summarise(\n    min_weight    = min(wt_lbs),\n    mean_weight   = mean(wt_lbs),\n    median_weight = median(wt_lbs),\n    mode_weight   = mode_val(wt_lbs) %&gt;% as.double(),\n    max_weight    = max(wt_lbs)\n  )\n\n# A tibble: 1 × 5\n  min_weight mean_weight median_weight mode_weight max_weight\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1        102       1113.          176.         176      19000\n\n\n👆 Here’s what we did above:\n\nWe used the mean() function, median() function, and our mode_val() function inside of dplyr’s summarise() function to find the mean, median, and mode values of the column “wt_lbs” in the “height_and_weight_20” data frame.\nWe also used the as.double() function to convert the value returned by mode_val() – “176” – from a character string to a numeric double. This isn’t strictly necessary, but does look better.\nFinally, we used base R’s min() and max() functions to view the lowest and highest weights in our sample.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#data-checking",
    "href": "chapters/central_tendency/central_tendency.html#data-checking",
    "title": "20  Measures of Central Tendency",
    "section": "20.5 Data checking",
    "text": "20.5 Data checking\nDo you see any red flags 🚩as you scan the results? Do you really think a mean weight of 1,113 pounds sounds reasonable? This should definitely be a red flag for you. Now move your gaze three columns to the right and notice that the maximum value of weight is 19,000 lbs – an impossible value for a study in human populations. In this case the real weight was supposed to be 190 pounds, but the person entering the data accidentally got a little trigger-happy with the zero key.\nThis is an example of what was meant by “We can use descriptive analysis to uncover errors in our data” in the Introduction to descriptive analysis chapter. Often times, for various reasons, some observations for a given variable take on values that don’t make sense. Starting by calculating some basic descriptive statistics for each variable is one approach you can use to try to figure out if you have values in your data that don’t make sense.\nIn this case we can just go back and fix our data, but what if we didn’t know this value was an error? What if it were a value that was technically possible, but very unlikely? Well, we can’t just go changing values in our data. It’s unethical, and in some cases illegal. Below, we discuss the how the properties of the median and mode can come in handy in situations such as this.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#properties-of-mean-median-and-mode",
    "href": "chapters/central_tendency/central_tendency.html#properties-of-mean-median-and-mode",
    "title": "20  Measures of Central Tendency",
    "section": "20.6 Properties of mean, median, and mode",
    "text": "20.6 Properties of mean, median, and mode\nDespite the fact that this impossibly extreme value is in our data, the median and mode estimates are reasonable estimates of the typical person’s weight in this sample. This is what we mean when we say that the median and mode are more “resistant to extreme values” than the mean.\nYou may also notice that no person in our sample had an actual weight of 1,112.75 (the mean) or even 176.5 (the median). This is what we we mean when we say that the mean and median values are “not necessarily observed in the data.”\nIn this case, the mode value (176) is also a more reasonable estimate of the average person’s weight than the mean. And unlike the mean and the median, participants 18 and 19 actually weigh 176 pounds. This is not to say that the mode is always the best measure of central tendency to use. However, you can often learn useful information from your data by calculating and comparing these relatively simple descriptive statistics on each of your numeric variables.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#na-rm",
    "href": "chapters/central_tendency/central_tendency.html#na-rm",
    "title": "20  Measures of Central Tendency",
    "section": "20.7 Missing data",
    "text": "20.7 Missing data\nIn numerical descriptions of categorical variables we saw that we could use the dplyr::filter() function to remove all the rows from our data frame that contained a missing value for any of our variables of interest. We learned that this is called a complete case analysis. This method should pretty much always work, but in this section, you will see an alternative method for dropping missing values from your analysis that you are likely to come across often when reading R documentation – the na.rm argument.\nMany R functions that perform calculations on numerical variables include an na.rm – short for “Remove NA” – argument. By default, this argument is typically set to FALSE. By passing the value TRUE to this argument, we can perform a complete case analysis. Let’s quickly take a look at how it works.\nWe already saw that we can calculate the mean value of a numeric vector using the mean() function:\n\nmean(c(1, 2, 3))\n\n[1] 2\n\n\nBut, what happens when our vector has a missing value?\n\nmean(c(1, NA, 3))\n\n[1] NA\n\n\nAs you can see, the mean() function returns NA by default when we pass it a numeric vector that contains a missing value. It can be confusing to understand why this is the case. The logic goes something like this. In R, an NA doesn’t represent the absence of a value – a value that doesn’t exist at all; rather, it represents a value that does exist, but is unknown to us. So, if you were asked to give the mean of a set of numbers that contains 1, some unknown number, and 3 what would your answer be? Well, you can’t just give the mean of 1 and 2. That would imply that the unknown number doesn’t exist. Further, you can’t really give any numeric answer because that answer will depend on the value of the missing number. So, the only logical answer to give is something like “I don’t know” or “it depends.” 🤷 That is essentially what R is telling us when it returns an NA.\nWhile this answer is technically correct, it usually isn’t very satisfying to us. Instead, we often want R to calculate the mean of the numbers that remain after all missing values are removed from the original set. The implicit assumption is that the mean of that reduced set of numbers will be “close enough” to the mean of the original set of numbers for our purposes. We can ask R to do this by changing the value of the na.rm argument from FALSE – the default – to TRUE.\n\nmean(c(1, NA, 3), na.rm = TRUE)\n\n[1] 2\n\n\nIn this case, the mean of the original set of numbers (2) and the mean of our complete case analysis (2) are identical. That won’t always be the case.\nFinally, let’s compare using filter() and na.rm = TRUE in a dplyr pipeline. We will first use the replace() function to add some missing values to our height_and_weight_20 data.\n\nheight_and_weight_20 &lt;- height_and_weight_20 %&gt;% \n  mutate(ht_in = replace(ht_in, c(1, 2), NA)) %&gt;% \n  print()\n\n# A tibble: 20 × 4\n   id    sex    ht_in wt_lbs\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 001   Male      NA    190\n 2 002   Male      NA    177\n 3 003   Female    64    130\n 4 004   Female    65    153\n 5 005   &lt;NA&gt;      73    173\n 6 006   Male      69    182\n 7 007   Female    68    186\n 8 008   &lt;NA&gt;      73    185\n 9 009   Female    71    157\n10 010   Male      66    155\n11 011   Male      71    213\n12 012   Female    69    151\n13 013   Female    66    147\n14 014   Female    68    196\n15 015   Male      75    212\n16 016   Female    69  19000\n17 017   Female    66    194\n18 018   Female    65    176\n19 019   Female    65    176\n20 020   Female    65    102\n\n\n👆Here’s what we did above:\n\nReplaced the 1st and 2nd value of ht_in with NA (missing) using the replace() function.\n\nHere’s what our results look like when we don’t perform a complete case analysis.\n\nheight_and_weight_20 %&gt;% \n  summarise(\n    min_height    = min(ht_in),\n    mean_height   = mean(ht_in),\n    median_height = median(ht_in),\n    mode_height   = mode_val(ht_in),\n    max_height    = max(ht_in)\n  )\n\n# A tibble: 1 × 5\n  min_height mean_height median_height mode_height max_height\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1         NA          NA            NA 65                  NA\n\n\nHere’s what our results look like when we use the filter() function.\n\nheight_and_weight_20 %&gt;% \n  filter(!is.na(ht_in)) %&gt;% \n  summarise(\n    min_height    = min(ht_in),\n    mean_height   = mean(ht_in),\n    median_height = median(ht_in),\n    mode_height   = mode_val(ht_in),\n    max_height    = max(ht_in)\n  )\n\n# A tibble: 1 × 5\n  min_height mean_height median_height mode_height max_height\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1         64        68.2            68 65                  75\n\n\nAnd, here’s what our results look like when we change the na.rm argument to TRUE.\n\nheight_and_weight_20 %&gt;% \n  summarise(\n    min_height    = min(ht_in, na.rm = TRUE),\n    mean_height   = mean(ht_in, na.rm = TRUE),\n    median_height = median(ht_in, na.rm = TRUE),\n    mode_height   = mode_val(ht_in),\n    max_height    = max(ht_in, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 5\n  min_height mean_height median_height mode_height max_height\n       &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1         64        68.2            68 65                  75\n\n\nAs you can see, both methods give us the same result. The method you choose to use will typically just come down to personal preference.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/central_tendency/central_tendency.html#using-meantables",
    "href": "chapters/central_tendency/central_tendency.html#using-meantables",
    "title": "20  Measures of Central Tendency",
    "section": "20.8 Using meantables",
    "text": "20.8 Using meantables\nIn the sections above, we learned how to use dplyr functions to calculate various measures of central tendency for continuous variables. However, there can be a fair amount of code writing involved when using those methods. The more we have to repeatedly type code, the more tedious and error-prone it becomes. This is an idea we will return to many times in this book. Luckily, the R programming language allows us to write our own functions, which solves both of those problems.\nLater in this book, you will be shown how to write your own functions. For the time being, we suggest that you install and use the meantables package. The meantables package is basically an enhanced version of the code we wrote in the sections above. We designed it to help us quickly make tables of descriptive statistics for continuous variables, and it’s specifically designed to work in a dplyr pipeline.\nLike all packages, you need to first install it…\n\n# You may be asked if you want to update other packages on your computer that\n# meantables uses. Go ahead and do so.\ninstall.packages(\"meantables\")\n\nAnd then load it…\n\n# After installing meantables on your computer, you can load it just like you\n# would any other package.\nlibrary(meantables)\n\nNow, let’s use the mean_table() function from meantables package to rerun our analysis from above.\n\nheight_and_weight_20 %&gt;%\n  filter(!is.na(ht_in)) %&gt;%\n  mean_table(ht_in)\n\n# A tibble: 1 × 9\n  response_var     n  mean    sd   sem   lcl   ucl   min   max\n  &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ht_in           18  68.2  3.28 0.774  66.6  69.8    64    75\n\n\n👆Here’s what we did above:\n\nWe used filter() to keep only the rows that have a non-missing value for ht_in and passed the data frame on to the mean_table() function using a pipe.\nWe told the mean_table() function to create a table of summary statistics for the variable ht_in. This is just an R data frame of useful statistics about a single continuous variable.\nThe summary statistics in the table above include:\n\nresponse_var: The name of the variable (column) we are analyzing.\nn: The number of non-missing values of response_var being analyzed in the current analysis.\nmean: The mean of all n values of response_var.\nsem: The standard error of the mean of all n values of response_var.\nlcl: The lower (95%, by default) confidence limit for the percentage mean.\nucl: The upper (95%, by default) confidence limit for the percentage mean.\nmin: The minimum value of response_var.\nmax: The maximum value of response_var.\n\n\nWe will continue using the meantables package at various points throughout the book. You will also be shown some other cool things we can do with meantables. For now, all you need to know how to do is use the mean_table() function to calculate basic descriptive statistics for single continuous variables.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "chapters/dispersion/dispersion.html",
    "href": "chapters/dispersion/dispersion.html",
    "title": "21  Measures of Dispersion",
    "section": "",
    "text": "21.1 Comparing distributions\nIn the chapter on measures of central tendency, we found the minimum value, mean value, median value, mode value, and maximum value of the weight variable in our hypothetical sample of students. We’ll go ahead and start this lesson by rerunning that analysis below, but this time we will analyze heights instead of weights.\nKeep in mind that our interest is in describing the “typical” or “average” person in our sample. The result of our analysis above tells us that the average person who answered the height question in our hypothetical class was: 68.4 inches. This information gets us reasonably close to understanding the typical height of the students in our hypothetical class. But remember, our average person does not necessarily have the same height as any actual person in our class. So a natural extension of our original question is: “how much like the average person, are the other people in class.”\nFor example, is everyone in class 68.4 inches?\nOr are there differences in everyone’s height, with the average person’s height always having a value in the middle of everyone else’s?\nThe measures used to answer this question are called measures of dispersion, which we can say is the amount of difference between people in the class, or more generally, the amount of variability in the data.\nThree common measures of dispersion used are the:\nRange\nThe range is simply the difference between the maximum and minimum value in the data.\nIn this case, the range is 11. The range can be useful because it tells us how much difference there is between the tallest person in our class and the shortest person in our class – 11 inches. However, it doesn’t tell us how close to 68.4 inches “most” people in the class are.\nIn other words, are most people in the class out at the edges of the range of values in the data?\nOr are people “evenly distributed” across the range of heights for the class?\nOr something else entirely?\nVariance\nThe variance is a measure of dispersion that is slightly more complicated to calculate, although not much, but gives us a number we can use to quantify the dispersion of heights around the mean. To do this, let’s work through a simple example that only includes six observations: 3 people who are 58 inches tall and 3 people who are 78 inches tall. In this sample of six people from our population the average height is 68 inches.\nNext, let’s draw an imaginary line straight up from the mean.\nThen, let’s measure the difference, or distance, between each person’s height and the mean height.\nThen we square the differences.\nThen we add up all the squared differences.\nAnd finally, we divide by n, the number of non-missing observations, minus 1. In this case n equals six, so n-1 equals five.\nGetting R to do this math for us is really straightforward. We simply use base R’s var() function.\n👆 Here’s what we did above:\nSo, 600 divided by 5 equals 120. Therefore, the sample variance in this case is 120. However, because the variance is expressed in squared units, instead of the original units, it isn’t necessarily intuitive to interpret.\nStandard deviation\nIf we take the square root of the variance, we get the standard deviation.\nThe standard deviation is 10.95 inches, which is much easier to interpret, and compare with other samples. Now that we know the sample standard deviation, we can use it to describe a value’s distance from the mean. Additionally, when our data is approximately normally distributed, then the percentage of values within each standard deviation from the mean follow the rules displayed in this table:\nThat is, about 68% of all the observations fall within one standard deviation of the mean (that is, 10.95 inches). About 95% of all observations are within 2 standard deviations of the mean (that is, 10.95 * 2 = 21.9 inches), and about 99.9% of all observations are within 3 standard deviations of the mean (that is, 10.95 * 3 = 32.85 inches).\nDon’t forget that these percentage rules apply to values around the mean. In other words, half the values will be greater than the mean and half the values will be lower than the mean. You will often see this graphically illustrated with a “normal curve” or “bell curve.”\nUnfortunately, the current data is nowhere near normally distributed and does not make for a good example of this rule.\nNow that you understand what the different measures of distribution are and how they are calculated, let’s further develop your “feel” for interpreting them. We can do this by comparing different simulated distributions.\nsim_data &lt;- tibble(\n  all_68     = rep(68, 20),\n  half_58_78 = c(rep(58, 10), rep(78, 10)),\n  even_58_78 = seq(from = 58, to = 78, length.out = 20),\n  half_48_88 = c(rep(48, 10), rep(88, 10)),\n  even_48_88 = seq(from = 48, to = 88, length.out = 20)\n)\nsim_data\n\n# A tibble: 20 × 5\n   all_68 half_58_78 even_58_78 half_48_88 even_48_88\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1     68         58       58           48       48  \n 2     68         58       59.1         48       50.1\n 3     68         58       60.1         48       52.2\n 4     68         58       61.2         48       54.3\n 5     68         58       62.2         48       56.4\n 6     68         58       63.3         48       58.5\n 7     68         58       64.3         48       60.6\n 8     68         58       65.4         48       62.7\n 9     68         58       66.4         48       64.8\n10     68         58       67.5         48       66.9\n11     68         78       68.5         88       69.1\n12     68         78       69.6         88       71.2\n13     68         78       70.6         88       73.3\n14     68         78       71.7         88       75.4\n15     68         78       72.7         88       77.5\n16     68         78       73.8         88       79.6\n17     68         78       74.8         88       81.7\n18     68         78       75.9         88       83.8\n19     68         78       76.9         88       85.9\n20     68         78       78           88       88\n👆 Here’s what we did above:\nWe will use this simulated data to quickly demonstrate a couple of these concepts. Let’s use R to calculate and compare the mean, variance, and standard deviation of each variable.\ntibble(\n  Column   = names(sim_data),\n  Mean     = purrr::map_dbl(sim_data, mean),\n  Variance = purrr::map_dbl(sim_data, var),\n  SD       = purrr::map_dbl(sim_data, sd)\n)\n\n# A tibble: 5 × 4\n  Column      Mean Variance    SD\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 all_68        68      0    0   \n2 half_58_78    68    105.  10.3 \n3 even_58_78    68     38.8  6.23\n4 half_48_88    68    421.  20.5 \n5 even_48_88    68    155.  12.5\n👆 Here’s what we did above:\nSo, for all the columns the mean is 68 inches. And that makes sense, right? We set the middle value and/or most commonly occurring value to be 68 inches for each of these variables. However, the variance and standard deviation are quite different.\nFor the column “all_68” the variance and standard deviation are both zero. If you think about it, this should make perfect sense: all the values are 68 – they don’t vary – and each observations distance from the mean (68) is zero.\nWhen comparing the rest of the columns notice that all of them have a non-zero variance. This is because not all people have the same value in that column – they vary. Additionally, we can see very clearly that variance (and standard deviation) are affected by at least two things:\nIn summary, although the variance and standard deviation don’t always have a really intuitive meaning all by themselves, we can get some useful information by comparing them. Generally speaking, the variance is larger when values are clustered at very low or very high values away from the mean, or when values are spread across a wider range.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "chapters/dispersion/dispersion.html#comparing-distributions",
    "href": "chapters/dispersion/dispersion.html#comparing-distributions",
    "title": "21  Measures of Dispersion",
    "section": "",
    "text": "We created a data frame with 5 simulated distributions:\n\nall_68 has a value of 68 repeated 20 times\nhalf_58_78 is made up of the values 58 and 78, each repeated 10 times (similar to our example above)\neven_58_78 is 20 evenly distributed numbers between 58 and 78\nhalf_48_88 is made up of the values 48 and 88, each repeated 10 times\neven_48_88 is 20 evenly distributed numbers between 48 and 88\n\n\n\n\n\n\nWe created a data frame to hold some summary statistics about each column in the “sim_data” data frame.\nWe used the map_dbl() function from the purrr package to iterate over each column in the data. Don’t worry too much about this right now. We will talk more about iteration and the purrr package later in the book.\n\n\n\n\n\nFirst is the distribution of values across the range of possible values. For example, half_58_78 and half_48_88 have a larger variance than even_58_78 and even_48_88 because all the values are clustered at the min and max - far away from the mean.\nThe second property of the data that is clearly influencing variance is the width of the range of values included in the distribution. For example, even_48_88 has a larger variance and standard deviation than even_58_78, even though both are evenly distributed across the range of possible values. The reason is because the range of possible values is larger, and therefore the range of distances from the mean is larger too.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "chapters/cont_out_cont_pred/cont_out_cont_pred.html",
    "href": "chapters/cont_out_cont_pred/cont_out_cont_pred.html",
    "title": "22  Describing the Relationship Between a Continuous Outcome and a Continuous Predictor",
    "section": "",
    "text": "22.1 Pearson Correlation Coefficient\nBefore covering anything new, let’s quickly review the importance and utility of descriptive analysis.\nIn the first few lessons on descriptive analysis we covered performing univariate analysis. That is, analyzing a single numerical or a single categorical variable. In this module, we’ll learn methods for describing relationships between two variables. This is also called bivariate analysis.\nFor example, we may be interested in knowing if there is a relationship between heart rate and exercise. If so, we may ask ourselves if heart rate differs, on average, by daily minutes of exercise. And, we could answer that question with the using a bivariate descriptive analysis.\nBefore performing any such bivariate descriptive analysis, you should ask yourself what types of variables you will analyze. We’ve already discussed the difference between numerical variables and categorical variables, but we will also need to decide whether each variable is an outcome or a predictor.\nSo, think back to our interest in whether or not heart rate differs by daily minutes of exercise. In this scenario, which variable is the predictor and which is the outcome?\nIn this scenario daily minutes of exercise is the predictor and heart rate is the outcome.\nHeart rate is the variable we’re interested in predicting or understanding, and exercise is a variable that we think helps to predict or explain heart rate.\nIn this first chapter on bivariate analysis, we will learn a simple method for describing the relationship between a continuous outcome variable and a continuous predictor variable – the Pearson Correlation Coefficient.\nPearson’s Correlation Coefficient is a parametric measure of the linear relationship between two numerical variables. It’s also referred to as rho (pronounced like “row”) and can be written shorthand as a lowercase \\(r\\). The Pearson Correlation Coefficient can take on values between -1 and 1, including zero.\nPearson’s correlation coefficient range of values\nA value of 0 indicates that there is no linear correlation between the two variables.\nPearson’s correlation coefficient value of 0\nA negative value indicates that there is a negative linear correlation between the two variables. In other words, as the value of x increases, the value of y decreases. Or, as the value of x decreases, the value of y increases.\nNegative Pearson’s correlation coefficient values\nA positive value indicates that there is a positive linear correlation between the two variables. As the value of x increases, the value of y increases. Or as the value of x decreases, the value of y decreases.\nPositive Pearson’s correlation coefficient values",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Describing the Relationship Between a Continuous Outcome and a Continuous Predictor</span>"
    ]
  },
  {
    "objectID": "chapters/cont_out_cont_pred/cont_out_cont_pred.html#pearson-correlation-coefficient",
    "href": "chapters/cont_out_cont_pred/cont_out_cont_pred.html#pearson-correlation-coefficient",
    "title": "22  Describing the Relationship Between a Continuous Outcome and a Continuous Predictor",
    "section": "",
    "text": "Warning\n\n\n\nWhen the relationship between two variables is nonlinear, or when outliers are present, the correlation coefficient might incorrectly estimate the strength of the relationship. Plotting the data enables you to verify the linear relationship and to identify the potential outliers.\n\n\n\n\n22.1.1 Calculating r\nIn this first code chunk, we’re going to use some simple simulated data to develop an intuition about describing the relationship between two continuous variables.\n\n# Load the dplyr package\nlibrary(dplyr)\n# Load the ggplot2 package\nlibrary(ggplot2)\n\n\nset.seed(123)\ndf &lt;- tibble(\n  id = 1:20,\n  x  = sample(x = 0:100, size = 20, replace = TRUE),\n  y  = sample(x = 0:100, size = 20, replace = TRUE)\n)\ndf\n\n# A tibble: 20 × 3\n      id     x     y\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1    30    71\n 2     2    78    25\n 3     3    50     6\n 4     4    13    41\n 5     5    66     8\n 6     6    41    82\n 7     7    49    35\n 8     8    42    77\n 9     9   100    80\n10    10    13    42\n11    11    24    75\n12    12    89    14\n13    13    90    31\n14    14    68     6\n15    15    90     8\n16    16    56    40\n17    17    91    73\n18    18     8    22\n19    19    92    26\n20    20    98    59\n\n\n👆 Here’s what we did above:\n\nWe created a data frame with 3 simulated variables – id, x, and y.\nWe used the sample() function to create x and y by sampling a number between 0 and 100 at random, 20 times.\nThe replace = TRUE option tells R that the same number can be selected more than once.\nThe set.seed() function is to ensure that we get the same random numbers every time we run the code chunk.\n\nThere is nothing special about 0 and 100; they are totally arbitrary. But, because all of these values are chosen at random, we have no reason to believe that there should be any relationship between them. Accordingly, we should also expect the Pearson Correlation Coefficient to be 0 (or very close to it).\nIn order to develop an intuition, let’s first plot this data, and get a feel for what it looks like.\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nAbove, we’ve created a nice scatter plot using ggplot2(). But, how do we interpret it? Well, each dot corresponds to a person in our data at the point where their x value intersects with their y value. This is made clearer by adding a geom_text() layer to our plot.\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_text(aes(label = id), nudge_x = 1.5, nudge_y = 2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n👆 Here’s what we did above:\n\nWe added a geom_text() layer to our plot in order to make it clear which person each dot represents.\nThe nudge_x = 1.5 option moves our text (the id number) to the right 1.5 units. The nudge_y = 2 option moves our text 2 units up. We did this to make the id number easier to read. If we had not nudged them, they would have been placed directly on top of the points.\n\nFor example, person 1 in our simulated data had an x value of 30 and a y value of 71. When you look at the plot above, does it look like person 1’s point is approximately at (x = 30, y = 71)? If we want to emphasize the point even further, we can plot a vertical line at x = 30 and a horizontal line at y = 71. Let’s do that below.\n\nggplot(df, aes(x, y)) +\n  geom_text(aes(label = id), nudge_x = 1.5, nudge_y = 2) +\n  geom_vline(xintercept = 30, col = \"red\", size = 0.25) +\n  geom_hline(yintercept = 71, col = \"red\", size = 0.25) +\n  geom_point() +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nAs you can see, the dot representing id 1 is at the intersection of these two lines.\nSo, we know how to read the plot now, but we still don’t really know anything about the relationship between x and y. Remember, we want to be able to characterize x and y as having one of these 5 relationships:\n\n\n\n\n\nRelationship between the outcome and the predictor\n\n\n\n\nLooking again at our scatter plot, which relationship do you think x and y have?\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_text(aes(label = id), nudge_x = 1.5, nudge_y = 2) +\n  geom_point(aes(x, y), tibble(x = 100, y = 80), shape = 1, size = 16, col = \"red\") +\n  geom_point(aes(x, y), tibble(x = 90, y = 8), shape = 1, size = 16, col = \"blue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWell, if you look at id 9 above, x is a high number (100) and y is a high number (80). But if you look at id 15, x is a high number (90) and y is a low number (8). In other words, these dots are scattered all over the chart area. There doesn’t appear to be much of a pattern, trend, or relationship. And that’s exactly what we would expect from randomly generated data.\nNow that we know what this data looks like, and we intuitively feel as though x and y are unrelated, it would be nice to quantify our results in some way. And, that is precisely what the Pearson Correlation Coefficient does.\n\ncor.test(x = df$x, y = df$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$x and df$y\nt = -0.60281, df = 18, p-value = 0.5542\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5490152  0.3218878\nsample estimates:\n       cor \n-0.1406703 \n\n\n👆 Here’s what we did above:\n\nBy default, R’s cor.test() function gives us a list of information about the relationship between x and y. The very last number in the output (-0.1406703) is the Pearson Correlation Coefficient.\nThe fact that this value is negative (between -1 and 0) tells us that x and y tend to vary in opposite directions.\nThe numeric value (0.1406703) tells us something about the strength of the relationship between x and y. In this case, the relationship is not strong – exactly what we expected.\n\nYou will sometimes hear rules of thumb for interpreting the strength of \\(r\\) such as1:\n\n±0.1 = Weak correlation\n±0.3 = Medium correlation\n±0.5 = Strong correlation\n\nRules of thumb like this are useful as you are learning; however, you want to make sure you don’t become overly reliant on them. As you get more experience, you will want to start interpreting effect sizes in the context of your data and the specific research question at hand.\n\nThe p-value (0.5542) tells us that we’d be pretty likely to get the result we got even if there really were no relationship between x and y – assuming all other assumptions are satisfied and the sample was collected without bias.\nTaken together, the weak negative correlation and p-value tell us that there is not much – if any – relationship between x and y. Another way to say the same thing is, “x and y are statistically independent.”\n\n\n\n22.1.2 Correlation intuition\nTo further bolster our intuition about these relationships, let’s look at a few positively and negatively correlated variables.\n\n# Positively correlated data\ntibble(\n  x = 1:10,\n  y = 100:109,\n  r = cor(x, y)\n) %&gt;% \n  ggplot() +\n    geom_point(aes(x, y)) +\n    geom_text(aes(x = 2.5, y = 107.5, label = paste(\"r = \", r)), col = \"blue\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nAbove, we created positively correlated data. In fact, this data is perfectly positively correlated. That is, every time the value of x increases, the value of y increases by a proportional amount. Now, instead of being randomly scattered around the plot area, the dots line up in a perfect, upward-sloping, diagonal line. We also added the correlation coefficient directly to the plot. As you can see, it is exactly 1. This is what you should expect from perfectly positively correlated data.\nHow about this next data set? Now, every time x decreases by one, y decreases by one. Is this positively or negatively correlated data?\n\ndf &lt;- tibble(\n  x = 1:-8,\n  y = 100:91\n)\ndf\n\n# A tibble: 10 × 2\n       x     y\n   &lt;int&gt; &lt;int&gt;\n 1     1   100\n 2     0    99\n 3    -1    98\n 4    -2    97\n 5    -3    96\n 6    -4    95\n 7    -5    94\n 8    -6    93\n 9    -7    92\n10    -8    91\n\n\n\ndf %&gt;% \n  mutate(r = cor(x, y)) %&gt;% \n  ggplot() +\n      geom_point(aes(x, y)) +\n      geom_text(aes(x = -6, y = 98, label = paste(\"r = \", r)), col = \"blue\") +\n      theme_classic()\n\n\n\n\n\n\n\n\nThis is still perfectly positively correlated data. The values for x and y are still changing in the same direction proportionately. The fact that the direction is one of decreasing value makes no difference.\nOne last simulated example here. This time, as x increases by one, y decreases by one. Let’s plot this data and calculate the Pearson Correlation Coefficient.\n\ntibble(\n  x = 1:10,\n  y = 100:91,\n  r = cor(x, y)\n) %&gt;% \n  ggplot() +\n    geom_point(aes(x, y)) +\n    geom_text(aes(x = 7.5, y = 98, label = paste(\"r = \", r)), col = \"blue\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nThis is what perfectly negatively correlated data looks like. The dots line up in a perfect, downward-sloping diagonal line, and when we check the value of rho, we see that it is exactly -1.\nOf course, as you may have suspected, in real life things are almost never this cut and dry. So, let’s investigate the relationship between continuous variables using more realistic data.\nIn this example, we will use data from an actual class survey conducted in the past:\n\nclass &lt;- tibble(\n  ht_in = c(70, 63, 62, 67, 67, 58, 64, 69, 65, 68, 63, 68, 69, 66, 67, 65, \n            64, 75, 67, 63, 60, 67, 64, 73, 62, 69, 67, 62, 68, 66, 66, 62, \n            64, 68, NA, 68, 70, 68, 68, 66, 71, 61, 62, 64, 64, 63, 67, 66, \n            69, 76, NA, 63, 64, 65, 65, 71, 66, 65, 65, 71, 64, 71, 60, 62, \n            61, 69, 66, NA),\n  wt_lbs = c(216, 106, 145, 195, 143, 125, 138, 140, 158, 167, 145, 297, 146, \n             125, 111, 125, 130, 182, 170, 121, 98, 150, 132, 250, 137, 124, \n             186, 148, 134, 155, 122, 142, 110, 132, 188, 176, 188, 166, 136, \n             147, 178, 125, 102, 140, 139, 60, 147, 147, 141, 232, 186, 212, \n             110, 110, 115, 154, 140, 150, 130, NA, 171, 156, 92, 122, 102, \n             163, 141, NA)\n)\n\nNext, we’re going to use a scatter plot to explore the relationship between height and weight in this data.\n\nggplot(class, aes(ht_in, wt_lbs)) +\n  geom_jitter() +\n  theme_classic()\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nQuickly, what do you think? Will height and weight be positively correlated, negatively correlated, or not correlated?\n\ncor.test(class$ht_in, class$wt_lbs)\n\n\n    Pearson's product-moment correlation\n\ndata:  class$ht_in and class$wt_lbs\nt = 5.7398, df = 62, p-value = 3.051e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4013642 0.7292714\nsample estimates:\n      cor \n0.5890576 \n\n\nThe dots don’t line up in a perfectly upward – or downward – slope. But the general trend is still an upward slope. Additionally, we can see that height and weight are positively correlated because the value of the correlation coefficient is between 0 and positive 1 (0.5890576). By looking at the p-value (3.051e-07), we can also see that the probability of finding a correlation value this large or larger in our sample if the true value of the correlation coefficient in the population from which our sample was drawn is zero, is very small.\nThat’s quite a mouthful, right? In more relatable terms, you can just think of it this way. In our data, as height increases weight tends to increase as well. Our p-value indicates that it’s pretty unlikely that we would get this result if there were truly no relationship in the population this sample was drawn from – assuming it’s an unbiased sample.\nQuick detour: The p-value above is written in scientific notation, which you may not have seen before. We’ll quickly show you how to basically disable scientific notation in R.\n\noptions(scipen = 999)\ncor.test(class$ht_in, class$wt_lbs)\n\n\n    Pearson's product-moment correlation\n\ndata:  class$ht_in and class$wt_lbs\nt = 5.7398, df = 62, p-value = 0.0000003051\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4013642 0.7292714\nsample estimates:\n      cor \n0.5890576 \n\n\n👆 Here’s what we did above:\n\nWe used the R global option options(scipen = 999) to display decimal numbers instead of scientific notation. Because this is a global option, it will remain in effect until you restart your R session. If you do restart your R session, you will have to run options(scipen = 999) again to disable scientific notation.\n\nFinally, wouldn’t it be nice if we could draw a line through this graph that sort of quickly summarizes this relationship (or lack thereof). Well, that is exactly what an Ordinary Least Squares (OLS) regression line does.\nTo add a regression line to our plot, all we need to do is add a geom_smooth() layer to our scatterplot with the method argument set to lm. Let’s do that below and take a look.\n\nggplot(class, aes(ht_in, wt_lbs)) +\n  geom_smooth(method = \"lm\") +\n  geom_jitter() +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe exact calculation for deriving this line is beyond the scope of this chapter. In general, though, you can think of the line as cutting through the middle of all of your points and representing the average change in the y value given a one-unit change in the x value. So here, the upward slope indicates that, on average, as height (the x value) increases, so does weight (the y value). And that is completely consistent with our previous conclusions about the relationship between height and weight.\n\n\n\n\n1. Field A, Miles J, Field Z. Discovering Statistics Using R. Sage; 2013.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Describing the Relationship Between a Continuous Outcome and a Continuous Predictor</span>"
    ]
  },
  {
    "objectID": "chapters/cont_out_cat_pred/cont_out_cat_pred.html",
    "href": "chapters/cont_out_cat_pred/cont_out_cat_pred.html",
    "title": "23  Describing the Relationship Between a Continuous Outcome and a Categorical Predictor",
    "section": "",
    "text": "23.1 Single predictor and single outcome\nUp until now, we have only ever looked at the overall mean of a continuous variable. For example, the mean height for the entire class. However, we often want to estimate the means within levels, or categories, of another variable. For example, we may want to look at the mean height within gender. Said another way, we want to know the mean height for men and separately the mean height for women.\nMore generally, in this lesson you will learn to perform bivariate analysis when the outcome is continuous and the predictor is categorical.\nTypically in a situation such as this, all we need to do is apply the analytic methods we’ve already learned for a single continuous outcome, but apply them separately within levels of our categorical predictor variable. Below, we’ll walk through doing so with R. To start with, we will again use our previously collected class survey data.\nWe can describe our continuous outcome variables using the same methods we learned in previous lessons. However, this time we will use dplyr's group_by() function to calculate these statistics within subgroups of interests. For example:\nclass_summary &lt;- class %&gt;% \n  filter(!is.na(ht_in)) %&gt;% \n  group_by(gender) %&gt;% \n  summarise(\n    n                    = n(),\n    mean                 = mean(ht_in),\n    `standard deviation` = sd(ht_in),\n    min                  = min(ht_in),\n    max                  = max(ht_in)\n  ) %&gt;% \n  print()\n\n# A tibble: 2 × 6\n  gender     n  mean `standard deviation`   min   max\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Female    43  64.3                 2.59    58    69\n2 Male      22  69.2                 2.89    65    76\n👆 Here’s what we did above:\nAs you look over this table, you should have an idea of whether male or female students in the class appear to be taller on average, and whether male or female students in the class appear to have more dispersion around the mean value.\nFinally, let’s plot this data to get a feel for the relationship between gender and height graphically.\nclass %&gt;% \n  filter(!is.na(ht_in)) %&gt;% \n  ggplot(aes(x = gender, y = ht_in)) +\n    geom_jitter(aes(col = gender), width = 0.20) +\n    geom_segment(\n      aes(x = c(0.75, 1.75), y = mean, xend = c(1.25, 2.25), yend = mean, col = gender), \n      size = 1.5, data = class_summary\n    ) +\n    scale_x_discrete(\"Gender\") +\n    scale_y_continuous(\"Height (Inches)\") +\n    scale_color_manual(values = c(\"#BC581A\", \"#00519B\")) +\n    theme_classic() +\n    theme(legend.position = \"none\", axis.text.x = element_text(size = 12))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n👆 Here’s what we did above:\nAfter checking both numerical and graphical descriptions of the relationship between gender and height we may conclude that male students were taller, on average, than female students.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Describing the Relationship Between a Continuous Outcome and a Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "chapters/cont_out_cat_pred/cont_out_cat_pred.html#single-predictor-and-single-outcome",
    "href": "chapters/cont_out_cat_pred/cont_out_cat_pred.html#single-predictor-and-single-outcome",
    "title": "23  Describing the Relationship Between a Continuous Outcome and a Categorical Predictor",
    "section": "",
    "text": "We used base R’s statistical functions inside dplyr's summarise() function to calculate the number of observations, mean, standard deviation, minimum value and maximum value of height within levels of gender.\nWe used filter(!is.na(ht_in)) to remove all rows from the data that have a missing value for “ht_in”. If we had not done so, R would have returned a value of “NA” for mean, standard deviation, min, and max. Alternatively, we could have added the na.rm = TRUE option to each of the mean(), sd(), min(), and max() functions.\nWe used group_by(gender) to calculate our statistics of interest separately within each category of the variable “gender.” In this case, “Female” and “Male.”\nYou may notice that we used back ticks around the variable name “standard deviation” – NOT single quotes. If you want to include a space in a variable name in R, you must surround it with back ticks. In general, it’s a really bad idea to create variable names with spaces in them. It is recommend that you only do so in situations where you are using a data frame to display summary information, as we did above.\nNotice too that we saved our summary statistics table as data frame named “class_summary.” Doing so is sometimes useful, especially for plotting as we will see below.\n\n\n\n\n\n\nWe used ggplot2 to plot each student’s height as well as the mean heights of female and male students respectively.\nThe geom_jitter() function plots a point for each student’s height, and then makes slight random adjustments to the location of the points so that they are less likely to overlap. One of the great things about plotting our data like this is that we can quickly see if there are many more observations in one category than another. That information would be obscured if we were to use a box plot.\nThe geom_segment() function creates the two horizontal lines at the mean values of height. Notice we used a different data frame – class_summary – using the data = class_summary argument to plot the mean values.\nWe changed the x and y axis titles using the scale_x_discrete() and scale_y_continuous() functions.\nWe changed the default ggplot colors to orange and blue (Go Gators! 🐊) using the scale_color_manual() function.\nWe simplified the plot using the theme_classic() function.\ntheme(legend.position = \"none\", axis.text.x = element_text(size = 12)) removed the legend and increased the size of the x-axis labels a little bit.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Describing the Relationship Between a Continuous Outcome and a Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "chapters/cont_out_cat_pred/cont_out_cat_pred.html#multiple-predictors",
    "href": "chapters/cont_out_cat_pred/cont_out_cat_pred.html#multiple-predictors",
    "title": "23  Describing the Relationship Between a Continuous Outcome and a Categorical Predictor",
    "section": "23.2 Multiple predictors",
    "text": "23.2 Multiple predictors\nAt times we may be interested in comparing continuous outcomes across levels of two or more categorical variables. As an example, perhaps we want to describe BMI by gender and age group. All we have to do is add age group to the group_by() function.\n\nclass_summary &lt;- class %&gt;% \n  filter(!is.na(bmi)) %&gt;% \n  group_by(gender, age_group) %&gt;% \n  summarise(\n    n                    = n(),\n    mean                 = mean(bmi),\n    `standard deviation` = sd(bmi),\n    min                  = min(bmi),\n    max                  = max(bmi)\n  ) %&gt;% \n  print()\n\n# A tibble: 4 × 7\n# Groups:   gender [2]\n  gender age_group           n  mean `standard deviation`   min   max\n  &lt;fct&gt;  &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Female Younger than 30    35  23.1                 5.41  17.4  45.2\n2 Female 30 and Older        8  21.8                 5.67  10.6  26.8\n3 Male   Younger than 30    19  24.6                 3.69  19.7  33.0\n4 Male   30 and Older        2  28.6                 3.32  26.3  31.0\n\n\nAnd we can see these statistics for BMI within levels of gender separately for younger and older students. Males that are 30 and older report, on average, the highest BMI (28.6). Females age 30 and older report, on average, the lowest BMI (21.8). This is good information, but often when comparing groups a picture really is worth a thousand words. Let’s wrap up this chapter with one final plot.\n\nclass %&gt;% \n  filter(!is.na(bmi)) %&gt;% \n  ggplot(aes(x = age_group, y = bmi)) +\n    facet_wrap(vars(gender)) +\n    geom_jitter(aes(col = age_group), width = 0.20) +\n    geom_segment(\n      aes(x = rep(c(0.75, 1.75), 2), y = mean, xend = rep(c(1.25, 2.25), 2), yend = mean, \n          col = age_group),\n      size = 1.5, data = class_summary\n    ) +\n    scale_x_discrete(\"Age Group\") +\n    scale_y_continuous(\"BMI\") +\n    scale_color_manual(values = c(\"#BC581A\", \"#00519B\")) +\n    theme_classic() +\n    theme(legend.position = \"none\", axis.text.x = element_text(size = 10))\n\n\n\n\n\n\n\n\n👆 Here’s what we did above:\n\nWe used the same code for this plot that we used for the first height by gender plot. The only difference is that we added facet_wrap(vars(gender)) to plot males and females on separate plot panels.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Describing the Relationship Between a Continuous Outcome and a Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "chapters/cat_out_cat_pred/cat_out_cat_pred.html",
    "href": "chapters/cat_out_cat_pred/cat_out_cat_pred.html",
    "title": "24  Describing the Relationship Between a Categorical Outcome and a Categorical Predictor",
    "section": "",
    "text": "24.1 Comparing two variables\nGenerally speaking, there is no good way to describe the relationship between a continuous predictor and a categorical outcome.\nSo, when your outcome is categorical, the predictor must also be categorical. Therefore, any continuous predictor variables must be collapsed into categories before conducting bivariate analysis when your outcome is categorical. The best categories are those that have scientific or clinical meaning. For example, collapsing raw scores on a test of cognitive function into a categorical variable for cognitive impairment. The variable could be dichotomous (yes, no) or it could have multiple levels (no, mild cognitive impairment, dementia).\nOnce your continuous variables are collapsed you’re ready to create n-way frequency tables that will allow you to describe the relationship between two or more categorical variables. To start with, we will once again use our previously collected class survey data.\nWe’ve already used R to create one-way descriptive tables for categorical variables. One-way frequency tables can be interesting in their own right; however, most of the time we are interested in the relationships between two variables. For example, think about when we looked at mean height within levels of gender. This told us something about the relationship between height and gender. While far from definite, our little survey provides some evidence that women, on average, are shorter than men.\nWell, we can describe the relationship between two categorical variables as well. One way of doing so is with two-way frequency tables, which are also sometimes referred to as crosstabs or contingency tables. Let’s start by simply looking at an example.\nBelow we use the same CrossTable() function that we used in the lesson on univariate analysis of categorical data. The only difference is that we pass two vectors to the function instead of one. The first variable will always form the rows, and the second variable will always form the columns. In other words, we can say that we are creating a two-way table of persdoc by genhealth.\ndf &lt;- filter(class, !is.na(bmi_3cat)) # Drop rows with missing bmi\ngmodels::CrossTable(df$persdoc, df$genhlth)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  61 \n\n \n                   | df$genhlth \n        df$persdoc | Excellent | Very Good |      Good |      Fair |      Poor | Row Total | \n-------------------|-----------|-----------|-----------|-----------|-----------|-----------|\n                No |         4 |         9 |         8 |         0 |         0 |        21 | \n                   |     0.090 |     0.097 |     0.180 |     0.344 |     0.344 |           | \n                   |     0.190 |     0.429 |     0.381 |     0.000 |     0.000 |     0.344 | \n                   |     0.400 |     0.310 |     0.400 |     0.000 |     0.000 |           | \n                   |     0.066 |     0.148 |     0.131 |     0.000 |     0.000 |           | \n-------------------|-----------|-----------|-----------|-----------|-----------|-----------|\n     Yes, only one |         4 |        12 |         6 |         1 |         0 |        23 | \n                   |     0.014 |     0.104 |     0.315 |     1.029 |     0.377 |           | \n                   |     0.174 |     0.522 |     0.261 |     0.043 |     0.000 |     0.377 | \n                   |     0.400 |     0.414 |     0.300 |     1.000 |     0.000 |           | \n                   |     0.066 |     0.197 |     0.098 |     0.016 |     0.000 |           | \n-------------------|-----------|-----------|-----------|-----------|-----------|-----------|\nYes, more than one |         2 |         8 |         6 |         0 |         1 |        17 | \n                   |     0.222 |     0.001 |     0.033 |     0.279 |     1.867 |           | \n                   |     0.118 |     0.471 |     0.353 |     0.000 |     0.059 |     0.279 | \n                   |     0.200 |     0.276 |     0.300 |     0.000 |     1.000 |           | \n                   |     0.033 |     0.131 |     0.098 |     0.000 |     0.016 |           | \n-------------------|-----------|-----------|-----------|-----------|-----------|-----------|\n      Column Total |        10 |        29 |        20 |         1 |         1 |        61 | \n                   |     0.164 |     0.475 |     0.328 |     0.016 |     0.016 |           | \n-------------------|-----------|-----------|-----------|-----------|-----------|-----------|\nOkay, let’s walk through this output together…\nCell contents\nThink of little box labeled “Cell Contents” as a legend that tells you how to interpret the rest of the boxes. Reading from top to bottom, the first number you encounter in a box will be the frequency or count of observations (labeled N). The second number you encounter will be the chi-square contribution. Please ignore that number for now. The third number will be the row proportion. The fourth number will be the column proportion. And the fifth number will be the overall proportion.\nTable of summary statistics row headers\nReading the table of summary statistics from top to bottom, the row headers describe categories of persdoc, which are one, only one, and more than one.\nTable of summary statistics column headers\nReading from left to right, the column headers describe categories of genhealth, which are excellent, very good, good, fair, and poor.\nTotal frequency and proportion of observations in each category defined by columns\nThe bottom row gives the total frequency and proportion of observations that fall in each of the categories defined by the columns. For example, 10 students – about 0.164 of the entire class – reported being in excellent general health.\nTotal frequency and proportion of observations in each category defined by rows\nThe far-right column gives the total frequency and proportion of observations that fall in each of the categories defined by the rows. For example, 23 students – about 0.377 of the entire class – reported that they have exactly one person that they think of as their personal doctor or healthcare provider.\nTotal frequency and proportion of observations in each category defined by rows\nAnd the bottom right corner gives the overall total frequency of observations in the table. Together, the last row, the far-right column, and the bottom right cell make up what are called the marginal totals because they are on the outer margin of the table.\nNext, let’s interpret the data contained in the first cell with data.\nFirst cell\nThe first number is the frequency. There are 4 students that do not have a personal doctor and report being in excellent health.\nFirst number - cell frequency\nThe third number is the row proportion. The row this cell is in is the No row, which includes 21 students. Out of the 21 total students in the No row, 4 reported being in excellent health. 4 divided by 21 is 0.190. Said another way, 19% of students with no personal doctor reported being in excellent health.\nThird number - row proportion\nThe fourth number is the column proportion. This cell is in the Excellent column. Of the 10 students in the Excellent column, 4 reported that they do not have a personal doctor. 4 out of 10 is 0.4. Said another way, 40% of students who report being in excellent health have no personal doctor.\nFourth number - column proportion\nThe last number is the overall proportion. So, 4 out of the 61 total students in this analysis have no personal doctor and report being in excellent health. Four out of 61 is 0.066. So, about 7% of all the students in the class have no personal doctor and are in excellent health.\nNow that you know how to read the table, let’s point out a couple subtleties that may not have jumped out at you above.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Describing the Relationship Between a Categorical Outcome and a Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "chapters/cat_out_cat_pred/cat_out_cat_pred.html#comparing-two-variables",
    "href": "chapters/cat_out_cat_pred/cat_out_cat_pred.html#comparing-two-variables",
    "title": "24  Describing the Relationship Between a Categorical Outcome and a Categorical Predictor",
    "section": "",
    "text": "The changing denominator. As we moved from the row proportion to the column proportion and then the overall proportion, all that changed was the denominator (the blue circle). And each time we did so we were describing the characteristics of a different group of people: (1) students without a personal doctor, (2) students in excellent general health, (3) all students – regardless of personal doctor or general health.\nLanguage matters. Because we are actually describing the characteristics of different subgroups, the language we use to interpret our results is important. For example, when interpreting the row proportion above, we wrote, “19% of students with no personal doctor reported being in excellent health.” This language implies that we’re describing the health (characteristic) of students with no personal doctor (subgroup). It would be completely incorrect to instead say, “19% of students in excellent health have no personal doctor” or “19% of students have no personal doctor.” Those are interpretations of the column percent and overall percent respectively. They are not interchangeable.",
    "crumbs": [
      "Descriptive Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Describing the Relationship Between a Categorical Outcome and a Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "chapters/intro_data_management/introduction.html",
    "href": "chapters/intro_data_management/introduction.html",
    "title": "25  Introduction to Data Management",
    "section": "",
    "text": "25.1 Multiple paradigms for data management in R\nWay back in the Getting Started chapter, I told you that managing data includes all the things you may have to do to your data to get it ready for analysis. We also talked about the 80/20 “rule.” The basic idea of the 80/20 rule is that data management is where we will spend the majority of our time and effort when we are involved in just about any project that makes use of data. Unfortunately, we can’t cover strategies for overcoming every single data management challenge that you will encounter in epidemiology. However, in this part of the book, we will try to give you a foundation in some of the most common data management tasks that you will encounter. We will also try to point you towards some of the best tools and resources for data management that the R community has to offer.\nBefore moving on to providing you with examples of how to accomplish specific data management tasks, we think this is the right point in the book to touch on a couple of high-level concepts that we have more or less ignored thus far.\nR is pretty unique among the major statistical programming applications used in epidemiology in many ways. Among them is that R has multiple paradigms for data management. That’s what we’re calling them anyway. What we mean by that is that there are 3 primary packages that the vast majority of R users use for data management. They are base R, data.table, and dplyr. There is a tremendous amount of overlap in the data management tasks you can perform with base R, data.table, and dplyr, but the syntax for each is very different. As are the relative strengths and weaknesses.\nIn this book, we will primarily use the dplyr paradigm for data management. We will do so because we believe in using the best tool to get the job done. Currently, we believe that the best tool for managing data in R is usually dplyr, and especially when you are new to R. However, there will be cases where we will show you how to use base R to accomplish a task. Where we do this, it’s because we think that base R is the best tool for the job or because we think you are very likely to see base R way used when you go looking for help with a related data management challenge and we don’t want you to be totally clueless about what you’re looking at.\nAs of this writing, we’ve decided not to specifically discuss using the data.table package for data management. We think the data.table package is a great package, and we use it when we think it’s the best tool for the job. However, we think the confusion caused by introducing data.table in this text aimed primarily at inexperienced R users would cause more problems than it would solve. The last thing we’ll say about data.table for now is that you may want to consider learning more about data.table if you routinely work with very large data sets (e.g., millions of rows). For reasons that are beyond the scope of this book, data.table is currently much faster than dplyr. However, for most of the work we do, and all of what we will do in this book, the time difference will be imperceptible to you. Literally milliseconds.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Data Management</span>"
    ]
  },
  {
    "objectID": "chapters/intro_data_management/introduction.html#the-dplyr-package",
    "href": "chapters/intro_data_management/introduction.html#the-dplyr-package",
    "title": "25  Introduction to Data Management",
    "section": "25.2 The dplyr package",
    "text": "25.2 The dplyr package\nAt this point in the book, you’ve already been exposed to several of the most important functions in the dplyr package. You saw the filter() function in the Speaking R’s language chapter, the mutate() function in the chapter on exporting data, and the summarise() function all over the descriptive analysis part of the book. However, we mostly glossed over the details at those points. In this section, we want to dive just a tiny bit deeper into how the dplyr functions work – but not too deep.\n\n25.2.1 The dplyr verbs\nThe dplyr package includes five main functions for managing data: mutate(), select(), filter(), arrange(), and summarise(). These five functions are often referred to as the dplyr verbs. And, the first two arguments to all five of these functions are .data and .... Let’s go ahead and discuss those two arguments a little bit more.\n\n\n\n\n\n\nNote\n\n\n\nwe don’t want to give you the impression that dplyr only contains 5 functions. In fact, dplyr contains many functions, and they are all designed to work together in a very intentional way.\n\n\n\n\n25.2.2 The .data argument\nI first introduced you to data frames in the Let’s get programming chapter and we’ve been using them as our primary structure for storing and analyzing data in ever since. The R language allows for other data structures (e.g., vectors, lists, and matrices), but data frames are the most commonly used data structure for most of the kinds of things we do in epidemiology. Thankfully, the dplyr package is designed specifically to help people like you and us manage data stored in data frames. Therefore, dplyr verbs always receive a data frame as an input and return a data frame as an output. Specifically, the value passed to the .data argument must always be a data frame, and you will get an error if you attempt to pass any other data structure to the .data argument. For example:\n\n# No problem\ndf &lt;- tibble(\n  id = c(1, 2, 3),\n  x  = c(0, 1, 0)\n)\n\ndf %&gt;% \n  filter(x == 0)\n\n# A tibble: 2 × 2\n     id     x\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     0\n2     3     0\n\n\n\n# Problem\nl &lt;- list(\n  id = c(1, 2, 3),\n  x  = c(0, 1, 0)\n) \n\nl %&gt;% \n  filter(x == 0)\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"list\"\n\n\n\n\n25.2.3 The … argument\nThe second value passed to all of the dplyr verbs is the ... argument. If you are new to R, this probably seems like a really weird argument. And, it kind of is! But, it’s also really useful. The ... argument (pronounced “dot dot dot”) has special meaning in the R language. This is true for all functions that use the ... argument – not just dplyr verbs. The ... argument can be used to pass multiple arguments to a function without knowing exactly what those arguments will look like ahead of time – including entire expressions. For example:\n\ndf %&gt;% \n  filter(x == 0)\n\n# A tibble: 2 × 2\n     id     x\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     0\n2     3     0\n\n\nAbove we passed a data frame to the .data argument of the filter() function. The second value we passed to the filter() function was x == 1. Think about it, x is an object (i.e. a column in the data frame), == is a function (remember that operators are functions in R), and 0 is a value. Together, they form an expression (x == 0) that tells R to perform a relatively complex operation – compare every value of x to the value 0 and tell me if they are the same. If you are new to programming, this may not seem like any big deal, but it’s really handy to be able to pass that much information to a single argument of a single function.\nIf this is all really confusing to you, don’t get too hung up on it right now. The ... argument is an important component of the R language, but it isn’t important that you fully understand it in order to use R. If nothing else, just know that that the ... is the second argument to all the dplyr verbs, and it is generally where you will tell R what you want to do to the columns of our data frame (i.e., keep them, drop them, create them, sort them, etc.).\n\n\n25.2.4 Non-standard evaluation\nA final little peculiarity about the tidyverse packages – dplyr being one of them – that we want to discuss in this chapter is something called non-standard evaluation. How non-standard evaluation works really isn’t that important for us. If we’re being honest, we don’t even fully understand how it works “under the hood.” But, it is one of the big advantages of using dplyr, and therefore worth mentioning. Do you remember the section in the Let’s get programming chapter on common errors? In that section I wrote about how a vector that lives in the global environment is a different thing to R than a vector that lives as a column in a data frame in the global environment. So, weight and class$weight are different things, and if you want to access the weight values in class$weight then you have to make sure and write the whole thing out. But, have you noticed that we don’t have to do that in dplyr verbs? For example:\n\ndf %&gt;% \n  filter(df$x == 0)\n\n# A tibble: 2 × 2\n     id     x\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     0\n2     3     0\n\n\nIn the example above we wrote out the column name using dollar sign notation. But, we don’t have to:\n\ndf %&gt;% \n  filter(x == 0)\n\n# A tibble: 2 × 2\n     id     x\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     0\n2     3     0\n\n\nWhen we don’t tell a dplyr verb exactly which data frame a column lives in, then the dplyr verb will assume it lives in the data frame that is passed to the .data argument. This is really handy for at least two reasons:\n\nIt reduces the amount of typing we have to do when we write our code. 👏\nIt makes it easier to glance at our code and see what it’s doing. Without all the data frame names and dollar signs strewn about our code, it’s much easier to see what the code is actually doing.\n\nOverall, non-standard evaluation is a great thing – at least in our opinion. However, it will present some challenges that we will have to overcome if we plan to use dplyr verbs inside of functions and loops. Don’t worry, we’ll come back to this topic later in the book.\nNow that you (hopefully) have a better general understanding of the dplyr verbs, let’s go take a look at how to use them for data management.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Data Management</span>"
    ]
  },
  {
    "objectID": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html",
    "href": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html",
    "title": "26  Creating and Modifying Columns",
    "section": "",
    "text": "26.1 Creating data frames\nTwo of the most fundamental data management tasks are to create new columns in your data frame and to modify existing columns in your data frame. In fact, we’ve already talked about creating and modifying columns at a few different places in the book.\nIn this book, we are actually going to learn 4 different methods for creating and modifying columns of a data frame. They are:\nVery early on, in the Let’s get programming chapter, we learned how to create data frame columns using name-value pairs passed directly into the tibble() function.\nclass &lt;- tibble(\n  names   = c(\"John\", \"Sally\", \"Brad\", \"Anne\"),\n  heights = c(68, 63, 71, 72)\n)\nclass\n\n# A tibble: 4 × 2\n  names heights\n  &lt;chr&gt;   &lt;dbl&gt;\n1 John       68\n2 Sally      63\n3 Brad       71\n4 Anne       72\nThis is an absolutely fundamental R programming skill, and one that you will likely use often. However, most people would not consider this to be a “data management” task, which is the focus of this part of the book. Further, we’ve really already covered all we need to cover about creating columns this way. So, we’re not going to write anything further about this method.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Creating and Modifying Columns</span>"
    ]
  },
  {
    "objectID": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#dollar-sign-notation",
    "href": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#dollar-sign-notation",
    "title": "26  Creating and Modifying Columns",
    "section": "26.2 Dollar sign notation",
    "text": "26.2 Dollar sign notation\nLater in the Let’s get programming chapter, we learned about dollar sign notation. At that time, we used dollar sign notation to access or “get” values from a column.\n\nclass$heights\n\n[1] 68 63 71 72\n\n\nHowever, we can also use dollar sign notation to create and/or modify columns in our data frame. For example:\n\nclass$heights &lt;- class$heights / 12\nclass\n\n# A tibble: 4 × 2\n  names heights\n  &lt;chr&gt;   &lt;dbl&gt;\n1 John     5.67\n2 Sally    5.25\n3 Brad     5.92\n4 Anne     6   \n\n\n👆Here’s what we did above:\n\nwe modified the values in the heights column of our class data frame using dollar sign notation. More specifically, we converted the values in the heights column from inches to feet. We did this by telling R to “get” the values for the heights column and divide them by 12 (class$heights / 12) and then assign those new values back to the heights column (class$heights &lt;-). In this case, that has the effect of modifying the values of a column that already exists.\n\n\n\n\n\n\n\nNote\n\n\n\nwe would actually suggest that you don’t typically do what we just did above in a real-world analysis. It’s typically safer to create a new variable with the modified values (e.g. height_feet) and leave the original values in the original variable as-is.\n\n\nwe can also create a new variable in our data frame in a similar way. All we have to do is use a valid column name (that doesn’t already exist in the data frame) on the left side of our assignment arrow. For example:\n\nclass$grades &lt;- c(89, 92, 86, 98)\nclass\n\n# A tibble: 4 × 3\n  names heights grades\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 John     5.67     89\n2 Sally    5.25     92\n3 Brad     5.92     86\n4 Anne     6        98\n\n\n👆Here’s what we did above:\n\nwe created a new column in our class data frame using dollar sign notation. We assigned the values 89, 92, 86, and 98 to that column with the assignment arrow.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Creating and Modifying Columns</span>"
    ]
  },
  {
    "objectID": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#bracket-notation",
    "href": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#bracket-notation",
    "title": "26  Creating and Modifying Columns",
    "section": "26.3 Bracket notation",
    "text": "26.3 Bracket notation\nwe also learned how to access or “get” values from a column using bracket notation in the Let’s get programming chapter. There, we actually used a combination of dollar sign and bracket notation to access single individual values from a data frame column. For example:\n\nclass$heights[3]\n\n[1] 5.916667\n\n\nBut, we can also use bracket notation to access or “get” the entire column. For example:\n\nclass[[\"heights\"]]\n\n[1] 5.666667 5.250000 5.916667 6.000000\n\n\n👆Here’s what we did above:\n\nwe used bracket notation to get all of the values from the heights column of the class data frame.\n\nwe’d like you to notice a couple of things about the example above. First, notice that this is the exact same result we got from (class$heights). Well, technically, the heights are now in feet instead of inches, but you know what we mean. R returned a numeric vector containing the values from the heights column to us. Second, notice that we used double brackets (i.e., two brackets on each side of the column name), and that the column name is wrapped in quotation marks. Both are required to get this result.\nSimilar to dollar sign notation, we can also create and/or modify columns in our data frame using bracket notation. For example, let’s convert those heights back to inches using bracket notation:\n\nclass[[\"heights\"]] &lt;- class[[\"heights\"]] * 12\nclass\n\n# A tibble: 4 × 3\n  names heights grades\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 John       68     89\n2 Sally      63     92\n3 Brad       71     86\n4 Anne       72     98\n\n\nAnd, let’s go ahead and add one more variable to our data frame using bracket notation.\n\nclass[[\"rank\"]] &lt;- c(3, 2, 4, 1)\nclass\n\n# A tibble: 4 × 4\n  names heights grades  rank\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 John       68     89     3\n2 Sally      63     92     2\n3 Brad       71     86     4\n4 Anne       72     98     1\n\n\nSomewhat confusingly, we can also access, create, and modify data frame columns using single brackets. For example:\n\nclass[\"heights\"]\n\n# A tibble: 4 × 1\n  heights\n    &lt;dbl&gt;\n1      68\n2      63\n3      71\n4      72\n\n\nNotice, however, that this returns a different result than class$heights and class[[\"heights]]. The results returned from class$heights and class[[\"heights]] were numeric vectors with 4 elements. The result returned from class[\"heights\"] was a data frame with 1 column and 4 rows.\nwe don’t want you to get too hung up on the difference between single and double brackets right now. As we said, we are primarily going to use mutate() to create and modify data frame columns in this book. For now, it’s enough for you to simply be aware that single brackets and double brackets are a thing, and they can sometimes return different results. We will make sure to point out whether or not that matters when we use bracket notation later in the book.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Creating and Modifying Columns</span>"
    ]
  },
  {
    "objectID": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#modify-individual-values",
    "href": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#modify-individual-values",
    "title": "26  Creating and Modifying Columns",
    "section": "26.4 Modify individual values",
    "text": "26.4 Modify individual values\nBefore moving on to the mutate() function, we wanted to quickly discuss using dollar sign and bracket notation for modifying individual values in a column. Recall that we already learned how to access individual column values in the Let’s get programming chapter.\n\nclass$heights[3]\n\n[1] 71\n\n\nAs you may have guessed, we can also get the result above using only bracket notation.\n\nclass[[\"heights\"]][3]\n\n[1] 71\n\n\nNot only can we use these methods to get individual values from a column in a data frame, but we can also use these methods to modify an individual value in a column of a data frame. When might we want to do this? Well, we generally do this in one of two different circumstances.\n\nFirst, we may do this when we’re writing our own R functions (you’ll learn how to do this later) and we want to make sure the function still behaves in the way we intended when there are small changes to the data. So, we may add a missing value to a column or something like that.\nThe second circumstance is when there are little one-off typos in the data. For example, let’s say we imported a data frame that looked like this:\n\n\nstudy_data &lt;- tibble(\n  id = c(1, 2, 3, 4),\n  site = c(\"TX\", \"CA\", \"tx\", \"CA\")\n)\n\nstudy_data\n\n# A tibble: 4 × 2\n     id site \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 TX   \n2     2 CA   \n3     3 tx   \n4     4 CA   \n\n\nNotice that tx in the third row of data isn’t capitalized. Remember, R is a case-sensitive language, so this will likely cause us problems down the road if we don’t fix it. The easiest way to do so is probably:\n\nstudy_data$site[3] &lt;- \"TX\"\nstudy_data\n\n# A tibble: 4 × 2\n     id site \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 TX   \n2     2 CA   \n3     3 TX   \n4     4 CA   \n\n\nKeep in mind that we said that we fix little one-off typos. If we needed to change tx to TX in multiple different places in the data, we wouldn’t use this method. Instead, we would use a conditional operation, which we will discuss later in the book.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Creating and Modifying Columns</span>"
    ]
  },
  {
    "objectID": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#the-mutate-function",
    "href": "chapters/creating_and_modifying_columns/creating_and_modifying_columns.html#the-mutate-function",
    "title": "26  Creating and Modifying Columns",
    "section": "26.5 The mutate() function",
    "text": "26.5 The mutate() function\n\n# Load dplyr for the mutate function\nlibrary(dplyr)\n\nwe first discussed mutate() in the chapter on exporting data, and again in the Introduction to data management chapter. As we said there, the first two arguments to mutate() are .data and ....\nThe value passed to .data should always be a data frame. In this book, we will often pass data frames to the .data argument using the pipe operator (e.g., df %&gt;% mutate()).\nThe value passed to the ... argument should be a name-value pair or multiple name value pairs separated by commas. The ... argument is where you will tell mutate() to create or modify columns in your data frame and how.\n\nName-value pairs look like this: column name = value. The only thing that distinguishes whether you are creating or modifying a column is the column name in the name-value pair. If the column name in the name-value pair matches the name of an existing column in the data frame, then mutate() will modify that existing column. If the column name in the name-value pair does NOT match the name of an existing column in the data frame, then mutate() will create a new column in the data frame with a matching column name.\n\nLet’s take a look at a couple of examples. To get us started, let’s simulate some data that is a little more interesting than the class data we used above.\n\nset.seed(123)\n\ndrug_trial &lt;- tibble(\n  # Study id, there are 20 people enrolled in the trial.\n  id = rep(1:20, each = 3),\n  # Follow-up year, 0 = baseline, 1 = year one, 2 = year two.\n  year = rep(0:2, times = 20),\n  # Participant age a baseline. Must be between the ages of 35 and 75 at \n  # baseline to be eligible for the study\n  age = sample(35:75, 20, TRUE) %&gt;% rep(each = 3),\n  # Drug the participant received, Placebo or active\n  drug = sample(c(\"Placebo\", \"Active\"), 20, TRUE) %&gt;% \n    rep(each = 3),\n  # Reported headaches side effect, Y/N\n  se_headache = if_else(\n    drug == \"Placebo\", \n    sample(0:1, 60, TRUE, c(.95,.05)), \n    sample(0:1, 60, TRUE, c(.10, .90))\n  ),\n  # Report diarrhea side effect, Y/N\n  se_diarrhea = if_else(\n    drug == \"Placebo\", \n    sample(0:1, 60, TRUE, c(.98,.02)), \n    sample(0:1, 60, TRUE, c(.20, .80))\n  ),\n  # Report dry mouth side effect, Y/N\n  se_dry_mouth = if_else(\n    drug == \"Placebo\", \n    sample(0:1, 60, TRUE, c(.97,.03)), \n    sample(0:1, 60, TRUE, c(.30, .70))\n  ),\n  # Participant had myocardial infarction in study year, Y/N\n  mi = if_else(\n    drug == \"Placebo\", \n    sample(0:1, 60, TRUE, c(.85, .15)), \n    sample(0:1, 60, TRUE, c(.80, .20))\n  )\n)\n\n👆Here’s what we did above:\n\nwe are simulating some drug trial data that includes the following variables:\n\nid: Study id, there are 20 people enrolled in the trial.\nyear: Follow-up year, 0 = baseline, 1 = year one, 2 = year two.\nage: Participant age a baseline. Must be between the ages of 35 and 75 at baseline to be eligible for the study.\ndrug: Drug the participant received, Placebo or active.\nse_headache: Reported headaches side effect, Y/N.\nse_diarrhea: Report diarrhea side effect, Y/N.\nse_dry_mouth: Report dry mouth side effect, Y/N.\nmi: Participant had myocardial infarction in study year, Y/N.\n\nwe used the tibble() function above to create our data frame instead of the data.frame() function. This allows us to pass the drug column as a value to the if_else() function when we create se_headache, se_diarrhea, se_dry_mouth, and mi. If we had used data.frame() instead, we would have had to create se_headache, se_diarrhea, se_dry_mouth, and mi in a separate step.\nwe used a new function, if_else(), above to help us simulate this data. This function allows us to do something called conditional operations. There will be an entire chapter on conditional operations later in the book.\nwe used a new function, sample(), above to help us simulate this data. We used this function to randomly assign values to age, drug, se_headache, se_diarrhea, se_dry_mouth, and mi instead of manually assigning each value ourselves.\n\nYou can type ?sample into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the sample() function is the x argument. You should pass a vector of values you want R to randomly choose from. For example, we told R to select values from a vector of numbers that spanned between 35 and 75 to fill-in the age column. Alternatively, we told R to select values from a character vector that included the values “Placebo” and “Active” to fill-in the drug column.\nThe second argument to the sample() function is the size argument. You should pass a number to the size argument. That number tells R how many times to choose a value from the vector of possible values passed to the x argument.\nThe third argument to the sample() function is the replace argument. The default value passed to the replace argument is FALSE. This tells R that once it has chosen a value from the vector of possible values passed to the x argument, it can’t choose that value again. If you want R to be able to choose the same value more than once, then you have to pass the value TRUE to the replace argument.\nThe fourth argument to the sample() function is the prob argument. The default value passed to the prob argument is NULL. This just means that this argument is optional. Passing a vector of probabilities to this argument allows you to adjust how likely it is that R will choose certain values from the vector of possible values passed to the x argument.\nFinally, notice that we also used the set.seed() function at the very top of the code chunk. We did this because, the sample() function chooses values at random. That means, every time we run the code above, we get different values. That makes it difficult for me to write about the data because it’s constantly changing. When we use the set.seed() function, the values will still be randomly selected, but they will be the same randomly selected values every time. It doesn’t matter what numbers you pass to the set.seed() function as long as you pass the same numbers every time you want to get the same random values. For example:\n\n\n\n# No set.seed - Random values\nsample(1:100, 10, TRUE)\n\n [1]  5 29 50 70 74 26 73 11  6 96\n\n\n\n# No set.seed - Different random values\nsample(1:100, 10, TRUE)\n\n [1] 76 83 91 56 96 27 94 68 88 28\n\n\n\n# Use set.seed - Random values\nset.seed(456)\nsample(1:100, 10, TRUE)\n\n [1] 35 38 85 27 25 78 31 73 79 90\n\n\n\n# Use set.seed again - Same random values\nset.seed(456)\nsample(1:100, 10, TRUE)\n\n [1] 35 38 85 27 25 78 31 73 79 90\n\n\n\n# Use set.seed with different value - Different random values\nset.seed(789)\nsample(1:100, 10, TRUE)\n\n [1]  45  12  42  26  99  37 100  43  67  70\n\n\n\nIt’s not important that you fully understand the sample() function at this point. We’re just including it for those of you who are interested in simulating some slightly more complex data than we have simulated so far. The rest of you can just copy and paste the code if you want to follow along.\n\n\n26.5.1 Adding or modifying a single column\nThis is probably the simplest case of adding a new column. We are going to use mutate() to add a single new column to the drug_trial data frame. Let’s say we want to add a column called complete that is equal to 1 if the participant showed up for all follow-up visits and equal to 0 if they didn’t. In this case, we simulated our data in such a way that we have complete follow-up for every participant. So, the value for complete should be 0 in all 60 rows of the data frame. We can do this in a few different ways.\n\ndrug_trial %&gt;% \n  mutate(complete = c(\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n  )\n\n# A tibble: 60 × 9\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi complete\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1     0    65 Active            0           1            1     0        0\n 2     1     1    65 Active            1           1            1     0        0\n 3     1     2    65 Active            1           1            0     0        0\n 4     2     0    49 Active            1           1            1     0        0\n 5     2     1    49 Active            0           0            1     0        0\n 6     2     2    49 Active            1           1            1     0        0\n 7     3     0    48 Placebo           0           0            0     0        0\n 8     3     1    48 Placebo           0           0            0     0        0\n 9     3     2    48 Placebo           0           0            0     0        0\n10     4     0    37 Placebo           0           0            0     0        0\n# ℹ 50 more rows\n\n\nSo, that works, but typing that out is no fun. Not to mention, this isn’t scalable at all. What if we needed 1,000 zeros? There’s actually a much easier way to get the result above, which may surprise you. Take a look 👀:\n\ndrug_trial %&gt;% \n  mutate(complete = 0)\n\n# A tibble: 60 × 9\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi complete\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1     0    65 Active            0           1            1     0        0\n 2     1     1    65 Active            1           1            1     0        0\n 3     1     2    65 Active            1           1            0     0        0\n 4     2     0    49 Active            1           1            1     0        0\n 5     2     1    49 Active            0           0            1     0        0\n 6     2     2    49 Active            1           1            1     0        0\n 7     3     0    48 Placebo           0           0            0     0        0\n 8     3     1    48 Placebo           0           0            0     0        0\n 9     3     2    48 Placebo           0           0            0     0        0\n10     4     0    37 Placebo           0           0            0     0        0\n# ℹ 50 more rows\n\n\nHow easy is that? Just pass the value to the name-value pair once and R will use it in every row. This works because of something called the recycling rules ♻️. In a nutshell, this means that R will change the length of vectors in certain situations all by itself when it thinks it knows what you “meant.” So, above we passed gave R a length 1 vector 0 (i.e. a numeric vector with one value in it), and R changed it to a length 60 vector behind the scenes so that it could complete the operation it thought you were trying to complete.\n\n\n26.5.2 Recycling rules\n♻️The recycling rules work as long as the length of the longer vector is an integer multiple of the length of the shorter vector. For example, every vector (column) in R data frames must have the same length. In this case, 60. The length of the value we used in the name-value pair above was 1 (i.e., a single 0). Therefore, the longer vector had a length of 60 and the shorter vector had a length of 1. Because 60 * 1 = But, what if we had tried to pass the values 0 and 1 to the column instead of just zero?\n\ndrug_trial %&gt;% \n  mutate(complete = c(0, 1))\n\nError in `mutate()`:\nℹ In argument: `complete = c(0, 1)`.\nCaused by error:\n! `complete` must be size 60 or 1, not 2.\n\n\nThis doesn’t work, but it actually isn’t for the reason you may be thinking. Because 30 * 2 = 60, the length of the longer vector (60) is an integer multiple (30) of the length of the shorter vector (2). However, tidyverse functions throw errors when you try to recycle anything other than a single number. They are designed this way to protect you from accidentally getting unexpected results. So, we’re going to switch back over to using base R to round out our discussion of the recycling rules. Let’s try our example above again using base R:\n\ndrug_trial$complete &lt;- c(0,1)\n\nError in `$&lt;-`:\n! Assigned data `c(0, 1)` must be compatible with existing data.\n✖ Existing data has 60 rows.\n✖ Assigned data has 2 rows.\nℹ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 2 to size 60.\n\ndrug_trial\n\n# A tibble: 60 × 8\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;\n 1     1     0    65 Active            0           1            1     0\n 2     1     1    65 Active            1           1            1     0\n 3     1     2    65 Active            1           1            0     0\n 4     2     0    49 Active            1           1            1     0\n 5     2     1    49 Active            0           0            1     0\n 6     2     2    49 Active            1           1            1     0\n 7     3     0    48 Placebo           0           0            0     0\n 8     3     1    48 Placebo           0           0            0     0\n 9     3     2    48 Placebo           0           0            0     0\n10     4     0    37 Placebo           0           0            0     0\n# ℹ 50 more rows\n\n\nWait, why are we still getting an error? Well, take a look at the output below and see if you can figure it out.\n\nclass(drug_trial)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nIt may not be totally obvious, but this is telling us that drug_trial is a tibble – an enhanced data frame. Remember, we created drug_trial using the tibble() function instead of the tibble() function. Because tibbles are part of the tidyverse they throw the same recycling errors that the mutate() function did above. So, we’ll need to create a non-tibble version of drug_trial to finish our discussion of recycling rules.\n\ndrug_trial_df &lt;- as.data.frame(drug_trial)\nclass(drug_trial_df)\n\n[1] \"data.frame\"\n\n\nThere we go! A regular old data frame.\n\ndrug_trial_df$complete &lt;- c(0,1)\ndrug_trial_df\n\n   id year age    drug se_headache se_diarrhea se_dry_mouth mi complete\n1   1    0  65  Active           0           1            1  0        0\n2   1    1  65  Active           1           1            1  0        1\n3   1    2  65  Active           1           1            0  0        0\n4   2    0  49  Active           1           1            1  0        1\n5   2    1  49  Active           0           0            1  0        0\n6   2    2  49  Active           1           1            1  0        1\n7   3    0  48 Placebo           0           0            0  0        0\n8   3    1  48 Placebo           0           0            0  0        1\n9   3    2  48 Placebo           0           0            0  0        0\n10  4    0  37 Placebo           0           0            0  0        1\n11  4    1  37 Placebo           0           0            0  0        0\n12  4    2  37 Placebo           0           0            0  1        1\n13  5    0  71 Placebo           0           0            0  0        0\n14  5    1  71 Placebo           0           0            0  0        1\n15  5    2  71 Placebo           0           0            0  0        0\n16  6    0  48 Placebo           0           0            0  0        1\n17  6    1  48 Placebo           0           0            0  1        0\n18  6    2  48 Placebo           0           0            0  1        1\n19  7    0  59  Active           1           1            1  0        0\n20  7    1  59  Active           1           1            0  0        1\n21  7    2  59  Active           1           1            1  0        0\n22  8    0  60 Placebo           0           0            0  0        1\n23  8    1  60 Placebo           0           0            0  0        0\n24  8    2  60 Placebo           0           0            0  0        1\n25  9    0  61  Active           1           1            1  0        0\n26  9    1  61  Active           0           1            1  0        1\n27  9    2  61  Active           1           0            0  0        0\n28 10    0  39  Active           1           0            1  0        1\n29 10    1  39  Active           1           0            0  0        0\n30 10    2  39  Active           1           1            1  0        1\n31 11    0  61 Placebo           0           0            0  0        0\n32 11    1  61 Placebo           0           0            0  1        1\n33 11    2  61 Placebo           0           0            0  0        0\n34 12    0  62 Placebo           1           0            1  0        1\n35 12    1  62 Placebo           0           0            0  0        0\n36 12    2  62 Placebo           0           0            0  0        1\n37 13    0  43 Placebo           0           0            0  0        0\n38 13    1  43 Placebo           0           0            0  0        1\n39 13    2  43 Placebo           0           0            0  0        0\n40 14    0  63 Placebo           0           0            0  0        1\n41 14    1  63 Placebo           0           0            0  0        0\n42 14    2  63 Placebo           0           0            0  0        1\n43 15    0  69  Active           1           1            1  0        0\n44 15    1  69  Active           1           0            1  0        1\n45 15    2  69  Active           1           1            1  0        0\n46 16    0  42 Placebo           0           0            0  0        1\n47 16    1  42 Placebo           0           0            1  0        0\n48 16    2  42 Placebo           0           0            0  1        1\n49 17    0  60 Placebo           0           0            0  0        0\n50 17    1  60 Placebo           0           0            0  0        1\n51 17    2  60 Placebo           1           0            0  0        0\n52 18    0  41  Active           1           1            1  0        1\n53 18    1  41  Active           1           1            1  0        0\n54 18    2  41  Active           1           1            0  1        1\n55 19    0  43 Placebo           0           0            0  0        0\n56 19    1  43 Placebo           0           0            0  0        1\n57 19    2  43 Placebo           0           0            0  0        0\n58 20    0  53 Placebo           0           0            0  0        1\n59 20    1  53 Placebo           0           0            0  0        0\n60 20    2  53 Placebo           0           0            0  0        1\n\n\nAs you can see, the values 0 and 1 are now recycled as expected. Because 30 * 2 = 60, the length of the longer vector (60) is an integer multiple (30) of the length of the shorter vector (2). Now, what happens in a situation where the length of the longer vector is not an integer multiple of the length of the shorter vector.\n\ndrug_trial_df$complete &lt;- c(0, 1, 2, 3, 4, 5, 6) # 7 values\n\nError in `$&lt;-.data.frame`(`*tmp*`, complete, value = c(0, 1, 2, 3, 4, : replacement has 7 rows, data has 60\n\n\n60 / 7 = 8.571429 – not an integer. Because there is no integer value that we can multiply by 7 to get the number 60, R throws us an error telling us that it isn’t able to use the recycling rules.\nFinally, the recycling rules don’t only apply to creating new data frame columns. It applies in all cases where R is using two vectors to perform an operation. For example, R uses the recycling rules in mathematical operations.\n\nnums &lt;- 1:10\nnums\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nTo demonstrate, we create a simple numeric vector above. This vector just contains the numbers 1 through 10. Now, we can add 1 to each of those numbers like so:\n\nnums + 1\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nNotice how R used the recycling rules to add 1 to every number in the nums vector. We didn’t have to explicitly tell R to add 1 to each number. This is sometimes referred to as vectorization. Functions that perform an action on all elements of a vector, rather than having to be explicitly programmed to perform an action on each element of a vector, is a vectorized function. Remember, that mathematical operators – including + – are functions in R. More specifically, + is a vectorized function. In fact, most built-in R functions are vectorized. Why are we telling you this? It isn’t intended to confuse you, but when I was learning R I came across this term all the time in R resources and help pages, and I had no idea what it meant. We hope that this very simple example above makes it easy to understand what vectorization means, and you won’t be intimidated when it pops up while you’re trying to get help with your R programs.\nOk, so what happens when we add a longer vector and a shorter vector?\n\nnums + c(1, 2)\n\n [1]  2  4  4  6  6  8  8 10 10 12\n\n\nAs expected, R uses the recycling rules to change the length of the short vector to match the length of the longer vector, and then performs the operation – in this case, addition. So, the net result is 1 + 1 = 2, 2 + 2 = 4, 3 + 1 = 4, 4 + 2 = 6, etc. You probably already guessed what’s going to happen if we try to add a length 3 vector to nums, but let’s go ahead and take a look for the sake of completeness:\n\nnums + c(1, 2, 3)\n\nWarning in nums + c(1, 2, 3): longer object length is not a multiple of shorter\nobject length\n\n\n [1]  2  4  6  5  7  9  8 10 12 11\n\n\nYep, we get an error. 10 / 3 = 3.333333 – not an integer. Because there is no integer value that we can multiply by 3 to get the number 10, R throws us an error telling us that it isn’t able to use the recycling rules.\nNow that you understand R’s recycling rules, let’s return to our motivating example.\n\ndrug_trial %&gt;% \n  mutate(complete = 0)\n\n# A tibble: 60 × 9\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi complete\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1     0    65 Active            0           1            1     0        0\n 2     1     1    65 Active            1           1            1     0        0\n 3     1     2    65 Active            1           1            0     0        0\n 4     2     0    49 Active            1           1            1     0        0\n 5     2     1    49 Active            0           0            1     0        0\n 6     2     2    49 Active            1           1            1     0        0\n 7     3     0    48 Placebo           0           0            0     0        0\n 8     3     1    48 Placebo           0           0            0     0        0\n 9     3     2    48 Placebo           0           0            0     0        0\n10     4     0    37 Placebo           0           0            0     0        0\n# ℹ 50 more rows\n\n\nThis method works, but not always. And, it can sometimes give us intended results. You may have originally thought to yourself, “we’ve already learned the rep() function. Let’s use that.” In fact, that’s a great idea!\n\ndrug_trial %&gt;% \n  mutate(complete = rep(0, 60))\n\n# A tibble: 60 × 9\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi complete\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1     0    65 Active            0           1            1     0        0\n 2     1     1    65 Active            1           1            1     0        0\n 3     1     2    65 Active            1           1            0     0        0\n 4     2     0    49 Active            1           1            1     0        0\n 5     2     1    49 Active            0           0            1     0        0\n 6     2     2    49 Active            1           1            1     0        0\n 7     3     0    48 Placebo           0           0            0     0        0\n 8     3     1    48 Placebo           0           0            0     0        0\n 9     3     2    48 Placebo           0           0            0     0        0\n10     4     0    37 Placebo           0           0            0     0        0\n# ℹ 50 more rows\n\n\nThat’s a lot less typing than the first method we tried, and it also has the added benefit of providing code that is easier for humans to read. We can both look at the code we used in the first method and tell that there are a bunch of zeros, but it’s hard to guess exactly how many, and it’s hard to feel completely confident that there isn’t a 1 in there somewhere that our eyes are missing. By contrast, it’s easy to look at rep(0, 60) and know that there are exactly 60 zeros, and only 60 zeros.\n\n\n26.5.3 Using existing variables in name-value pairs\nIn the example above, we create a new column called complete by directly supplying values for that column in the name-value pair. In our experience, it is probably more common to create new columns in our data frames by combining or transforming the values of columns that already exist in our data frame. You’ve already seen an example of doing so when we created factor versions of variables. As an additional example, we could create a factor version of our mi variable like this:\n\ndrug_trial %&gt;% \n  mutate(mi_f = factor(mi, c(0, 1), c(\"No\", \"Yes\")))\n\n# A tibble: 60 × 9\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi mi_f \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;fct&gt;\n 1     1     0    65 Active            0           1            1     0 No   \n 2     1     1    65 Active            1           1            1     0 No   \n 3     1     2    65 Active            1           1            0     0 No   \n 4     2     0    49 Active            1           1            1     0 No   \n 5     2     1    49 Active            0           0            1     0 No   \n 6     2     2    49 Active            1           1            1     0 No   \n 7     3     0    48 Placebo           0           0            0     0 No   \n 8     3     1    48 Placebo           0           0            0     0 No   \n 9     3     2    48 Placebo           0           0            0     0 No   \n10     4     0    37 Placebo           0           0            0     0 No   \n# ℹ 50 more rows\n\n\nNotice that in the code above, we didn’t tell R what values to use for mi_f by typing them explicitly in the name-value pair. Instead, we told R to go get the values of the column mi, do some stuff to those values, and then assign those modified values to a column in the data frame and name that column mi_f.\nHere’s another example. It’s common to mean-center numeric values for many different kinds of analyses. For example, this is often done in regression analysis to aid in the interpretation of regression coefficients. We can easily mean-center numeric variables inside our mutate() function like so:\n\ndrug_trial %&gt;% \n  mutate(age_center = age - mean(age))\n\n# A tibble: 60 × 9\n      id  year   age drug  se_headache se_diarrhea se_dry_mouth    mi age_center\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n 1     1     0    65 Acti…           0           1            1     0       11.3\n 2     1     1    65 Acti…           1           1            1     0       11.3\n 3     1     2    65 Acti…           1           1            0     0       11.3\n 4     2     0    49 Acti…           1           1            1     0       -4.7\n 5     2     1    49 Acti…           0           0            1     0       -4.7\n 6     2     2    49 Acti…           1           1            1     0       -4.7\n 7     3     0    48 Plac…           0           0            0     0       -5.7\n 8     3     1    48 Plac…           0           0            0     0       -5.7\n 9     3     2    48 Plac…           0           0            0     0       -5.7\n10     4     0    37 Plac…           0           0            0     0      -16.7\n# ℹ 50 more rows\n\n\nNotice how succinctly we were able to express this fairly complicated task. We had to figure out the find the mean of the variable age in the drug_trial data frame, subtract that value from the value for age in each row of the data frame, and then create a new column in the data frame containing the mean-centered values. Because of the fact that mutate()’s name-value pairs can accept complex expressions a value, and because all of the functions used in the code above are vectorized, we can perform this task using only a single, easy-to-read line of code (age_center = age - mean(age)).\n\n\n26.5.4 Adding or modifying multiple columns\nIn all of the examples above, we passed a single name-value pair to the ... argument of the mutate() function. If we want to create or modify multiple columns, we don’t need to keep typing the mutate() function over and over. We can simply pass multiple name-value pairs, separated by columns, to the ... argument. And, there is no limit to the number of pairs we can pass. This is part of the beauty of the ... argument in R. For example, we have three variables in drug_trial that capture information about whether or not the participant reported side effects including headache, diarrhea, and dry mouth. Currently, those are all stored as integer vectors that can take the values 0 and 1. Let’s say that we want to also create factor versions of those vectors:\n\ndrug_trial %&gt;% \n  mutate(\n    se_headache_f  = factor(se_headache, c(0, 1), c(\"No\", \"Yes\")),\n    se_diarrhea_f  = factor(se_diarrhea, c(0, 1), c(\"N0\", \"Yes\")),\n    se_dry_mouth_f = factor(se_dry_mouth, c(0, 1), c(\"No\", \"Yes\"))\n  )\n\n# A tibble: 60 × 11\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;\n 1     1     0    65 Active            0           1            1     0\n 2     1     1    65 Active            1           1            1     0\n 3     1     2    65 Active            1           1            0     0\n 4     2     0    49 Active            1           1            1     0\n 5     2     1    49 Active            0           0            1     0\n 6     2     2    49 Active            1           1            1     0\n 7     3     0    48 Placebo           0           0            0     0\n 8     3     1    48 Placebo           0           0            0     0\n 9     3     2    48 Placebo           0           0            0     0\n10     4     0    37 Placebo           0           0            0     0\n# ℹ 50 more rows\n# ℹ 3 more variables: se_headache_f &lt;fct&gt;, se_diarrhea_f &lt;fct&gt;,\n#   se_dry_mouth_f &lt;fct&gt;\n\n\n👆Here’s what we did above:\n\nwe created three new factor columns in the drug_trial data called se_headache_f, se_diarrhea_f, and se_dry_mouth_f.\nwe created all columns inside a single mutate() function.\nNotice that we created one variable per line. We suggest you do the same. It just makes your code much easier to read.\n\nSo, adding or modifying multiple columns is really easy with mutate(). But, did any of you notice an error? Take a look at the structure of the data the line of code that creates se_diarrhea_f. Instead of writing the “No” label with an “N” and an “o”, we accidently wrote it with an “N” and a zero. We find that when we have to type something over and over like this, we are more likely to make a mistake. Further, if we ever need to change the levels or labels, we will have to change them in every factor() function in the code above.\nFor these reasons (and others), programmers of many languages – including R – are taught the DRY principle. DRY is an acronym for don’t repeat yourself. We will discuss the DRY principle again in the chapter on repeated operations, but for now, it just means that you typically don’t want to type code that is the same (or nearly the same) over and over in your programs. Here’s one way we could reduce the repetition in the code above:\n\n# Create a vector of 0/1 levels that can be reused below.\nyn_levs &lt;- c(0, 1)\n# Create a vector of \"No\"/\"Yes\" labels that can be reused below.\nyn_labs &lt;- c(\"No\", \"Yes\")\n\ndrug_trial %&gt;% \n  mutate(\n    se_headache_f  = factor(se_headache, yn_levs, yn_labs),\n    se_diarrhea_f  = factor(se_diarrhea, yn_levs, yn_labs),\n    se_dry_mouth_f = factor(se_dry_mouth, yn_levs, yn_labs)\n  )\n\n# A tibble: 60 × 11\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;\n 1     1     0    65 Active            0           1            1     0\n 2     1     1    65 Active            1           1            1     0\n 3     1     2    65 Active            1           1            0     0\n 4     2     0    49 Active            1           1            1     0\n 5     2     1    49 Active            0           0            1     0\n 6     2     2    49 Active            1           1            1     0\n 7     3     0    48 Placebo           0           0            0     0\n 8     3     1    48 Placebo           0           0            0     0\n 9     3     2    48 Placebo           0           0            0     0\n10     4     0    37 Placebo           0           0            0     0\n# ℹ 50 more rows\n# ℹ 3 more variables: se_headache_f &lt;fct&gt;, se_diarrhea_f &lt;fct&gt;,\n#   se_dry_mouth_f &lt;fct&gt;\n\n\nNotice that in the code above we type c(0, 1) and c(\"No\", \"Yes\") once each instead of 3 times each. In the chapter on repeated operations we will learn techniques for removing even more repetition from the code above.\n\n\n26.5.5 Rowwise mutations\nIn all the examples above we used the values from a single already existing variable in our name-value pair. However, we can also use the values from multiple variables in our name-value pairs.\nFor example, we have three variables in our drug_trial data that capture information about whether or not the participant reported side effects including headache, diarrhea, and dry mouth (sounds like every drug commercial that exists 😂). What if we want to know if our participants reported any side effect at each follow-up? That requires us to combine and transform data from across three different columns! This is one of those situations where there are many different ways we could accomplish this task, but we’re going to use dplyr’s rowwise() function to do so in the following code:\n\ndrug_trial %&gt;% \n  rowwise() %&gt;% \n  mutate(any_se_year = sum(se_headache, se_diarrhea, se_dry_mouth) &gt; 0)\n\n# A tibble: 60 × 9\n# Rowwise: \n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;\n 1     1     0    65 Active            0           1            1     0\n 2     1     1    65 Active            1           1            1     0\n 3     1     2    65 Active            1           1            0     0\n 4     2     0    49 Active            1           1            1     0\n 5     2     1    49 Active            0           0            1     0\n 6     2     2    49 Active            1           1            1     0\n 7     3     0    48 Placebo           0           0            0     0\n 8     3     1    48 Placebo           0           0            0     0\n 9     3     2    48 Placebo           0           0            0     0\n10     4     0    37 Placebo           0           0            0     0\n# ℹ 50 more rows\n# ℹ 1 more variable: any_se_year &lt;lgl&gt;\n\n\n👆Here’s what we did above:\n\nwe created a new column in the drug_trial data called any_se_year using the mutate() function.\nwe used the rowwise() function to tell R to group the data frame by rows. Said another way rowwise() tells R to do any calculations that follow across columns instead within columns. Don’t worry, there are more examples below.\nThe value we passed to the name-value pair inside mutate() was actually the result of two calculations.\n\nFirst, R summed the values of se_headache, se_diarrhea, and se_dry_mouth (i.e., sum(se_headache, se_diarrhea, se_dry_mouth)).\nNext, R compared that the summed value to 0. If the summed value was greater than 0, then the value assigned to any_se_year was TRUE. Otherwise, the value assigned to any_se_year was FALSE.\n\n\nBecause there is some new stuff in the code above, I’m going break it down a little bit further. We’ll start with rowwise(). And, to reduce distractions a much as possible, I’m going to create a new data frame with only the columns we need for this example (sneak peek at the next chapter):\n\ndrug_trial_sub &lt;- drug_trial %&gt;% \n  select(id, year, starts_with(\"se\")) %&gt;% \n  print()\n\n# A tibble: 60 × 5\n      id  year se_headache se_diarrhea se_dry_mouth\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;\n 1     1     0           0           1            1\n 2     1     1           1           1            1\n 3     1     2           1           1            0\n 4     2     0           1           1            1\n 5     2     1           0           0            1\n 6     2     2           1           1            1\n 7     3     0           0           0            0\n 8     3     1           0           0            0\n 9     3     2           0           0            0\n10     4     0           0           0            0\n# ℹ 50 more rows\n\n\nLet’s start by discussing what rowwise() does. As we discussed above, most built-in R functions are vectorized. They do things to entire vectors, and data frame columns are vectors. So, without using rowwise() the sum() function would have returned the value 54:\n\ndrug_trial_sub %&gt;% \n  mutate(any_se_year = sum(se_headache, se_diarrhea, se_dry_mouth))\n\n# A tibble: 60 × 6\n      id  year se_headache se_diarrhea se_dry_mouth any_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n 1     1     0           0           1            1          54\n 2     1     1           1           1            1          54\n 3     1     2           1           1            0          54\n 4     2     0           1           1            1          54\n 5     2     1           0           0            1          54\n 6     2     2           1           1            1          54\n 7     3     0           0           0            0          54\n 8     3     1           0           0            0          54\n 9     3     2           0           0            0          54\n10     4     0           0           0            0          54\n# ℹ 50 more rows\n\n\nAny guesses why it returns 54? Here’s a hint:\n\nsum(c(0, 1, 0))\n\n[1] 1\n\n\n\nsum(c(1, 1, 0))\n\n[1] 2\n\n\n\nsum(\n  c(0, 1, 0),\n  c(1, 1, 0)\n)\n\n[1] 3\n\n\nWhen we pass a single numeric vector to the sum() function, it adds together all the numbers in that function. When we pass two or more numeric vectors to the sum() function, it adds together all the numbers in all the vectors combined. Our data frame columns are no different:\n\nsum(drug_trial_sub$se_headache)\n\n[1] 20\n\n\n\nsum(drug_trial_sub$se_diarrhea)\n\n[1] 16\n\n\n\nsum(drug_trial_sub$se_dry_mouth)\n\n[1] 18\n\n\n\nsum(\n  drug_trial_sub$se_headache,\n  drug_trial_sub$se_diarrhea,\n  drug_trial_sub$se_dry_mouth\n)\n\n[1] 54\n\n\nHopefully, you see that the sum() function is taking the total of all three vectors added together, which is a single number (54), and then using recycling rules to assign that value to every row of any_se_year.\nUsing rowwise() tells R to add across the columns instead of within the columns. So, add the first value for se_headache to the first value for se_diarrhea to the first value for se_dry_mouth, assign that value to the first value of any_se_year, and then repeat for each subsequent row. This is what that result looks like:\n\ndrug_trial_sub %&gt;% \n  rowwise() %&gt;% \n  mutate(any_se_year = sum(se_headache, se_diarrhea, se_dry_mouth))\n\n# A tibble: 60 × 6\n# Rowwise: \n      id  year se_headache se_diarrhea se_dry_mouth any_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n 1     1     0           0           1            1           2\n 2     1     1           1           1            1           3\n 3     1     2           1           1            0           2\n 4     2     0           1           1            1           3\n 5     2     1           0           0            1           1\n 6     2     2           1           1            1           3\n 7     3     0           0           0            0           0\n 8     3     1           0           0            0           0\n 9     3     2           0           0            0           0\n10     4     0           0           0            0           0\n# ℹ 50 more rows\n\n\nBecause the value for each side effect could only be 0 (if not reported) or 1 (if reported) then the rowwise sum of those numbers is a count of the number of side effects reported in each row. For example, person 1 reported not having headaches (0), having diarrhea (1), and having dry mouth (1) at baseline (year == 0). And, 0 + 1 + 1 = 2 – the same value you see for any_se_year in that row. For instructional purposes, let’s run the code above again, but change the name of the variable to n_se_year (i.e., the count of side effects a participant reported in a given year).\nThis may be a useful result in and of itself. However, we said we wanted a variable that captured whether a participant reported any side effect at each follow-up. Well, because any_se_year is currently a count of side effects reported for that participant in that year, then where the value of any_se_year is 0 no side effects were reported. If the current value of any_se_year is greater than 0, then one or more side effects were reported. Generally, we can test inequalities like this in the following way:\n\n# Is 0 greater than 0?\n0 &gt; 0\n\n[1] FALSE\n\n\n\n# Is 2 greater than 0?\n2 &gt; 0\n\n[1] TRUE\n\n\nIn our specific situation, instead of using a number on the left side of the inequality, we can use our calculated n_se_year variable values on the left side of the inequality:\n\ndrug_trial_sub %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    n_se_year   = sum(se_headache, se_diarrhea, se_dry_mouth),\n    any_se_year = n_se_year &gt; 0\n  )\n\n# A tibble: 60 × 7\n# Rowwise: \n      id  year se_headache se_diarrhea se_dry_mouth n_se_year any_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;     &lt;int&gt; &lt;lgl&gt;      \n 1     1     0           0           1            1         2 TRUE       \n 2     1     1           1           1            1         3 TRUE       \n 3     1     2           1           1            0         2 TRUE       \n 4     2     0           1           1            1         3 TRUE       \n 5     2     1           0           0            1         1 TRUE       \n 6     2     2           1           1            1         3 TRUE       \n 7     3     0           0           0            0         0 FALSE      \n 8     3     1           0           0            0         0 FALSE      \n 9     3     2           0           0            0         0 FALSE      \n10     4     0           0           0            0         0 FALSE      \n# ℹ 50 more rows\n\n\nIn this way, any_se_year is TRUE if the participant reported any side effect in that year and false if they reported no side effects in that year. We could write the code more succinctly like this:\n\ndrug_trial_sub %&gt;% \n  rowwise() %&gt;% \n  mutate(any_se_year = sum(se_headache, se_diarrhea, se_dry_mouth) &gt; 0)\n\n# A tibble: 60 × 6\n# Rowwise: \n      id  year se_headache se_diarrhea se_dry_mouth any_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;lgl&gt;      \n 1     1     0           0           1            1 TRUE       \n 2     1     1           1           1            1 TRUE       \n 3     1     2           1           1            0 TRUE       \n 4     2     0           1           1            1 TRUE       \n 5     2     1           0           0            1 TRUE       \n 6     2     2           1           1            1 TRUE       \n 7     3     0           0           0            0 FALSE      \n 8     3     1           0           0            0 FALSE      \n 9     3     2           0           0            0 FALSE      \n10     4     0           0           0            0 FALSE      \n# ℹ 50 more rows\n\n\nBut, is that really what we want to do? The answer is it depends. If we are going to stop here, then the succinct code may be what we want. But, what if we want to also know if the participant reported all side effects in each year. Perhaps, you’ve already worked out what that code would look like. Perhaps you’re thinking something like:\n\ndrug_trial_sub %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    any_se_year = sum(se_headache, se_diarrhea, se_dry_mouth) &gt; 0,\n    all_se_year = sum(se_headache, se_diarrhea, se_dry_mouth) == 3\n  )\n\n# A tibble: 60 × 7\n# Rowwise: \n      id  year se_headache se_diarrhea se_dry_mouth any_se_year all_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;lgl&gt;       &lt;lgl&gt;      \n 1     1     0           0           1            1 TRUE        FALSE      \n 2     1     1           1           1            1 TRUE        TRUE       \n 3     1     2           1           1            0 TRUE        FALSE      \n 4     2     0           1           1            1 TRUE        TRUE       \n 5     2     1           0           0            1 TRUE        FALSE      \n 6     2     2           1           1            1 TRUE        TRUE       \n 7     3     0           0           0            0 FALSE       FALSE      \n 8     3     1           0           0            0 FALSE       FALSE      \n 9     3     2           0           0            0 FALSE       FALSE      \n10     4     0           0           0            0 FALSE       FALSE      \n# ℹ 50 more rows\n\n\nThat works, and hopefully, you’re able to reason out why it works. But, there we go repeating code again! So, in this case, we have to choose between more succinct code and the DRY principle. When presented with that choice, I will typically favor the DRY principle. Therefore, my code would look like this:\n\ndrug_trial_sub %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    n_se_year   = sum(se_headache, se_diarrhea, se_dry_mouth),\n    any_se_year = n_se_year &gt; 0,\n    all_se_year = n_se_year == 3\n  )\n\n# A tibble: 60 × 8\n# Rowwise: \n      id  year se_headache se_diarrhea se_dry_mouth n_se_year any_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;     &lt;int&gt; &lt;lgl&gt;      \n 1     1     0           0           1            1         2 TRUE       \n 2     1     1           1           1            1         3 TRUE       \n 3     1     2           1           1            0         2 TRUE       \n 4     2     0           1           1            1         3 TRUE       \n 5     2     1           0           0            1         1 TRUE       \n 6     2     2           1           1            1         3 TRUE       \n 7     3     0           0           0            0         0 FALSE      \n 8     3     1           0           0            0         0 FALSE      \n 9     3     2           0           0            0         0 FALSE      \n10     4     0           0           0            0         0 FALSE      \n# ℹ 50 more rows\n# ℹ 1 more variable: all_se_year &lt;lgl&gt;\n\n\nNot only am I less like to make a typing error in this code, but I think the differences between each line of code (i.e., what that line of code is doing) stands out more. In other words, the intent of the code isn’t buried in unneeded words.\nBefore moving on, I also want to point out that the method above would not have worked on factors. For example:\n\ndrug_trial_sub %&gt;% \n  mutate(\n    se_headache  = factor(se_headache, yn_levs, yn_labs),\n    se_diarrhea  = factor(se_diarrhea, yn_levs, yn_labs),\n    se_dry_mouth = factor(se_dry_mouth, yn_levs, yn_labs)\n  ) %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    n_se_year   = sum(se_headache, se_diarrhea, se_dry_mouth),\n    any_se_year = n_se_year &gt; 0,\n    all_se_year = n_se_year == 3\n  )\n\nError in `mutate()`:\nℹ In argument: `n_se_year = sum(se_headache, se_diarrhea,\n  se_dry_mouth)`.\nℹ In row 1.\nCaused by error in `Summary.factor()`:\n! 'sum' not meaningful for factors\n\n\nThe sum() function cannot add factors. Back when I first introduced factors in this book, I suggested that you keep the numeric version of your variables in your data frames and create factors as new variables. I said that I thought this was a good idea because I often find that it can be useful to have both versions of the variable hanging around during the analysis process. The situation above is an example of what I was talking about.\n\n\n26.5.6 Group_by mutations\nSo far, we’ve created variables that tell us if our participants reported any side effects in a given and if they reported all 3 side effects in a given year. The next logical question might be to ask if each participant experienced any side effect in any year. For that, we will need dplyr’s group_by() function. Before discussing group_by(), I’m going to show you the code I would use to accomplish this task:\n\ndrug_trial_sub %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    n_se_year   = sum(se_headache, se_diarrhea, se_dry_mouth),\n    any_se_year = n_se_year &gt; 0,\n    all_se_year = n_se_year == 3\n  ) %&gt;% \n  group_by(id) %&gt;% \n  mutate(any_se = sum(any_se_year) &gt; 0)\n\n# A tibble: 60 × 9\n# Groups:   id [20]\n      id  year se_headache se_diarrhea se_dry_mouth n_se_year any_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;     &lt;int&gt; &lt;lgl&gt;      \n 1     1     0           0           1            1         2 TRUE       \n 2     1     1           1           1            1         3 TRUE       \n 3     1     2           1           1            0         2 TRUE       \n 4     2     0           1           1            1         3 TRUE       \n 5     2     1           0           0            1         1 TRUE       \n 6     2     2           1           1            1         3 TRUE       \n 7     3     0           0           0            0         0 FALSE      \n 8     3     1           0           0            0         0 FALSE      \n 9     3     2           0           0            0         0 FALSE      \n10     4     0           0           0            0         0 FALSE      \n# ℹ 50 more rows\n# ℹ 2 more variables: all_se_year &lt;lgl&gt;, any_se &lt;lgl&gt;\n\n\n👆Here’s what we did above:\n\nWe created a new column in the drug_trial_sub data called any_se using the mutate() function. The any_se column is TRUE if the participant reported any side effect in any year and FALSE if they never reported a side effect in any year.\nWe first grouped the data by id using the group_by() function. Note that grouping the data by id with group_by() overrides grouping the data by row with rowwise() as soon as R gets to that point in the code. In other words, the data is grouped by row from rowwise() %&gt;% to group_by(id) %&gt;% and grouped by id after.\n\n\n🗒Side Note: You can use dplyr::ungroup() to ungroup your data frames. This works regardless of whether you grouped them with rowwise() or group_by().\n\nI already introduced group_by() in the chapter on numerical descriptions of categorical variables. I also said that group_by() operationalizes the Split - Apply - Combine strategy for data analysis. That means is that we split our data frame up into smaller data frames, apply our calculation separately to each smaller data frame, and then combine those individual results back together as a single result.\nSo, in the example above, the drug_trial_sub data frame was split into twenty separate little data frames (i.e., one for each study id). Because there are 3 rows for each study id, each of these 20 little data frames had three rows.\nEach of those 20 little data frames was then passed to the mutate() function. The name-value pair inside the mutate() function any_se = sum(any_se_year) &gt; 0 told R to add up all the values for the column any_se_year (i.e., sum(any_se_year)), compare that summed value to 0 (i.e., sum(any_se_year) &gt; 0), and then assign TRUE to any_se if the summed value is greater than zero and FALSE otherwise. Then, all 20 of the little data frames are combined back together and returned to us as a single data frame.\n\ndrug_trial_sub %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    n_se_year   = sum(se_headache, se_diarrhea, se_dry_mouth),\n    any_se_year = n_se_year &gt; 0,\n    all_se_year = n_se_year == 3\n  ) %&gt;% \n  mutate(any_se = sum(any_se_year) &gt; 0)\n\n# A tibble: 60 × 9\n# Rowwise: \n      id  year se_headache se_diarrhea se_dry_mouth n_se_year any_se_year\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;     &lt;int&gt; &lt;lgl&gt;      \n 1     1     0           0           1            1         2 TRUE       \n 2     1     1           1           1            1         3 TRUE       \n 3     1     2           1           1            0         2 TRUE       \n 4     2     0           1           1            1         3 TRUE       \n 5     2     1           0           0            1         1 TRUE       \n 6     2     2           1           1            1         3 TRUE       \n 7     3     0           0           0            0         0 FALSE      \n 8     3     1           0           0            0         0 FALSE      \n 9     3     2           0           0            0         0 FALSE      \n10     4     0           0           0            0         0 FALSE      \n# ℹ 50 more rows\n# ℹ 2 more variables: all_se_year &lt;lgl&gt;, any_se &lt;lgl&gt;\n\n\nYou may be wondering why I used the sum() function when the values for any_se_year are not numbers. The way R treats logical vectors can actually be pretty useful in situations like this. That is, when mathematical operations are applied to logical vectors, R treats FALSE as a 0 and TRUE as a 1. So, for participant 1, R calculated the value for any_se something like this:\n\nany_se_year &lt;- c(TRUE, TRUE, TRUE)\nany_se_year\n\n[1] TRUE TRUE TRUE\n\n\n\nsum_any_se_year &lt;- sum(any_se_year)\nsum_any_se_year\n\n[1] 3\n\n\n\nany_se &lt;- sum_any_se_year &gt; 0\nany_se\n\n[1] TRUE\n\n\nR used the recycling rules to copy that result to the other two rows of data from participant 1. R then repeated that process for every other participant, and then returned the combined data frame to us.\nI hope you found the example above useful. I think it’s fairly representative of the kinds of data management stuff I tend to do on a day-to-day basis. Of course, missing data always complicates things (more to come on that!). In the next chapter, we will round out our introduction to the basics of data management by learning how to subset rows and columns of a data frame.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Creating and Modifying Columns</span>"
    ]
  },
  {
    "objectID": "chapters/subsetting_data_frames/subsetting_data_frames.html",
    "href": "chapters/subsetting_data_frames/subsetting_data_frames.html",
    "title": "27  Subsetting Data Frames",
    "section": "",
    "text": "27.1 The select() function\nSubsetting data frames is another one of the most common data management tasks we carryout in our data analysis projects. Subsetting data frames just refers to the process of deciding which columns and rows to keep in your data frame and which to drop.\nFor example, we may need to subset the rows of a data frame because we’re interested in understanding a subpopulation in our sample. Below, we only want to analyze the rows that correspond to participants from Texas.\nOr, perhaps we’re only interested in a subset of the statistics returned to me in a data frame of analysis results. Below, we only want to view and present the variable name, variable category, count, and percent.\nFortunately, the dplyr package includes functions that make it really easy for us to subset our data frames – even in some fairly complicated ways. Let’s start by simulating the same drug trial data we simulated in the last chapter and use it to work through some examples.\nAs a reminder, we are simulating some drug trial data that includes the following variables:\nActually, this data is slightly different than the data we used in the last chapter. Did you catch the difference? Take another look:\nwe forgot to put a study id in our data. Because we simulated this data above, the best way to fix this oversite is to make the necessary change to the simulation code above. But, let’s pretend that someone sent us this data instead, and we have to add a new study id column to it. Well, we now know how to use the mutate() function to columns to our data frame. We can do so like this:\nAnd now we have the study id in our data. But, by default R adds new columns as the rightmost column of the data frame. In terms of analysis, it doesn’t really matter where this column is located in our data. R couldn’t care less. However, when humans look at this data, they typically expect the study id (or some other identifier) to be the first column in the data frame. That is a job for select().\ndrug_trial %&gt;% \n  select(id, year, age, se_headache, se_diarrhea, se_dry_mouth, mi)\n\n# A tibble: 60 × 7\n      id  year   age se_headache se_diarrhea se_dry_mouth    mi\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;\n 1     1     0    65           0           1            1     0\n 2     1     1    65           1           1            1     0\n 3     1     2    65           1           1            0     0\n 4     2     0    49           1           1            1     0\n 5     2     1    49           0           0            1     0\n 6     2     2    49           1           1            1     0\n 7     3     0    48           0           0            0     0\n 8     3     1    48           0           0            0     0\n 9     3     2    48           0           0            0     0\n10     4     0    37           0           0            0     0\n# ℹ 50 more rows\n👆Here’s what we did above:\nMore generally, the select() function tells R which variables in your data frame to keep (or drop) and in what order.\nThe code above gave us the result we wanted. 👏 But, it can be tedious and error prone to manually type every variable name inside the select() function. Did you notice that we forgot the drug column “by accident”?\nThankfully, the select() function is one of several dplyr functions that accept tidy-select argument modifiers (i.e., functions and operators). In this chapter, we will show you some of the tidy-select argument modifiers we regularly use, but you can always type ?dplyr_tidy_select into your console to see a complete list.\nIn our little example above, we could have used the tidy-select everything() function to make our code easier to write and we wouldn’t have accidently missed the drug column. We can do so like this:\ndrug_trial &lt;- drug_trial %&gt;% \n  select(id, everything()) %&gt;% \n  print()\n\n# A tibble: 60 × 8\n      id  year   age drug    se_headache se_diarrhea se_dry_mouth    mi\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;         &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;\n 1     1     0    65 Active            0           1            1     0\n 2     1     1    65 Active            1           1            1     0\n 3     1     2    65 Active            1           1            0     0\n 4     2     0    49 Active            1           1            1     0\n 5     2     1    49 Active            0           0            1     0\n 6     2     2    49 Active            1           1            1     0\n 7     3     0    48 Placebo           0           0            0     0\n 8     3     1    48 Placebo           0           0            0     0\n 9     3     2    48 Placebo           0           0            0     0\n10     4     0    37 Placebo           0           0            0     0\n# ℹ 50 more rows\n👆Here’s what we did above:\nFor our next example, let’s go ahead and add our mean-centered age variable to our drug_trial data again. We did this for the first time in the last chapter, in case you missed.\ndrug_trial &lt;- drug_trial %&gt;% \n  mutate(age_center = age - mean(age)) %&gt;% \n  print()\n\n# A tibble: 60 × 9\n      id  year   age drug  se_headache se_diarrhea se_dry_mouth    mi age_center\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n 1     1     0    65 Acti…           0           1            1     0       11.3\n 2     1     1    65 Acti…           1           1            1     0       11.3\n 3     1     2    65 Acti…           1           1            0     0       11.3\n 4     2     0    49 Acti…           1           1            1     0       -4.7\n 5     2     1    49 Acti…           0           0            1     0       -4.7\n 6     2     2    49 Acti…           1           1            1     0       -4.7\n 7     3     0    48 Plac…           0           0            0     0       -5.7\n 8     3     1    48 Plac…           0           0            0     0       -5.7\n 9     3     2    48 Plac…           0           0            0     0       -5.7\n10     4     0    37 Plac…           0           0            0     0      -16.7\n# ℹ 50 more rows\nOne way we will often use select() is for performing quick little data checks. For example, let’s say that we wanted to make sure the code we wrote above actually did what we intended it to do. If we print the entire data frame to the screen, age and age_center aren’t directly side-by-side, and there’s a lot of other visual clutter from the other variables. In a case like this, we would use select() to get a clearer picture:\ndrug_trial %&gt;% \n  select(age, age_center)\n\n# A tibble: 60 × 2\n     age age_center\n   &lt;int&gt;      &lt;dbl&gt;\n 1    65       11.3\n 2    65       11.3\n 3    65       11.3\n 4    49       -4.7\n 5    49       -4.7\n 6    49       -4.7\n 7    48       -5.7\n 8    48       -5.7\n 9    48       -5.7\n10    37      -16.7\n# ℹ 50 more rows\n👆Here’s what we did above:\n⚠️Warning: Notice that we didn’t assign our result above to anything (i.e., there’s no drug_trial &lt;-). If we had done so, the drug_trial data would have contained these two columns only. We didn’t want to drop the other columns. We could have assigned the result of the code to a different R object (e.g., check_age &lt;-, but it wasn’t really necessary. We just wanted to quickly view age and age_center side-by-side for data checking purposes. When we’re satisfied that we coded it correctly, we can move on. There’s no need to save those results to an R object.\nYou may also recall that we wanted to subset the drug_trial data to include only the columns we needed for the rowwise demonstrations. Here is the code we used to do so:\ndrug_trial %&gt;% \n  select(id, year, starts_with(\"se\"))\n\n# A tibble: 60 × 5\n      id  year se_headache se_diarrhea se_dry_mouth\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;\n 1     1     0           0           1            1\n 2     1     1           1           1            1\n 3     1     2           1           1            0\n 4     2     0           1           1            1\n 5     2     1           0           0            1\n 6     2     2           1           1            1\n 7     3     0           0           0            0\n 8     3     1           0           0            0\n 9     3     2           0           0            0\n10     4     0           0           0            0\n# ℹ 50 more rows\n👆Here’s what we did above:\nwe already know that we can use everything() to select all of the other variables in a data frame, but what if we just want to grab a range or group of other variables in a data frame? tidy-select makes it easy for us. Above, we used the starts_with() function to select all the columns with names that literally start with the letters “se”. Because all of the side effect columns are directly next to each other (i.e., no columns in between them) we could have also used the colon operator : like this:\ndrug_trial %&gt;% \n  select(id, year, se_headache:se_dry_mouth)\n\n# A tibble: 60 × 5\n      id  year se_headache se_diarrhea se_dry_mouth\n   &lt;int&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;\n 1     1     0           0           1            1\n 2     1     1           1           1            1\n 3     1     2           1           1            0\n 4     2     0           1           1            1\n 5     2     1           0           0            1\n 6     2     2           1           1            1\n 7     3     0           0           0            0\n 8     3     1           0           0            0\n 9     3     2           0           0            0\n10     4     0           0           0            0\n# ℹ 50 more rows\nWhile either method gets us the same result, we tend to prefer using starts_with() when possible. We think it makes your code easier to read (i.e., “Oh, he’s selecting all the side effect columns here.”).\nIn addition to starts_with(), there is also an ends_with() tidy-select function that can also be useful. For example, we’ve named factors with the _f naming convention throughout the book. We could use that, along with the ends-with() function to create a subset of our data that includes only the factor versions of our side effects columns.\n# Add the side effect factor columns to our data frame again...\nyn_levs &lt;- c(0, 1)\nyn_labs &lt;- c(\"No\", \"Yes\")\n\ndrug_trial &lt;- drug_trial %&gt;% \n  mutate(\n    se_headache_f  = factor(se_headache, yn_levs, yn_labs),\n    se_diarrhea_f  = factor(se_diarrhea, yn_levs, yn_labs),\n    se_dry_mouth_f = factor(se_dry_mouth, yn_levs, yn_labs)\n  )\ndrug_trial %&gt;% \n  select(id, year, ends_with(\"_f\"))\n\n# A tibble: 60 × 5\n      id  year se_headache_f se_diarrhea_f se_dry_mouth_f\n   &lt;int&gt; &lt;int&gt; &lt;fct&gt;         &lt;fct&gt;         &lt;fct&gt;         \n 1     1     0 No            Yes           Yes           \n 2     1     1 Yes           Yes           Yes           \n 3     1     2 Yes           Yes           No            \n 4     2     0 Yes           Yes           Yes           \n 5     2     1 No            No            Yes           \n 6     2     2 Yes           Yes           Yes           \n 7     3     0 No            No            No            \n 8     3     1 No            No            No            \n 9     3     2 No            No            No            \n10     4     0 No            No            No            \n# ℹ 50 more rows\nwe can also select columns we want to keep by position instead of name. We don’t do this often. We think it’s generally better to use column names or tidy-select argument modifiers when subsetting columns in your data frame. However, we do sometimes select columns by position when we’re writing our own functions. Therefore, we want to quickly show you what this looks like:\ndrug_trial %&gt;% \n  select(1:2, 4)\n\n# A tibble: 60 × 3\n      id  year drug   \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;  \n 1     1     0 Active \n 2     1     1 Active \n 3     1     2 Active \n 4     2     0 Active \n 5     2     1 Active \n 6     2     2 Active \n 7     3     0 Placebo\n 8     3     1 Placebo\n 9     3     2 Placebo\n10     4     0 Placebo\n# ℹ 50 more rows\n👆Here’s what we did above:\nFinally, in addition to using select() to keep columns in our data frame, we can also use select() to explicitly drop columns from our data frame. To do so, we just need to use either the subtraction symbol (-) or the Not operator (!).\nThink back to our example from the previous chapter. There we created some new variables that captured information about participants reporting any and all side effects. During that process we created a column that contained a count of the side effects experienced in each year – n_se_year.\ndrug_trial_sub &lt;- drug_trial %&gt;% \n  rowwise() %&gt;% \n  mutate(\n    n_se_year   = sum(se_headache, se_diarrhea, se_dry_mouth),\n    any_se_year = n_se_year &gt; 0,\n    all_se_year = n_se_year == 3\n  ) %&gt;% \n  group_by(id) %&gt;% \n  mutate(any_se = sum(any_se_year) &gt; 0) %&gt;% \n  ungroup() %&gt;% \n  select(id:year, n_se_year:any_se) %&gt;% \n  print()\n\n# A tibble: 60 × 6\n      id  year n_se_year any_se_year all_se_year any_se\n   &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;lgl&gt;       &lt;lgl&gt;       &lt;lgl&gt; \n 1     1     0         2 TRUE        FALSE       TRUE  \n 2     1     1         3 TRUE        TRUE        TRUE  \n 3     1     2         2 TRUE        FALSE       TRUE  \n 4     2     0         3 TRUE        TRUE        TRUE  \n 5     2     1         1 TRUE        FALSE       TRUE  \n 6     2     2         3 TRUE        TRUE        TRUE  \n 7     3     0         0 FALSE       FALSE       FALSE \n 8     3     1         0 FALSE       FALSE       FALSE \n 9     3     2         0 FALSE       FALSE       FALSE \n10     4     0         0 FALSE       FALSE       FALSE \n# ℹ 50 more rows\nLet’s say we decided we don’t need n_se_year column now that we created any_se_year, all_se_year, and any_se. We can easily drop it from the data frame in a couple of ways:\ndrug_trial_sub %&gt;% \n  select(-n_se_year)\n\n# A tibble: 60 × 5\n      id  year any_se_year all_se_year any_se\n   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;       &lt;lgl&gt;       &lt;lgl&gt; \n 1     1     0 TRUE        FALSE       TRUE  \n 2     1     1 TRUE        TRUE        TRUE  \n 3     1     2 TRUE        FALSE       TRUE  \n 4     2     0 TRUE        TRUE        TRUE  \n 5     2     1 TRUE        FALSE       TRUE  \n 6     2     2 TRUE        TRUE        TRUE  \n 7     3     0 FALSE       FALSE       FALSE \n 8     3     1 FALSE       FALSE       FALSE \n 9     3     2 FALSE       FALSE       FALSE \n10     4     0 FALSE       FALSE       FALSE \n# ℹ 50 more rows\ndrug_trial_sub %&gt;% \n  select(!n_se_year)\n\n# A tibble: 60 × 5\n      id  year any_se_year all_se_year any_se\n   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;       &lt;lgl&gt;       &lt;lgl&gt; \n 1     1     0 TRUE        FALSE       TRUE  \n 2     1     1 TRUE        TRUE        TRUE  \n 3     1     2 TRUE        FALSE       TRUE  \n 4     2     0 TRUE        TRUE        TRUE  \n 5     2     1 TRUE        FALSE       TRUE  \n 6     2     2 TRUE        TRUE        TRUE  \n 7     3     0 FALSE       FALSE       FALSE \n 8     3     1 FALSE       FALSE       FALSE \n 9     3     2 FALSE       FALSE       FALSE \n10     4     0 FALSE       FALSE       FALSE \n# ℹ 50 more rows\nNote that we could have also dropped it indirectly by selecting everything else:\ndrug_trial_sub %&gt;% \n  select(id:year, any_se_year:any_se)\n\n# A tibble: 60 × 5\n      id  year any_se_year all_se_year any_se\n   &lt;int&gt; &lt;int&gt; &lt;lgl&gt;       &lt;lgl&gt;       &lt;lgl&gt; \n 1     1     0 TRUE        FALSE       TRUE  \n 2     1     1 TRUE        TRUE        TRUE  \n 3     1     2 TRUE        FALSE       TRUE  \n 4     2     0 TRUE        TRUE        TRUE  \n 5     2     1 TRUE        FALSE       TRUE  \n 6     2     2 TRUE        TRUE        TRUE  \n 7     3     0 FALSE       FALSE       FALSE \n 8     3     1 FALSE       FALSE       FALSE \n 9     3     2 FALSE       FALSE       FALSE \n10     4     0 FALSE       FALSE       FALSE \n# ℹ 50 more rows\nBut, we think this is generally a bad idea. Not only is it more typing, but skimming through your code doesn’t really tell us (or future you) what you were trying to accomplish there.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Subsetting Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/subsetting_data_frames/subsetting_data_frames.html#the-select-function",
    "href": "chapters/subsetting_data_frames/subsetting_data_frames.html#the-select-function",
    "title": "27  Subsetting Data Frames",
    "section": "",
    "text": "we used the select() function to change the order of the columns in the drug_trial data frame so that id would be the first variable in the data frame when reading from left to right.\nYou can type ?select into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the select() function is .data. The value passed to .data should always be a data frame. In this book, we will often pass data frames to the .data argument using the pipe operator (e.g., df %&gt;% select()).\nThe second argument to the select() function is .... The value passed to the ... argument should column names or expressions that return column positions. We’ll dive deeper into this soon.\n\n\n\n\n\n\n\n\nwe used the select() function to change the order of the columns in the drug_trial data frame so that id would be the first variable in the data frame when reading from left to right.\nRather than explicitly typing the other column names, we used the everything() tidy-select function. As you may have guessed, everything() tells R to do X (in this keep) to all the other variables not explicitly mentioned.\n\n\n\n\n\n\n\nwe used the select() function to view the age and age_center columns only.\nwe can type individual column names, separated by commas, into select() to return a data frame containing only those columns, and in that order.\n\n\n\n\n\n\nwe used the select() function to view the id year, se_headache, se_diarrhea, and se_dry_mouth columns only.\nwe used the tidy-select starts_with() function to select all the side effect variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVariable names are important! Throughout this book, I’ve tried to repeatedly emphasize the importance of coding style – including the way we name our R objects. Many people who are new to data management and analysis (and some who aren’t, MDL) don’t fully appreciate the importance of such things. We hope that the preceding two examples are helping you to see why the little details, like variable names, are important. Using consistent variable naming conventions, for example, allows us to write code that requires less typing, is easier for humans to skim and understand, and is less prone to typos and other related errors.\n\n\n\n\n\n\nwe passed column numbers to the select() function to keep the 1st, 2nd, and 4th columns from our drug_trial data frame.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Subsetting Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/subsetting_data_frames/subsetting_data_frames.html#the-rename-function",
    "href": "chapters/subsetting_data_frames/subsetting_data_frames.html#the-rename-function",
    "title": "27  Subsetting Data Frames",
    "section": "27.2 The rename() function",
    "text": "27.2 The rename() function\nSometimes, we want to change the names of some, or all, of the columns in our data frame. For me, this most commonly comes up with data I’ve imported from someone else. For example, let’s say I’m importing data that uses column names that aren’t super informative. We saw column names like that when we imported NHANES data. It looked something like this:\n\nnhanes &lt;- tibble(\n  SEQN = c(1:4),\n  ALQ101 = c(1, 2, 1, 2),\n  ALQ110 = c(2, 2, 2, 1)\n) %&gt;% \n  print()\n\n# A tibble: 4 × 3\n   SEQN ALQ101 ALQ110\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1      1      2\n2     2      2      2\n3     3      1      2\n4     4      2      1\n\n\nwe previously learned how to change these column names on import (i.e., col_names), but let’s say we didn’t do that for whatever reason. We can rename columns in our data frame using the rename() function like so:\n\nnhanes %&gt;% \n  rename(\n    id = SEQN,\n    drinks_12_year = ALQ101,\n    drinks_12_life = ALQ110\n  )\n\n# A tibble: 4 × 3\n     id drinks_12_year drinks_12_life\n  &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1     1              1              2\n2     2              2              2\n3     3              1              2\n4     4              2              1\n\n\n👆Here’s what we did above:\n\nwe used the rename() function to change the name of each column in the drug_trial data frame to be more informative.\nYou can type ?rename into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the rename() function is .data. The value passed to .data should always be a data frame. In this book, we will often pass data frames to the .data argument using the pipe operator (e.g., df %&gt;% rename()).\nThe second argument to the rename() function is .... The value passed to the ... argument should be a name value pair, or series of name-value pairs separated by columns. The name-value pairs should be in the format new name = original name.\n\nwe think these names are much better, but for the sake of argument let’s say that we wanted to keep the original names – just coerce them to lowercase. We can do that using the rename_with() variation of the rename() function in combination with the tolower() function:\n\nnhanes %&gt;% \n  rename_with(tolower)\n\n# A tibble: 4 × 3\n   seqn alq101 alq110\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1      1      2\n2     2      2      2\n3     3      1      2\n4     4      2      1\n\n\n👆Here’s what we did above:\n\nwe used the rename_with() function to coerce all column names in the drug_trial data frame to lowercase.\nYou can type ?rename into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the rename_with() function is .data. The value passed to .data should always be a data frame. In this book, we will often pass data frames to the .data argument using the pipe operator (e.g., df %&gt;% rename_with()).\nThe second argument to the rename_with() function is .fn. The value passed to the .fn argument should be a function that you want to apply to all the columns selected in the .cols argument (see below).\nThe third argument to the rename_with() function is .cols. The value passed to the .cols argument should be the columns you want to apply the function passed to the .fn argument to. You can select the columns using tidy-select argument modifiers.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Subsetting Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/subsetting_data_frames/subsetting_data_frames.html#the-filter-function",
    "href": "chapters/subsetting_data_frames/subsetting_data_frames.html#the-filter-function",
    "title": "27  Subsetting Data Frames",
    "section": "27.3 The filter() function",
    "text": "27.3 The filter() function\nwe just saw how to keep and drop columns in our data frame using the select() function. We can keep and drop rows in our data frame using the filter() function or the slice() function.\nSimilar to selecting columns by position instead of name:\n\ndrug_trial %&gt;% \n  select(1:2, 4)\n\n# A tibble: 60 × 3\n      id  year drug   \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;  \n 1     1     0 Active \n 2     1     1 Active \n 3     1     2 Active \n 4     2     0 Active \n 5     2     1 Active \n 6     2     2 Active \n 7     3     0 Placebo\n 8     3     1 Placebo\n 9     3     2 Placebo\n10     4     0 Placebo\n# ℹ 50 more rows\n\n\nwe can also select rows we want to keep by position. Again, we don’t do this often, but it is sometimes useful when we’re writing our own functions. Therefore, we want to quickly show you what this looks like:\n\ndrug_trial %&gt;% \n  slice(1:5)\n\n# A tibble: 5 × 12\n     id  year   age drug   se_headache se_diarrhea se_dry_mouth    mi age_center\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1     1     0    65 Active           0           1            1     0       11.3\n2     1     1    65 Active           1           1            1     0       11.3\n3     1     2    65 Active           1           1            0     0       11.3\n4     2     0    49 Active           1           1            1     0       -4.7\n5     2     1    49 Active           0           0            1     0       -4.7\n# ℹ 3 more variables: se_headache_f &lt;fct&gt;, se_diarrhea_f &lt;fct&gt;,\n#   se_dry_mouth_f &lt;fct&gt;\n\n\n👆Here’s what we did above:\n\nwe used the slice() function to keep only the first 5 rows in the drug_trial data frame.\nYou can type ?slice into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the slice() function is .data. The value passed to .data should always be a data frame. In this book, we will often pass data frames to the .data argument using the pipe operator (e.g., df %&gt;% slice()).\nThe second argument to the slice() function is .... The value passed to the ... argument should be a row numbers you want returned to you.\n\nGenerally speaking, we’re far more likely to use the filter() function to select only a subset of rows from our data frame. Two of the most common scenarios, of many possible scenarios, where want to subset rows include:\n\nPerforming a subgroup analysis. This is a situation where we want our analysis to include only some of the people (or places, or things) in our data frame.\nPerforming a complete case analysis. This is a situation where we want to remove rows that contain missing values from our data frame before performing an analysis.\n\n\n27.3.1 Subgroup analysis\nLet’s say that we want to count the number of people in the drug trial who reported having headaches in the baseline year by drug status (active vs. placebo). We would first use filter() to keep only the rows that contain data from the baseline year:\n\ndrug_trial %&gt;% \n  filter(year == 0)\n\n# A tibble: 20 × 12\n      id  year   age drug  se_headache se_diarrhea se_dry_mouth    mi age_center\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n 1     1     0    65 Acti…           0           1            1     0     11.3  \n 2     2     0    49 Acti…           1           1            1     0     -4.7  \n 3     3     0    48 Plac…           0           0            0     0     -5.7  \n 4     4     0    37 Plac…           0           0            0     0    -16.7  \n 5     5     0    71 Plac…           0           0            0     0     17.3  \n 6     6     0    48 Plac…           0           0            0     0     -5.7  \n 7     7     0    59 Acti…           1           1            1     0      5.3  \n 8     8     0    60 Plac…           0           0            0     0      6.3  \n 9     9     0    61 Acti…           1           1            1     0      7.3  \n10    10     0    39 Acti…           1           0            1     0    -14.7  \n11    11     0    61 Plac…           0           0            0     0      7.3  \n12    12     0    62 Plac…           1           0            1     0      8.3  \n13    13     0    43 Plac…           0           0            0     0    -10.7  \n14    14     0    63 Plac…           0           0            0     0      9.3  \n15    15     0    69 Acti…           1           1            1     0     15.3  \n16    16     0    42 Plac…           0           0            0     0    -11.7  \n17    17     0    60 Plac…           0           0            0     0      6.3  \n18    18     0    41 Acti…           1           1            1     0    -12.7  \n19    19     0    43 Plac…           0           0            0     0    -10.7  \n20    20     0    53 Plac…           0           0            0     0     -0.700\n# ℹ 3 more variables: se_headache_f &lt;fct&gt;, se_diarrhea_f &lt;fct&gt;,\n#   se_dry_mouth_f &lt;fct&gt;\n\n\n👆Here’s what we did above:\n\nwe used the filter() function to keep only the rows in the drug_trial data frame that contain data from the baseline year.\nYou can type ?filter into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the filter() function is .data. The value passed to .data should always be a data frame. In this book, we will often pass data frames to the .data argument using the pipe operator (e.g., df %&gt;% filter()).\nThe second argument to the filter() function is .... The value passed to the ... argument should be a name-value pair or multiple name value pairs separated by commas. The ... argument is where you will tell filter() how to decide which rows to keep.\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember, that in the R language = (i.e., one equal sign) and == (i.e., two equal signs) are different things. The = operator tells R to make the thing on the left equal to the thing on the right. In other words, it assigns values. The == asks R if the thing on the left is equal to the thing on the right. In other words, it test the equality of values.\n\n\nNow, we can use the descriptive analysis techniques we’ve already learned to answer our research question:\n\ndrug_trial %&gt;% \n  filter(year == 0) %&gt;% \n  group_by(drug, se_headache_f) %&gt;% \n  summarise(n = n())\n\n# A tibble: 4 × 3\n# Groups:   drug [2]\n  drug    se_headache_f     n\n  &lt;chr&gt;   &lt;fct&gt;         &lt;int&gt;\n1 Active  No                1\n2 Active  Yes               6\n3 Placebo No               12\n4 Placebo Yes               1\n\n\nSo, 6 out of 7 (~ 86%) of the people in our active drug group reported headaches in the baseline year. Now, let’s say that we have reason to suspect that the drug affects people differently based on their age. Let’s go ahead and repeat this analysis, but only in a subgroup of people who are below age 65. Again, we can use the filter() function to do this:\n\ndrug_trial %&gt;% \n  filter(year == 0) %&gt;% \n  filter(age &lt; 65) %&gt;% \n  group_by(drug, se_headache_f) %&gt;% \n  summarise(n = n())\n\n# A tibble: 3 × 3\n# Groups:   drug [2]\n  drug    se_headache_f     n\n  &lt;chr&gt;   &lt;fct&gt;         &lt;int&gt;\n1 Active  Yes               5\n2 Placebo No               11\n3 Placebo Yes               1\n\n\nWow! It looks like everyone under age 65 who received active drug also reported headaches!\nwe can show this more explicitly by using passing the value FALSE to the .drop argument of group_by(). This tells R to keep all factor levels in the output, even if they were observed in the data zero times.\n\ndrug_trial %&gt;% \n  filter(year == 0) %&gt;% \n  filter(age &lt; 65) %&gt;% \n  group_by(drug, se_headache_f, .drop = FALSE) %&gt;% \n  summarise(n = n())\n\n# A tibble: 4 × 3\n# Groups:   drug [2]\n  drug    se_headache_f     n\n  &lt;chr&gt;   &lt;fct&gt;         &lt;int&gt;\n1 Active  No                0\n2 Active  Yes               5\n3 Placebo No               11\n4 Placebo Yes               1\n\n\nFinally, we could make our code above more succinct by combining our two filter functions into one:\n\ndrug_trial %&gt;% \n  filter(year == 0 & age &lt; 65) %&gt;% \n  group_by(drug, se_headache_f, .drop = FALSE) %&gt;% \n  summarise(n = n())\n\n# A tibble: 4 × 3\n# Groups:   drug [2]\n  drug    se_headache_f     n\n  &lt;chr&gt;   &lt;fct&gt;         &lt;int&gt;\n1 Active  No                0\n2 Active  Yes               5\n3 Placebo No               11\n4 Placebo Yes               1\n\n\n👆Here’s what we did above:\n\nwe used the filter() function to keep only the rows in the drug_trial data frame that contain data from the baseline year AND (&) contain data from rows with a value that is less than 65 in the age column. The AND (&) here is important. A row must satisfy both of these conditions in order for R to keep it in the returned data frame. If we had used OR instead (filter(year == 0 | age &lt; 65)), then only one condition OR the other would need to be met for R to keep the row in the returned data frame.\n\n\n\n\n\n\n\nNote\n\n\n\nIn the R language, we use the pipe operator to create OR conditions. The pipe operator looks like | and is probably the key immediately to the right of your enter/return key on your keyboard.\n\n\n\n\n27.3.2 Complete case analysis\nNow let’s say that we want to compare age at baseline by drug status (active vs. placebo). Additionally, let’s say that we have some missing values in our data.\nLet’s first simulate some new data with missing values:\n\ndrug_trial_short &lt;- drug_trial %&gt;%\n  filter(year == 0) %&gt;% \n  slice(1:10) %&gt;% \n  mutate(\n    age  = replace(age, 1, NA),\n    drug = replace(drug, 4, NA)\n  ) %&gt;% \n  print()\n\n# A tibble: 10 × 12\n      id  year   age drug  se_headache se_diarrhea se_dry_mouth    mi age_center\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n 1     1     0    NA Acti…           0           1            1     0       11.3\n 2     2     0    49 Acti…           1           1            1     0       -4.7\n 3     3     0    48 Plac…           0           0            0     0       -5.7\n 4     4     0    37 &lt;NA&gt;            0           0            0     0      -16.7\n 5     5     0    71 Plac…           0           0            0     0       17.3\n 6     6     0    48 Plac…           0           0            0     0       -5.7\n 7     7     0    59 Acti…           1           1            1     0        5.3\n 8     8     0    60 Plac…           0           0            0     0        6.3\n 9     9     0    61 Acti…           1           1            1     0        7.3\n10    10     0    39 Acti…           1           0            1     0      -14.7\n# ℹ 3 more variables: se_headache_f &lt;fct&gt;, se_diarrhea_f &lt;fct&gt;,\n#   se_dry_mouth_f &lt;fct&gt;\n\n\n👆Here’s what we did above:\n\nwe used the filter() and slice() functions to create a new data frame that contains only a subset of our original drug_trial data frame. The subset includes only the first 10 rows of the data frame remaining after selecting only the baseline year rows from the original data frame.\nwe used the replace() function to replace the first value of age with NA and the fourth value of drug with NA.\nYou can type ?replace into your R console to view the help documentation for this function.\n\nIf we try to answer our research question above without dealing with the missing data, we get the following undesirable results:\n\ndrug_trial_short %&gt;% \n  group_by(drug) %&gt;% \n  summarise(mean_age = mean(age))\n\n# A tibble: 3 × 2\n  drug    mean_age\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Active      NA  \n2 Placebo     56.8\n3 &lt;NA&gt;        37  \n\n\nOne way we can improve our result is by adding the na.rm argument to the mean() function.\n\ndrug_trial_short %&gt;% \n  group_by(drug) %&gt;% \n  summarise(mean_age = mean(age, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  drug    mean_age\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Active      52  \n2 Placebo     56.8\n3 &lt;NA&gt;        37  \n\n\nBut, we previously saw how it can sometimes be more efficient to drop the row with missing data from the data frame explicitly. This is called a complete case analysis or list-wise deletion.\n\ndrug_trial_short %&gt;% \n  filter(!is.na(age)) %&gt;% \n  group_by(drug) %&gt;% \n  summarise(mean_age = mean(age))\n\n# A tibble: 3 × 2\n  drug    mean_age\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Active      52  \n2 Placebo     56.8\n3 &lt;NA&gt;        37  \n\n\nHowever, we still have that missing value for drug. We can easily drop the row with the missing value by adding an additional value to the ... argument of our filter() function:\n\ndrug_trial_short %&gt;% \n  filter(!is.na(age) & !is.na(drug)) %&gt;% \n  group_by(drug) %&gt;% \n  summarise(mean_age = mean(age))\n\n# A tibble: 2 × 2\n  drug    mean_age\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Active      52  \n2 Placebo     56.8",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Subsetting Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/subsetting_data_frames/subsetting_data_frames.html#deduplication",
    "href": "chapters/subsetting_data_frames/subsetting_data_frames.html#deduplication",
    "title": "27  Subsetting Data Frames",
    "section": "27.4 Deduplication",
    "text": "27.4 Deduplication\nAnother common data management task that we want to discuss in this chapter is deduplicating data. Let’s go ahead and simulate some data to illustrate what we mean:\n\ndf &lt;- tribble(\n  ~id, ~day, ~x,\n    1, 1, 1,\n    1, 2, 11,\n    2, 1, 12,\n    2, 2, 13,\n    2, 2, 14,\n    3, 1, 12,\n    3, 1, 12,\n    3, 2, 13,\n    4, 1, 13,\n    5, 1, 10,\n    5, 2, 11,\n    5, 1, 10\n) %&gt;% \n  print()\n\n# A tibble: 12 × 3\n      id   day     x\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1     1\n 2     1     2    11\n 3     2     1    12\n 4     2     2    13\n 5     2     2    14\n 6     3     1    12\n 7     3     1    12\n 8     3     2    13\n 9     4     1    13\n10     5     1    10\n11     5     2    11\n12     5     1    10\n\n\n\nAll id’s but 4 have multiple observations.\nID 2 has row with duplicate values for id and day, but a non-duplicate value for x. These rows are partial duplicates.\nID 3 has a row with duplicate values for all three columns (i.e., 3, 1, 12). These rows are complete duplicates.\nID 5 has a row with duplicate values for all three columns (i.e., 5, 1, 10). These rows are complete duplicates. However, they are not in sequential order in the dataset.\n\n\n27.4.1 The distinct() function\nwe can use dplyr’s distinct() function to remove all complete duplicates from the data frame:\n\ndf %&gt;% \n  distinct()\n\n# A tibble: 10 × 3\n      id   day     x\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1     1\n 2     1     2    11\n 3     2     1    12\n 4     2     2    13\n 5     2     2    14\n 6     3     1    12\n 7     3     2    13\n 8     4     1    13\n 9     5     1    10\n10     5     2    11\n\n\n👆Here’s what we did above:\n\nwe used the distinct() function to keep only one row from a group of complete duplicate rows in the df data frame.\nYou can type ?distinct into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the distinct() function is .data. The value passed to .data should always be a data frame. In this book, we will often pass data frames to the .data argument using the pipe operator (e.g., df %&gt;% distinct()).\nThe second argument to the distinct() function is .... The value passed to the ... argument should be the variables to use when determining uniqueness. Passing no variables to the ... argument is equivalent to pass all variables to the ... argument.\n\n\n\n27.4.2 Complete duplicate row add tag\nIf want to identify the complete duplicate rows, without immediately dropping them, we can use the duplicated() function inside the mutate() function. This creates a new column in our data frame that has the value TRUE when the row is a complete duplicate and the value FALSE otherwise.\n\ndf %&gt;% \n  mutate(dup = duplicated(df))\n\n# A tibble: 12 × 4\n      id   day     x dup  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;\n 1     1     1     1 FALSE\n 2     1     2    11 FALSE\n 3     2     1    12 FALSE\n 4     2     2    13 FALSE\n 5     2     2    14 FALSE\n 6     3     1    12 FALSE\n 7     3     1    12 TRUE \n 8     3     2    13 FALSE\n 9     4     1    13 FALSE\n10     5     1    10 FALSE\n11     5     2    11 FALSE\n12     5     1    10 TRUE \n\n\nAlternatively, we could get the same result using:\n\ndf %&gt;% \n  group_by_all() %&gt;% \n  mutate(\n    n_row = row_number(),\n    dup   = n_row &gt; 1\n  )\n\n# A tibble: 12 × 5\n# Groups:   id, day, x [10]\n      id   day     x n_row dup  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;\n 1     1     1     1     1 FALSE\n 2     1     2    11     1 FALSE\n 3     2     1    12     1 FALSE\n 4     2     2    13     1 FALSE\n 5     2     2    14     1 FALSE\n 6     3     1    12     1 FALSE\n 7     3     1    12     2 TRUE \n 8     3     2    13     1 FALSE\n 9     4     1    13     1 FALSE\n10     5     1    10     1 FALSE\n11     5     2    11     1 FALSE\n12     5     1    10     2 TRUE \n\n\n👆Here’s what we did above:\n\nwe used the group_by_all() function to split our data frame into multiple data frames grouped by all the columns in df.\nwe used the row_number() to sequentially count every row in each of the little data frames created by group_by_all(). We assigned the sequential count to a new column named n_row.\nwe created a new column named dup that has a value of TRUE when the value of n_row is greater than 1 and FALSE otherwise.\n\nNotice that R only tags the second in a set of duplicate rows as a duplicate. Below we tag both rows with complete duplicate values.\n\ndf %&gt;% \n  mutate(dup = duplicated(.) | duplicated(., fromLast = TRUE))\n\n# A tibble: 12 × 4\n      id   day     x dup  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;\n 1     1     1     1 FALSE\n 2     1     2    11 FALSE\n 3     2     1    12 FALSE\n 4     2     2    13 FALSE\n 5     2     2    14 FALSE\n 6     3     1    12 TRUE \n 7     3     1    12 TRUE \n 8     3     2    13 FALSE\n 9     4     1    13 FALSE\n10     5     1    10 TRUE \n11     5     2    11 FALSE\n12     5     1    10 TRUE \n\n\n\n\n27.4.3 Partial duplicate rows\n\ndf %&gt;% \n  distinct(id, day, .keep_all = TRUE)\n\n# A tibble: 9 × 3\n     id   day     x\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1\n2     1     2    11\n3     2     1    12\n4     2     2    13\n5     3     1    12\n6     3     2    13\n7     4     1    13\n8     5     1    10\n9     5     2    11\n\n\n👆Here’s what we did above:\n\nwe used the distinct() function to keep only one row from a group of duplicate rows in the df data frame.\nYou can type ?distinct into your R console to view the help documentation for this function and follow along with the explanation below.\nThis time we passed the column names id and day to the ... argument. This tells R to consider any rows that have the same value of id AND day to be duplicates – even if they have different values in their other columns.\nThe .keep_all argument tells R to return all of the columns in df to us – not just the columns that we are testing for uniqueness (i.e., id and day).\n\n\n\n27.4.4 Partial duplicate rows - add tag\nwe can tag partial duplicate rows in a similar fashion to the way we tagged complete duplicate rows above:\n\ndf %&gt;% \n  group_by(id, day) %&gt;% \n  mutate(\n    count = row_number(), # Counts rows by group\n    dup   = count &gt; 1     # TRUE if there is more than one row per group\n  )\n\n# A tibble: 12 × 5\n# Groups:   id, day [9]\n      id   day     x count dup  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;\n 1     1     1     1     1 FALSE\n 2     1     2    11     1 FALSE\n 3     2     1    12     1 FALSE\n 4     2     2    13     1 FALSE\n 5     2     2    14     2 TRUE \n 6     3     1    12     1 FALSE\n 7     3     1    12     2 TRUE \n 8     3     2    13     1 FALSE\n 9     4     1    13     1 FALSE\n10     5     1    10     1 FALSE\n11     5     2    11     1 FALSE\n12     5     1    10     2 TRUE \n\n\n\n\n27.4.5 Count the number of duplicates\nFinally, sometimes it can be useful to get a count of the number of duplicate rows. The code below returns a data frame that summarizes the number of rows that contain duplicate values for id and day, and what those duplicate values are.\n\ndf %&gt;% \n  group_by(id, day) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  count()\n\n# A tibble: 3 × 3\n# Groups:   id, day [3]\n     id   day     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     2     2     2\n2     3     1     2\n3     5     1     2\n\n\n\n\n27.4.6 What to do about duplicates\nFinding duplicates is only half the battle. After finding them, you have to decide what to do about them. In some ways it’s hard to give clear-cut advice on this because different situations require different decisions. However, here are some things you may want to consider:\n\nIf two or more rows are complete duplicates, then the additional rows provide no additional information. I have a hard time thinking of a scenario where dropping them would be a problem. Additionally, because they are completely identical, it doesn’t matter which row you drop.\nIf have two more rows that are partial duplicates, then you will want to look for obvious errors in the other variables. When you have two rows that are partial duplicates, and one row has very obvious errors in it, then keeping the row without the obvious errors is usually the correct decision. Having said that, you should meticulously document which rows you dropped and why, and make that information known to anyone consuming the results of your analysis.\nWhen there are no obvious errors, deciding which rows to keep and which to drop can be really tricky. In this situation the best advice I can give is to be systematic in your approach. What I mean by that is to choose a strategy that seems least likely to introduce bias into your data and then apply that strategy consistently throughout your data. So, something like always keeping the first row among a group of duplicate rows. However, keep in mind that if rows are ordered by data, this strategy could easily introduce bias. In that case, some other strategy may be more appropriate. And again, you should meticulously document which rows you dropped and why, and make that information known to anyone consuming the results of your analysis.\nFinally, I can definitively tell you a strategy that you should never use. That is, you should never pick and choose, or even give the appearance of picking and choosing, rows with values that are aligned with the results you want to see. I hope the unethical nature of this strategy is blatantly obvious to you.\n\nCongratulations! 🎉 At this point, you are well-versed in all of the dplyr verbs. More importantly, you now have a foundation of tools you can call upon to complete the many of basic data management tasks that you will encounter. In the rest of the data management part of the book we will build on these tools, and learn some new tools, we can use to solve more complex data management problems.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Subsetting Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html",
    "href": "chapters/working_with_dates/working_with_dates.html",
    "title": "28  Working with Dates",
    "section": "",
    "text": "28.1 Date vector types\nIn epidemiology, it isn’t uncommon at all for the data we are analyzing to include important date values. Some common examples include date of birth, hospital admission date, date of symptom onset, and follow-up dates in longitudinal studies. In this chapter, we will learn about two new vector types that we can use to work with date and date-time data. Additionally, we will learn about a new package, lubridate, which provides a robust set of functions designed specifically for working with date and date-time data in R.\nIn R, there are two different vector types that we can use to store, and work with, dates. They are:\n📅 date vectors for working with date values. By default, R will display dates in this format: 4-digit year, a dash, 2-digit month, a dash, and 2-digit day. For example, the date that the University of Florida won its last national football championship, January 8, 2009, looks like this as a date in R: 2009-01-08. It’s about time for another championship!\n📅🕓 POSIXct vectors for working with date-time values. Date-time values are just dates with time values added to them. By default, R will display date-times in this format: 4-digit year, a dash, 2-digit month, a dash, 2-digit day, a space, 2-digit hour value, a colon, 2-digit minute value, a colon, and 2-digit second value. So, let’s say that kickoff for the previously mentioned national championship game was at 8:00 PM local time. In R, that looks like this: 2009-01-08 20:00:00.\nIn general, we try to work with date values, rather than date-time values, whenever possible. Working with date-time values is slightly more complicated than working with date values, and we rarely have time data anyway. However, that doesn’t stop some R functions from trying to store dates as POSIXct vectors by default, which can sometimes cause unexpected errors in our R code. But, don’t worry. We are going to show you how to coerce POSIXct vectors to date vectors below.\nBefore we go any further, let’s go ahead and look at some data that we can use to help us learn to work with dates in R.\nYou can click here to download the data and import it into your R session, if you want to follow along.\nRows: 10 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): name_first, name_last, dob_typical, dob_long\ndttm (1): dob_actual\ndate (1): dob_default\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# A tibble: 10 × 6\n   name_first name_last dob_actual          dob_default dob_typical dob_long    \n   &lt;chr&gt;      &lt;chr&gt;     &lt;dttm&gt;              &lt;date&gt;      &lt;chr&gt;       &lt;chr&gt;       \n 1 Nathaniel  Watts     1996-03-04 16:59:18 1996-03-04  03/04/1996  March 04, 1…\n 2 Sophia     Gomez     1998-11-21 21:52:08 1998-11-21  11/21/1998  November 21…\n 3 Emmett     Steele    1994-09-03 23:26:19 1994-09-03  09/03/1994  September 0…\n 4 Levi       Sanchez   1996-08-03 17:18:50 1996-08-03  08/03/1996  August 03, …\n 5 August     Murray    1980-06-13 18:27:13 1980-06-13  06/13/1980  June 13, 19…\n 6 Juan       Clark     1996-12-09 05:33:24 1996-12-08  12/08/1996  December 08…\n 7 Lilly      Levy      1992-11-27 17:36:43 1992-11-27  11/27/1992  November 27…\n 8 Natalie    Rogers    1983-04-27 23:31:56 1983-04-27  04/27/1983  April 27, 1…\n 9 Solomon    Harding   1988-06-28 16:13:46 1988-06-28  06/28/1988  June 28, 19…\n10 Olivia     House     1997-08-02 22:09:50 1997-08-02  08/02/1997  August 02, …\n👆Here’s what we did above:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#date-vector-types",
    "href": "chapters/working_with_dates/working_with_dates.html#date-vector-types",
    "title": "28  Working with Dates",
    "section": "",
    "text": "Note\n\n\n\nYou were probably pretty confused when you saw the 20:00:00 above if you’ve never used 24-hour clock time (also called military time) before. We’ll let you read the details on Wikipedia, but here’s a couple of simple tips to get you started working with 24-hour time. Any time before noon is written the same as you would write it if you were using 12-hour (AM/PM) time. So, 8:00 AM would be 8:00 in 24-hour time. After noon, just add 12 to whatever time you want to write. So, 1:00 PM is 13:00 (1 + 12 = 13) and 8:00 PM is 20:00 (8 + 12 = 20).\n\n\n\n\n\n\n\n\nNote\n\n\n\nBase R does not have a built-in vector type for working with pure time (as opposed to date-time) values. If you need to work with pure time values only, then the hms package is what you want to try first.\n\n\n\n\n\n\n\n\n\nwe used the read_csv() function to import a csv file containing simulated data into R.\nThe simulated data contains the first name, last name, and date of birth for 10 fictitious people.\nIn this data, date of birth is recorded in the four most common formats that we typically come across.\n\n\ndob_actual is each person’s actual date of birth measured down to the second. Notice that this column’s type is &lt;S3: POSIXct&gt;. Again, that means that this vector contains date-time values. Also, notice that the format of these values matches the format we discussed for date-time vectors above: 4-digit year, a dash, 2-digit month, a dash, 2-digit day, a space, 2-digit hour value, a colon, 2-digit minute value, a colon, and 2-digit second value.\ndob_default is each person’s date of birth without their time of birth included. Notice that this column’s type is &lt;date&gt;. Also, notice that the format of these values matches the format we discussed for date vectors above: 4-digit year, a dash, 2-digit month, a dash, and 2-digit day.\ndob_typical is each person’s date of birth written in the format that is probably most often used in the United States: 2-digit month, a forward slash, 2-digit day, a forward slash, and 4-digit year.\ndob_long is each person’s date of birth written out in a sometimes-used long format. That is, the month name written out, 2-digit day, a comma, and 4-digit year.\n\n\nNotice that readr did a good job of importing dob_actual and dob_default as date-time and date values respectively. It did so because the values were stored in the csv file in the default format that R expects to see date-time and date values have.\nNotice that readr imported dob_typical and dob_long as character strings. It does so because the values in these columns were not stored in a format that R recognizes as a date or date-time.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#dates-under-the-hood",
    "href": "chapters/working_with_dates/working_with_dates.html#dates-under-the-hood",
    "title": "28  Working with Dates",
    "section": "28.2 Dates under the hood",
    "text": "28.2 Dates under the hood\nUnder the hood, R actually stores dates as numbers. Specifically, the number of days before or after January 1st, 1970, 00:00:00 UTC.\n\n\n\n\n\n\nNote\n\n\n\nWhy January 1st, 1970, 00:00:00 UTC? Well, it’s not really important to know the answer for the purposes of this book, or for programming in R, but Kristina Hill (a former student) figured out the answer for those of you who are curious. New Year’s Day in 1970 was an easy date for early Unix developers to use as a uniform date for the start of time. So, January 1st, 1970 at 00:00:00 UTC is referred to as the “Unix epoch”, and it’s a popular epoch used by many (but not all) software platforms. The use of any epoch date is mostly arbitrary, and this one leads to some interesting situations (like the Year 2038 Problem and this little issue that Apple had a few years ago (yikes!). Generally speaking, though, this is in no way likely to impact your day-to-day programming in R, or your life at all (unless you happen to also be a software developer in a platform that uses this epoch date).\n\n\n\n\n\n\n\n\n\n\n\nFor example, let’s use base R’s as.Date() function to create a date value from the string “2000-01-01”.\n\nas.Date(\"2000-01-01\")\n\n[1] \"2000-01-01\"\n\n\nOn the surface, it doesn’t look like anything happened. However, we can use base R’s unclass() function to see R’s internal integer representation of the date.\n\nunclass(as.Date(\"2000-01-01\"))\n\n[1] 10957\n\n\nSpecifically, January 1st, 2000 is apparently 10,957 days after January 1st, 1970. What number would you expect to be returned if we used the date “1970-01-01”?\n\nunclass(as.Date(\"1970-01-01\"))\n\n[1] 0\n\n\nWhat number would you expect to be returned if we used the date “1970-01-02”?\n\nunclass(as.Date(\"1970-01-02\"))\n\n[1] 1\n\n\nAnd finally, what number would you expect to be returned if we used the date “1969-12-31”?\n\nunclass(as.Date(\"1969-12-31\"))\n\n[1] -1\n\n\nThis numeric representation of dates also works in the other direction. For example, we can pass the number 10,958 to the as.Date() function, along with the date origin, and R will return a human-readable date.\n\nas.Date(10958, origin = \"1970-01-01\")\n\n[1] \"2000-01-02\"\n\n\nYou may be wondering why we had to tell R the date origin. After all, didn’t we already say that the origin is January 1st, 1970? Well, not all programs and programming languages use the same date origin. For example, SAS uses the date January 1st, 1960 as its origin. In our experience, this differing origin value can occasionally give us incorrect dates. When that happens, one option is to strip the date value down to its numeric representation, and then tell R what the origin was for that numeric representation in the program you are importing the data from.\nFor example, if we imported a data set from SAS, we could correctly produce human-readable dates in the manner shown below:\n\nfrom_sas &lt;- tibble(\n  date = c(10958, 10959, 10960)\n)\n\n\nfrom_sas %&gt;% \n  mutate(new_date = as.Date(date, origin = \"1960-01-01\"))\n\n# A tibble: 3 × 2\n   date new_date  \n  &lt;dbl&gt; &lt;date&gt;    \n1 10958 1990-01-01\n2 10959 1990-01-02\n3 10960 1990-01-03\n\n\nHopefully, you now have a good intuition about how R stores dates under the hood. This numeric representation of dates is what will allow us to perform calculations with dates later in the chapter.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#coercing-date-times-to-dates",
    "href": "chapters/working_with_dates/working_with_dates.html#coercing-date-times-to-dates",
    "title": "28  Working with Dates",
    "section": "28.3 Coercing date-times to dates",
    "text": "28.3 Coercing date-times to dates\nAs we said above, it’s usually preferable to work with date values instead of date-time values. Fortunately, converting date-time values to dates is usually really easy. All we need to do is pass those values to the same as.Date() function we already saw above. For example:\n\nbirth_dates %&gt;% \n  mutate(posix_to_date = as.Date(dob_actual)) %&gt;% \n  select(dob_actual, posix_to_date)\n\n# A tibble: 10 × 2\n   dob_actual          posix_to_date\n   &lt;dttm&gt;              &lt;date&gt;       \n 1 1996-03-04 16:59:18 1996-03-04   \n 2 1998-11-21 21:52:08 1998-11-21   \n 3 1994-09-03 23:26:19 1994-09-03   \n 4 1996-08-03 17:18:50 1996-08-03   \n 5 1980-06-13 18:27:13 1980-06-13   \n 6 1996-12-09 05:33:24 1996-12-09   \n 7 1992-11-27 17:36:43 1992-11-27   \n 8 1983-04-27 23:31:56 1983-04-27   \n 9 1988-06-28 16:13:46 1988-06-28   \n10 1997-08-02 22:09:50 1997-08-02   \n\n\n👆Here’s what we did above:\n\nwe created a new column in the birth_dates data frame called posix_to_date.\nwe used the as.Date() function to coerce the date-time values in dob_actual to dates. In other words, we dropped the time part of the date-time. Make sure to capitalize the “D” in as.Date().\nwe used the select() function to keep only the columns we are interested in comparing side-by-side in our output.\nNotice that dob_actual’s column type is still &lt;S3: POSIXct&gt;, but posix_to_date’s column type is &lt;date&gt;.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#coercing-character-strings-to-dates",
    "href": "chapters/working_with_dates/working_with_dates.html#coercing-character-strings-to-dates",
    "title": "28  Working with Dates",
    "section": "28.4 Coercing character strings to dates",
    "text": "28.4 Coercing character strings to dates\nConverting character strings to dates can be slightly more complicated than converting date-times to dates. This is because we have to explicitly tell R which characters in the character string correspond to each date component. For example, let’s say we have a date value of 04-05-06. Is that April 5th, 2006? Is it April 5th, 1906? Or perhaps it’s May 6th, 2004?\nwe need to use a series of special symbols to tell R which characters in the character string correspond to each date component. We’ll list some of the most common ones first and then show you how to use them. The examples below assume that date each symbol is being applied to is 2000-01-15.\n\ntribble(\n  ~Symbol, ~Description, ~Example,\n  \"%a\", \"Abbreviated weekday name\", \"Sat\",\n  \"%A\", \"Full weekday name\", \"Saturday\",\n  \"%b\", \"Abbreviated month name\", \"Jan\",\n  \"%B\", \"Full month name\", \"January\",\n  \"%d\", \"Day of the month as a number (01–31)\", \"15\",\n  \"%m\", \"Month as a number\", \"01\",\n  \"%u\", \"Weekday as a number (1–7, Monday is 1)\", \"6\",\n  \"%U\", \"Week of the year as a number (00–53) using Sunday as the first day 1 of the week\", \"02\",\n  \"%y\", \"Year without century (00-99)\", \"00\",\n  \"%Y\", \"Year with century\", \"2000\"\n) %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\nSymbol\nDescription\nExample\n\n\n\n\n%a\nAbbreviated weekday name\nSat\n\n\n%A\nFull weekday name\nSaturday\n\n\n%b\nAbbreviated month name\nJan\n\n\n%B\nFull month name\nJanuary\n\n\n%d\nDay of the month as a number (01–31)\n15\n\n\n%m\nMonth as a number\n01\n\n\n%u\nWeekday as a number (1–7, Monday is 1)\n6\n\n\n%U\nWeek of the year as a number (00–53) using Sunday as the first day 1 of the week\n02\n\n\n%y\nYear without century (00-99)\n00\n\n\n%Y\nYear with century\n2000\n\n\n\n\n\nNow that we have a list of useful symbols that we can use to communicate with R, let’s take another look at our birth date data.\n\nbirth_dates\n\n# A tibble: 10 × 6\n   name_first name_last dob_actual          dob_default dob_typical dob_long    \n   &lt;chr&gt;      &lt;chr&gt;     &lt;dttm&gt;              &lt;date&gt;      &lt;chr&gt;       &lt;chr&gt;       \n 1 Nathaniel  Watts     1996-03-04 16:59:18 1996-03-04  03/04/1996  March 04, 1…\n 2 Sophia     Gomez     1998-11-21 21:52:08 1998-11-21  11/21/1998  November 21…\n 3 Emmett     Steele    1994-09-03 23:26:19 1994-09-03  09/03/1994  September 0…\n 4 Levi       Sanchez   1996-08-03 17:18:50 1996-08-03  08/03/1996  August 03, …\n 5 August     Murray    1980-06-13 18:27:13 1980-06-13  06/13/1980  June 13, 19…\n 6 Juan       Clark     1996-12-09 05:33:24 1996-12-08  12/08/1996  December 08…\n 7 Lilly      Levy      1992-11-27 17:36:43 1992-11-27  11/27/1992  November 27…\n 8 Natalie    Rogers    1983-04-27 23:31:56 1983-04-27  04/27/1983  April 27, 1…\n 9 Solomon    Harding   1988-06-28 16:13:46 1988-06-28  06/28/1988  June 28, 19…\n10 Olivia     House     1997-08-02 22:09:50 1997-08-02  08/02/1997  August 02, …\n\n\nFor our first example, let’s try converting the character strings stored in the dob_typical to date values. Let’ start by passing the values to as.Date() exactly as we did above and see what happens:\n\nbirth_dates %&gt;% \n  mutate(dob_typical_to_date = as.Date(dob_typical)) %&gt;% \n  select(dob_typical, dob_typical_to_date)\n\n# A tibble: 10 × 2\n   dob_typical dob_typical_to_date\n   &lt;chr&gt;       &lt;date&gt;             \n 1 03/04/1996  0003-04-19         \n 2 11/21/1998  NA                 \n 3 09/03/1994  0009-03-19         \n 4 08/03/1996  0008-03-19         \n 5 06/13/1980  NA                 \n 6 12/08/1996  0012-08-19         \n 7 11/27/1992  NA                 \n 8 04/27/1983  NA                 \n 9 06/28/1988  NA                 \n10 08/02/1997  0008-02-19         \n\n\nThis is definitely not the result we wanted, right? Why didn’t it work? Well, R was looking for the values in dob_typical to have the format 4-digit year, a dash, 2-digit month, a dash, and 2-digit day. In reality, dob_typical has the format 2-digit month, a forward slash, 2-digit day, a forward slash, and 4-digit year. Now, all we have to do is tell R how to read this character string as a date using some of the symbols we learned about in the table above.\nLet’s try again:\n\nbirth_dates %&gt;% \n  mutate(dob_typical_to_date = as.Date(dob_typical, format = \"%m %d %Y\")) %&gt;% \n  select(dob_typical, dob_typical_to_date)\n\n# A tibble: 10 × 2\n   dob_typical dob_typical_to_date\n   &lt;chr&gt;       &lt;date&gt;             \n 1 03/04/1996  NA                 \n 2 11/21/1998  NA                 \n 3 09/03/1994  NA                 \n 4 08/03/1996  NA                 \n 5 06/13/1980  NA                 \n 6 12/08/1996  NA                 \n 7 11/27/1992  NA                 \n 8 04/27/1983  NA                 \n 9 06/28/1988  NA                 \n10 08/02/1997  NA                 \n\n\nWait, what? We told R that the values were 2-digit month (%m), 2-digit day (%d), and 4-digit year (%Y). Why didn’t it work this time? It didn’t work because we didn’t pass the forward slashes to the format argument. Yes, it’s that literal. We even have to tell R that there are symbols mixed in with our date values in the character string we want to convert to a date.\nLet’s try one more time:\n\nbirth_dates %&gt;% \n  mutate(dob_typical_to_date = as.Date(dob_typical, format = \"%m/%d/%Y\")) %&gt;% \n  select(dob_typical, dob_typical_to_date)\n\n# A tibble: 10 × 2\n   dob_typical dob_typical_to_date\n   &lt;chr&gt;       &lt;date&gt;             \n 1 03/04/1996  1996-03-04         \n 2 11/21/1998  1998-11-21         \n 3 09/03/1994  1994-09-03         \n 4 08/03/1996  1996-08-03         \n 5 06/13/1980  1980-06-13         \n 6 12/08/1996  1996-12-08         \n 7 11/27/1992  1992-11-27         \n 8 04/27/1983  1983-04-27         \n 9 06/28/1988  1988-06-28         \n10 08/02/1997  1997-08-02         \n\n\n👆Here’s what we did above:\n\nwe created a new column in the birth_dates data frame called dob_typical_to_date.\nwe used the as.Date() function to coerce the character string values in dob_typical to dates.\nwe did so by passing the value \"%m/%d/%Y\" to the format argument of the as.Date() function. These symbols tell R to read the character strings in dob_typical as 2-digit month (%m), a forward slash (/), 2-digit day (%d), a forward slash (/), and 4-digit year (%Y).\nwe used the select() function to keep only the columns we are interested in comparing side-by-side in our output.\nNotice that dob_typical’s column type is still character (&lt;chr&gt;), but dob_typical_to_date’s column type is &lt;date&gt;.\n\nLet’s try one more example, just to make sure we’ve got this down. Take a look at the dob_long column. What value will we need to pass to as.Date()’s format argument in order to convert these character strings to dates?\n\nselect(birth_dates, dob_long)\n\n# A tibble: 10 × 1\n   dob_long          \n   &lt;chr&gt;             \n 1 March 04, 1996    \n 2 November 21, 1998 \n 3 September 03, 1994\n 4 August 03, 1996   \n 5 June 13, 1980     \n 6 December 08, 1996 \n 7 November 27, 1992 \n 8 April 27, 1983    \n 9 June 28, 1988     \n10 August 02, 1997   \n\n\nDid you figure it out? The solution is below:\n\nbirth_dates %&gt;% \n  mutate(dob_long_to_date = as.Date(dob_long, format = \"%B %d, %Y\")) %&gt;% \n  select(dob_long, dob_long_to_date)\n\n# A tibble: 10 × 2\n   dob_long           dob_long_to_date\n   &lt;chr&gt;              &lt;date&gt;          \n 1 March 04, 1996     1996-03-04      \n 2 November 21, 1998  1998-11-21      \n 3 September 03, 1994 1994-09-03      \n 4 August 03, 1996    1996-08-03      \n 5 June 13, 1980      1980-06-13      \n 6 December 08, 1996  1996-12-08      \n 7 November 27, 1992  1992-11-27      \n 8 April 27, 1983     1983-04-27      \n 9 June 28, 1988      1988-06-28      \n10 August 02, 1997    1997-08-02      \n\n\n👆Here’s what we did above:\n\nwe created a new column in the birth_dates data frame called dob_long_to_date.\nwe used the as.Date() function to coerce the character string values in dob_long to dates.\nwe did so by passing the value \"%B %d, %Y\" to the format argument of the as.Date() function. These symbols tell R to read the character strings in dob_long as full month name (%B), 2-digit day (%d), a comma (,), and 4-digit year (%Y).\nwe used the select() function to keep only the columns we are interested in comparing side-by-side in our output.\nNotice that dob_long’s column type is still character (&lt;chr&gt;), but dob_long_to_date’s column type is &lt;date&gt;.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#change-the-appearance-of-dates-with-format",
    "href": "chapters/working_with_dates/working_with_dates.html#change-the-appearance-of-dates-with-format",
    "title": "28  Working with Dates",
    "section": "28.5 Change the appearance of dates with format()",
    "text": "28.5 Change the appearance of dates with format()\nSo, far we’ve talked about transforming character strings into dates. However, the reverse is also possible. Meaning, we can transform date values into character strings that we can style (i.e., format) in just about any way you could possibly want to style a date. For example:\n\nbirth_dates %&gt;% \n  mutate(dob_abbreviated = format(dob_actual, \"%d %b %y\")) %&gt;% \n  select(dob_actual, dob_abbreviated)\n\n# A tibble: 10 × 2\n   dob_actual          dob_abbreviated\n   &lt;dttm&gt;              &lt;chr&gt;          \n 1 1996-03-04 16:59:18 04 Mar 96      \n 2 1998-11-21 21:52:08 21 Nov 98      \n 3 1994-09-03 23:26:19 03 Sep 94      \n 4 1996-08-03 17:18:50 03 Aug 96      \n 5 1980-06-13 18:27:13 13 Jun 80      \n 6 1996-12-09 05:33:24 09 Dec 96      \n 7 1992-11-27 17:36:43 27 Nov 92      \n 8 1983-04-27 23:31:56 27 Apr 83      \n 9 1988-06-28 16:13:46 28 Jun 88      \n10 1997-08-02 22:09:50 02 Aug 97      \n\n\n👆Here’s what we did above:\n\nwe created a new column in the birth_dates data frame called dob_abbreviated.\nwe used the format() function to coerce the date values in dob_actual to character string values in dob_abbreviated.\nwe did so by passing the value \"%d %b %y\" to the ... argument of the format() function. These symbols tell R to create a character string as 2-digit day (%d), a space (\" \"), abbreviated month name (%b), a space (\" \"), and 2-digit year (%y).\nwe used the select() function to keep only the columns we are interested in comparing side-by-side in our output.\nNotice that dob_actual’s column type is still date_time (&lt;S3: POSIXct&gt;), but dob_abbreviated’s column type is character (&lt;chr&gt;). So, while dob_abbreviated looks like a date to us, it is no longer a date value to R. In other words, dob_abbreviated doesn’t have an integer representation under the hood. It is simply a character string.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#some-useful-built-in-dates",
    "href": "chapters/working_with_dates/working_with_dates.html#some-useful-built-in-dates",
    "title": "28  Working with Dates",
    "section": "28.6 Some useful built-in dates",
    "text": "28.6 Some useful built-in dates\nBase R actually includes a few useful built-in dates that we can use. They can often be useful when doing calculations with dates. Here are a few examples:\n\n28.6.1 Today’s date\n\nSys.Date()\n\n[1] \"2025-06-12\"\n\n\n\nlubridate::today()\n\n[1] \"2025-06-12\"\n\n\nThese functions can be useful for calculating any length of time up to today. For example, your age today is just the length of time that spans between your birth date and today.\n\n\n28.6.2 Today’s date-time\n\nSys.time()\n\n[1] \"2025-06-12 20:14:23 CDT\"\n\n\n\nlubridate::now()\n\n[1] \"2025-06-12 20:14:23 CDT\"\n\n\nBecause these functions also return the current time, they can be useful for timing how long it takes your R code to run. As we’ve said many times, there is typically multiple ways to accomplish a given task in R. Sometimes, the difference between any to ways to accomplish the task is basically just a matter of preference. However, sometimes one way can be much faster than another way. All the examples we’ve seen so far in this book take a trivial amount of time to run – usually less than a second. However, we have written R programs that took several minutes to several hours to complete. For example, complex data simulations and multiple imputation procedures can both take a long time to run. In such cases, we will sometimes check to see if there any significant performance differences between two different approaches to accomplishing the coding task.\nAs a silly example to show you how this works, let’s generate 1,000,000 random numbers.\n\nset.seed(703)\nrand_mill &lt;- rnorm(1000000)\n\nNow, let’s find the mean value of those numbers two different ways, and check to see if there is any time difference between the two:\n\n# Save the start time\nstart  &lt;- lubridate::now()\nsum    &lt;- sum(rand_mill)\nlength &lt;- length(rand_mill)\nmean   &lt;- sum / length\nmean\n\n[1] 0.0009259691\n\n# Save the stop time\nstop   &lt;- lubridate::now()\n\n\nstop - start\n\nTime difference of 0.002330065 secs\n\n\n\nrm(mean)\n\nSo, finding the mean this way took less than a second. Let’s see how long using the mean() function takes:\n\n# Save the start time\nstart  &lt;- lubridate::now()\nmean(rand_mill)\n\n[1] 0.0009259691\n\n# Save the stop time\nstop   &lt;- lubridate::now()\n\n\nstop - start\n\nTime difference of 0.001876831 secs\n\n\nAlthough both methods above took less than a second to complete the calculations we were interested in, the second method (i.e., using the mean() function) took only about a third as as much time as the first. Again, it obviously doesn’t matter in this scenario, but doing these kinds of checks can be useful when calculations take much longer. For example, that time savings we saw above would be pretty important if we were comparing two methods to accomplish a task where the longer method took an hour to complete and the shorter method took a third as much time (About 20 minutes).\n\n\n28.6.3 Character vector of full month names\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\n\n28.6.4 Character vector of abbreviated month names\n\nmonth.abb\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\n\nmonth.name and month.abb aren’t functions. They don’t do anything. Rather, they are just saved values that can save us some typing if you happen to be working with data that requires you create variables, or perform calculations, by month.\n\n\n28.6.5 Creating a vector containing a sequence of dates\nIn the same way that we can simulate a sequence of numbers using the seq() function, we can simulate a sequence of dates using the seq.Date() function. We sometimes find this function useful for simulating data (including some of the data used in this book), and for filling in missing dates in longitudinal data. For example, we can use the seq.Date() function to return a vector of dates that includes all days between January 1st, 2020 and January 15th, 2020 like this:\n\nseq.Date(\n  from = as.Date(\"2020-01-01\"),\n  to   = as.Date(\"2020-01-15\"),\n  by   = \"days\"\n)\n\n [1] \"2020-01-01\" \"2020-01-02\" \"2020-01-03\" \"2020-01-04\" \"2020-01-05\"\n [6] \"2020-01-06\" \"2020-01-07\" \"2020-01-08\" \"2020-01-09\" \"2020-01-10\"\n[11] \"2020-01-11\" \"2020-01-12\" \"2020-01-13\" \"2020-01-14\" \"2020-01-15\"",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#calculating-date-intervals",
    "href": "chapters/working_with_dates/working_with_dates.html#calculating-date-intervals",
    "title": "28  Working with Dates",
    "section": "28.7 Calculating date intervals",
    "text": "28.7 Calculating date intervals\nSo far, we’ve learned how to create and format dates in R. However, the real value in being able to coerce character strings to date values is that doing so allows us to perform calculations with the dates that we could not perform with the character strings. In our experience, calculating intervals of time between dates is probably the most common type of calculation we will want to perform.\nBefore we get into some examples, we are going to drop some of the columns from our birth_dates data frame because we won’t need them anymore.\n\nages &lt;- birth_dates %&gt;% \n  select(name_first, dob = dob_default) %&gt;% \n  print()\n\n# A tibble: 10 × 2\n   name_first dob       \n   &lt;chr&gt;      &lt;date&gt;    \n 1 Nathaniel  1996-03-04\n 2 Sophia     1998-11-21\n 3 Emmett     1994-09-03\n 4 Levi       1996-08-03\n 5 August     1980-06-13\n 6 Juan       1996-12-08\n 7 Lilly      1992-11-27\n 8 Natalie    1983-04-27\n 9 Solomon    1988-06-28\n10 Olivia     1997-08-02\n\n\n👆Here’s what we did above:\n\nwe created a new data frame called ages by subsetting the birth_dates data frame.\nwe used the select() function to keep only the name_first and dob_default columns from birth_dates. We used a name-value pair (dob = dob_default) inside the select() function to rename dob_default to dob.\n\nNext, let’s create a variable in our data frame that is equal to today’s date. In reality, this would be a great time to use Sys.Date() to ask R to return today’s date.\n\nages %&gt;% \n  mutate(today = Sys.Date())\n\nHowever, we are not going to do that here, because it would cause the value of the today variable to update every time we update the book. That would make it challenging to write about the results we get. So, we’re going to pretend that today is May 7th, 2020. We’ll add that to our data frame like so:\n\nages &lt;- ages %&gt;% \n  mutate(today = as.Date(\"2020-05-07\")) %&gt;% \n  print()\n\n# A tibble: 10 × 3\n   name_first dob        today     \n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;    \n 1 Nathaniel  1996-03-04 2020-05-07\n 2 Sophia     1998-11-21 2020-05-07\n 3 Emmett     1994-09-03 2020-05-07\n 4 Levi       1996-08-03 2020-05-07\n 5 August     1980-06-13 2020-05-07\n 6 Juan       1996-12-08 2020-05-07\n 7 Lilly      1992-11-27 2020-05-07\n 8 Natalie    1983-04-27 2020-05-07\n 9 Solomon    1988-06-28 2020-05-07\n10 Olivia     1997-08-02 2020-05-07\n\n\n👆Here’s what we did above:\n\nwe created a new column in the ages data frame called today.\nwe made set the value of the today column to May 7th, 2020 by passing the value \"2020-05-07\" to the as.Date() function.\n\n\n28.7.1 Calculate age as the difference in time between dob and today\nCalculating age from date of birth is a pretty common data management task. While you know what ages are, you probably don’t think much about their calculation. Age is just the difference between two points in time. The starting point is always the date of birth. However, because age is constantly changing the end point changes as well. For example, you’re one day older today than you were yesterday. So, to calculate age, we must always have a start date (i.e., date of birth) and an end date. In the example below, our end date will be May 7th, 2020.\nOnce we have those two pieces of information, we can ask R to calculate age for us in a few different ways. We are going to suggest that you use the method below that uses functions from the lubridate package. We will show you why soon. However, we want to show you the base R way of calculating time intervals for comparison, and because a lot of the help documentation we’ve seen online uses the base R methods shown below.\nLet’s go ahead and load the lubridate package now.\n\nlibrary(lubridate)\n\nNext, let’s go ahead and calculate age 3 different ways:\n\nages %&gt;% \n  mutate(\n    age_subtraction = today - dob,\n    age_difftime    = difftime(today, dob),\n    age_lubridate   = dob %--% today # lubridate's %--% operator creates a time interval\n  )\n\n# A tibble: 10 × 6\n   name_first dob        today      age_subtraction age_difftime\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;drtn&gt;          &lt;drtn&gt;      \n 1 Nathaniel  1996-03-04 2020-05-07  8830 days       8830 days  \n 2 Sophia     1998-11-21 2020-05-07  7838 days       7838 days  \n 3 Emmett     1994-09-03 2020-05-07  9378 days       9378 days  \n 4 Levi       1996-08-03 2020-05-07  8678 days       8678 days  \n 5 August     1980-06-13 2020-05-07 14573 days      14573 days  \n 6 Juan       1996-12-08 2020-05-07  8551 days       8551 days  \n 7 Lilly      1992-11-27 2020-05-07 10023 days      10023 days  \n 8 Natalie    1983-04-27 2020-05-07 13525 days      13525 days  \n 9 Solomon    1988-06-28 2020-05-07 11636 days      11636 days  \n10 Olivia     1997-08-02 2020-05-07  8314 days       8314 days  \n# ℹ 1 more variable: age_lubridate &lt;Interval&gt;\n\n\n👆Here’s what we did above:\n\nwe created three new columns in the ages data frame called age_subtraction, age_difftime, and age_lubridate.\n\nwe created age_subtraction using the subtraction operator (-). Remember, R stores dates values as numbers under the hood. So, we literally just asked R to subtract the value for dob from the value for today. The value returned to us was a vector of time differences measured in days.\nwe created age_difftime base R’s difftime() function. The value returned to us was a vector of time differences measured in days. As you can see, the results returned by today - dob and difftime(today, dob) are identical.\nwe created age_lubridate using lubridate’s time interval operator (%--%). Notice that the order of dob and today are switched here compared to the previous two methods. By itself, the %--% operator doesn’t return a time difference value. It returns a time interval value.\n\n\nHere is how we can convert the time difference and time interval values to age in years:\n\nages %&gt;% \n  mutate(\n    age_subtraction = as.numeric(today - dob) / 365.25,\n    age_difftime    = as.numeric(difftime(today, dob)) / 365.25,\n    age_lubridate   = (dob %--% today) / years(1)\n  )\n\n# A tibble: 10 × 6\n   name_first dob        today      age_subtraction age_difftime age_lubridate\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;               &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 Nathaniel  1996-03-04 2020-05-07            24.2         24.2          24.2\n 2 Sophia     1998-11-21 2020-05-07            21.5         21.5          21.5\n 3 Emmett     1994-09-03 2020-05-07            25.7         25.7          25.7\n 4 Levi       1996-08-03 2020-05-07            23.8         23.8          23.8\n 5 August     1980-06-13 2020-05-07            39.9         39.9          39.9\n 6 Juan       1996-12-08 2020-05-07            23.4         23.4          23.4\n 7 Lilly      1992-11-27 2020-05-07            27.4         27.4          27.4\n 8 Natalie    1983-04-27 2020-05-07            37.0         37.0          37.0\n 9 Solomon    1988-06-28 2020-05-07            31.9         31.9          31.9\n10 Olivia     1997-08-02 2020-05-07            22.8         22.8          22.8\n\n\n👆Here’s what we did above:\n\nwe created three new columns in the ages data frame called age_subtraction, age_difftime, and age_lubridate.\n\nwe used the as.numeric() function to convert the values of age_subtraction from a time differences to a number – the number of days. We then divided the number of days by 365.25 – roughly the number of days in a year. The result is age in years.\nwe used the as.numeric() function to convert the values of age_difftime from a time differences to a number – the number of days. We then divided the number of days by 365.25 – roughly the number of days in a year. The result is age in years.\nAgain, the results of the first two methods are identical.\nwe asked R to show us the time interval values we created age_lubridate using lubridate’s time interval operator (%--%) as years of time. We did so by dividing the time interval into years. Specifically, we used the division operator (/) and lubridate’s years() function. The value we passed to the years() function was 1. In other words, we asked R to tell us how many 1-year periods are in each time interval we created with dob %--% today.\nIn case you’re wondering, here’s the value returned by the years() function alone:\n\n\n\nyears(1)\n\n[1] \"1y 0m 0d 0H 0M 0S\"\n\n\nSo, why did the results of the first two methods differ from the results of the third method? Well, dates are much more complicated to work with than they may seem on the surface. Specifically, each day doesn’t have exactly 24 hours and each year doesn’t have exactly 365 days. Some have more and some have less – so called, leap years. You can find more details on the lubridate website, but the short answer is that lubridate’s method gives us a more precise answer than the first two methods do because it accounts for date complexities in a different way.\nHere’s an example to quickly illustrate what we mean:\nSay we want to calculate the number of years between “2017-03-01” and “2018-03-01”.\n\nstart &lt;- as.Date(\"2017-03-01\")\nend   &lt;- as.Date(\"2018-03-01\")\n\nThe most meaningful result in this situation is obviously 1 year.\n\n# The base R way\nas.numeric(difftime(end, start)) / 365.25\n\n[1] 0.9993155\n\n\n\n# The lubridate way\n(start %--% end) / years(1)\n\n[1] 1\n\n\nNotice that lubridate’s method returns exactly one year, but the base R method returns an approximation of a year.\nTo further illustrate this point, let’s look at what happens when the time interval includes a leap year. The year 2020 is a leap year, so let’s calculate the number of years between “2019-03-01” and “2020-03-01”. Again, a meaningful result here should be a year.\n\nstart &lt;- as.Date(\"2019-03-01\")\nend   &lt;- as.Date(\"2020-03-01\")\n\n\n# The base R way\nas.numeric(difftime(end, start)) / 36\n\n[1] 10.16667\n\n\n\n# The lubridate way\n(start %--% end) / years(1)\n\n[1] 1\n\n\nNotice that lubridate’s method returns exactly one year, but the base R method returns an approximation of a year.\nTo further illustrate this point, let’s look at what happens when the time interval includes a leap year. The year 2020 is a leap year, so let’s calculate the number of years between “2019-03-01” and “2020-03-01”. Again, a meaningful result here should be a year.\n\nstart &lt;- as.Date(\"2019-03-01\")\nend   &lt;- as.Date(\"2020-03-01\")\n\n\n# The base R way\nas.numeric(difftime(end, start)) / 365.25\n\n[1] 1.002053\n\n\n\n# The lubridate way\n(start %--% end) / years(1)\n\n[1] 1\n\n\nOnce again, the lubridate method returns exactly one year, while the base R method returns an approximation of a year.\n\n\n28.7.2 Rounding time intervals\nOkay, so now we know how to get age in years, and hopefully I convinced you that using functions from the lubridate package can help us do so in the most precise way possible. However, in most situations we would want to take our calculations one step further and round to whole years. There are actually a couple different ways to do so. For example:\n\nages %&gt;% \n  mutate(\n    age_years = (dob %--% today) / years(1),\n    # If you want the age (in years) as of the person's last birthday\n    age_last  = trunc(age_years),\n    # If you want to round the age to the nearest year\n    age_near  = round(age_years)\n  )\n\n# A tibble: 10 × 6\n   name_first dob        today      age_years age_last age_near\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Nathaniel  1996-03-04 2020-05-07      24.2       24       24\n 2 Sophia     1998-11-21 2020-05-07      21.5       21       21\n 3 Emmett     1994-09-03 2020-05-07      25.7       25       26\n 4 Levi       1996-08-03 2020-05-07      23.8       23       24\n 5 August     1980-06-13 2020-05-07      39.9       39       40\n 6 Juan       1996-12-08 2020-05-07      23.4       23       23\n 7 Lilly      1992-11-27 2020-05-07      27.4       27       27\n 8 Natalie    1983-04-27 2020-05-07      37.0       37       37\n 9 Solomon    1988-06-28 2020-05-07      31.9       31       32\n10 Olivia     1997-08-02 2020-05-07      22.8       22       23\n\n\n👆Here’s what we did above:\n\nWe created two new columns in the ages data frame called age_last, and age_near.\n\nWe created age_last using the trunc() (for truncate) function. The value returned by the trunc() function can be interpreted as each person’s age in years at their last birthday.\nWe created age_near using the round() function. The value returned by the round() function can be interpreted as each person’s age in years at their nearest birthday – which may not have occurred yet. This is probably not the value that you will typically be looking for. So, just make sure you choose the correct function for the type of rounding you want to do.\n\n\nAs a shortcut, we can use the integer division operator (%/%) to calculate each person’s age in years at their nearest birthday without the trunc() function.\n\nages %&gt;% \n  mutate(\n    # If you want the age (in years) as of the person's last birthday\n    age_years = (dob %--% today) %/% years(1)\n  )\n\n# A tibble: 10 × 4\n   name_first dob        today      age_years\n   &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;         &lt;dbl&gt;\n 1 Nathaniel  1996-03-04 2020-05-07        24\n 2 Sophia     1998-11-21 2020-05-07        21\n 3 Emmett     1994-09-03 2020-05-07        25\n 4 Levi       1996-08-03 2020-05-07        23\n 5 August     1980-06-13 2020-05-07        39\n 6 Juan       1996-12-08 2020-05-07        23\n 7 Lilly      1992-11-27 2020-05-07        27\n 8 Natalie    1983-04-27 2020-05-07        37\n 9 Solomon    1988-06-28 2020-05-07        31\n10 Olivia     1997-08-02 2020-05-07        22",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#extracting-out-date-parts",
    "href": "chapters/working_with_dates/working_with_dates.html#extracting-out-date-parts",
    "title": "28  Working with Dates",
    "section": "28.8 Extracting out date parts",
    "text": "28.8 Extracting out date parts\nSometimes it can be useful to store parts of a date in separate columns. For example, it is common to break date values up into their component parts when linking records across multiple data frames. We will learn how to link data frames a little later in the book. For now, we’re just going to learn how separate dates into their component parts.\nWe won’t need the today column anymore, so I’ll go ahead a drop it here.\n\nages &lt;- ages %&gt;% \n  select(-today) %&gt;% \n  print()\n\n# A tibble: 10 × 2\n   name_first dob       \n   &lt;chr&gt;      &lt;date&gt;    \n 1 Nathaniel  1996-03-04\n 2 Sophia     1998-11-21\n 3 Emmett     1994-09-03\n 4 Levi       1996-08-03\n 5 August     1980-06-13\n 6 Juan       1996-12-08\n 7 Lilly      1992-11-27\n 8 Natalie    1983-04-27\n 9 Solomon    1988-06-28\n10 Olivia     1997-08-02\n\n\nTypically, separating the date will include creating separate columns for the day, the month, and the year. Fortunately, lubridate includes intuitively named functions that make this really easy:\n\nages %&gt;% \n  mutate(\n    day   = day(dob),\n    month = month(dob),\n    year  = year(dob)\n  )\n\n# A tibble: 10 × 5\n   name_first dob          day month  year\n   &lt;chr&gt;      &lt;date&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Nathaniel  1996-03-04     4     3  1996\n 2 Sophia     1998-11-21    21    11  1998\n 3 Emmett     1994-09-03     3     9  1994\n 4 Levi       1996-08-03     3     8  1996\n 5 August     1980-06-13    13     6  1980\n 6 Juan       1996-12-08     8    12  1996\n 7 Lilly      1992-11-27    27    11  1992\n 8 Natalie    1983-04-27    27     4  1983\n 9 Solomon    1988-06-28    28     6  1988\n10 Olivia     1997-08-02     2     8  1997\n\n\n👆Here’s what we did above:\n\nWe created three new columns in the ages data frame called day, month, and year. We created them by passing the dob column to the x argument of lubridate’s day(), month(), and year() functions respectively.\n\nlubridate also includes functions for extracting other information from date values. For example:\n\nages %&gt;% \n  mutate(\n    wday         = wday(dob),\n    day_full     = wday(dob, label = TRUE, abbr = FALSE),\n    day_abb      = wday(dob, label = TRUE, abbr = TRUE),\n    week_of_year = week(dob),\n    week_cdc     = epiweek(dob)\n  )\n\n# A tibble: 10 × 7\n   name_first dob         wday day_full  day_abb week_of_year week_cdc\n   &lt;chr&gt;      &lt;date&gt;     &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 Nathaniel  1996-03-04     2 Monday    Mon               10       10\n 2 Sophia     1998-11-21     7 Saturday  Sat               47       46\n 3 Emmett     1994-09-03     7 Saturday  Sat               36       35\n 4 Levi       1996-08-03     7 Saturday  Sat               31       31\n 5 August     1980-06-13     6 Friday    Fri               24       24\n 6 Juan       1996-12-08     1 Sunday    Sun               49       50\n 7 Lilly      1992-11-27     6 Friday    Fri               48       48\n 8 Natalie    1983-04-27     4 Wednesday Wed               17       17\n 9 Solomon    1988-06-28     3 Tuesday   Tue               26       26\n10 Olivia     1997-08-02     7 Saturday  Sat               31       31\n\n\n👆Here’s what we did above:\n\nWe created five new columns in the ages data frame called wday, day_abb,day_full, week_of_year, and week_cdc. We created them by passing the dob column to the x argument of lubridate’s wday(), week(), and epiweek() functions respectively.\nThe wday() function returns the day of the week the given date falls on. By default, the wday() returns an integer value between 1 and 7. We can adjust the values passed to wday()’s label and abbr arguments to return full day names (day_full) and abbreviated day names (day_abb).\nThe week() function returns the week of the year the given date falls in. More formally, the week() function “returns the number of complete seven-day periods that have occurred between the date and January 1st, plus one.” You can see this information by typing ?week in your console.\nThe epiweek() function also returns the week of the year the given date falls in. However, it calculates the week in a slightly different way. Specifically, “it uses the US CDC version of epidemiological week. It follows same rules as isoweek() but starts on Sunday. In other parts of the world the convention is to start epidemiological weeks on Monday, which is the same as isoweek.” Again, you can see this information by typing ?week in your console.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_dates/working_with_dates.html#sorting-dates",
    "href": "chapters/working_with_dates/working_with_dates.html#sorting-dates",
    "title": "28  Working with Dates",
    "section": "28.9 Sorting dates",
    "text": "28.9 Sorting dates\nAnother really common thing we might want to do with date values is sort them chronologically. Fortunately, this is really easy to do with dplyr’s arrange() function. If we want to sort our dates in ascending order (i.e., oldest to most recent), we just pass the date column to the ... argument of the arrange() function like so:\n\n# Oldest (top) to most recent (bottom)\n# Ascending order\nages %&gt;% \n  arrange(dob)\n\n# A tibble: 10 × 2\n   name_first dob       \n   &lt;chr&gt;      &lt;date&gt;    \n 1 August     1980-06-13\n 2 Natalie    1983-04-27\n 3 Solomon    1988-06-28\n 4 Lilly      1992-11-27\n 5 Emmett     1994-09-03\n 6 Nathaniel  1996-03-04\n 7 Levi       1996-08-03\n 8 Juan       1996-12-08\n 9 Olivia     1997-08-02\n10 Sophia     1998-11-21\n\n\nIf we want to sort our dates in descending order (i.e., most recent to oldest), we just pass the date column to the desc() function before passing it to the ... argument of the arrange() function.\n\n# Most recent (top) to oldest (bottom)\n# Descending order\nages %&gt;% \n  arrange(desc(dob))\n\n# A tibble: 10 × 2\n   name_first dob       \n   &lt;chr&gt;      &lt;date&gt;    \n 1 Sophia     1998-11-21\n 2 Olivia     1997-08-02\n 3 Juan       1996-12-08\n 4 Levi       1996-08-03\n 5 Nathaniel  1996-03-04\n 6 Emmett     1994-09-03\n 7 Lilly      1992-11-27\n 8 Solomon    1988-06-28\n 9 Natalie    1983-04-27\n10 August     1980-06-13\n\n\nMuch of the data we work with in epidemiology includes dates. In fact, it isn’t uncommon for the length of time that passes between to events to be the primary outcome that we are trying to understand. Hopefully, the tools we’ve learned in this chapter will give you a solid foundation for working with dates in R. For more information on dates, including a handy cheat sheet, I recommend visiting the lubridate website.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Working with Dates</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_character_strings/working_with_character_strings.html",
    "href": "chapters/working_with_character_strings/working_with_character_strings.html",
    "title": "29  Working with Character Strings",
    "section": "",
    "text": "29.1 Coerce to lowercase\nIn previous chapters, we learned how to create character vectors, which can be useful on their own. We also learned how to coerce character vectors to factor vectors that we can use for categorical data analysis. However, up to this point, we haven’t done a lot of manipulation of the values stored inside of the character strings themselves. Sometimes, however, we will need to manipulate the character string before we can complete other data management tasks or analysis. Some common examples from my projects include separating character strings into multiple parts and creating dummy variables from character strings that can take multiple values. In this chapter, we’ll see some specific example of both, and we’ll learn a few new tools for working with character strings along the way.\nTo get started, feel free to download the simulated electronic health record that we will use in the following examples. Additionally, we will use the readr, dplyr, and stringr packages in the code below. You will be able to recognize functions from the stringr package because they will all begin with str_.\n👆Here’s what we did above:\nA common initial question we may need to ask of this kind of data is, “how many unique people are represented in this data?” Well, there are 15 rows, so a good first guess might be 15 unique people. However, let’s arrange the data by the name column and see if that guess still looks reasonable.\nClearly, some of these people are the same. However, little data entry discrepancies in their name values would prevent us from calculating the number of unique people in a programmatic way. Let’s take a closer look at the values in the name column and see if we can figure out exactly what these data entry discrepancies are:\n👆Here’s what we did above:\nA good place to start cleaning these character strings is by coercing them all to lowercase. We’ve already used base R’s tolower() function a couple of times before. So, you may have already guessed how to complete this task. However, before moving on to coercing all the names in our ehr data to lowercase, we want to show you some of the other functions that the stringr package contains for changing the case of character strings. For example:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Working with Character Strings</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_character_strings/working_with_character_strings.html#coerce-to-lowercase",
    "href": "chapters/working_with_character_strings/working_with_character_strings.html#coerce-to-lowercase",
    "title": "29  Working with Character Strings",
    "section": "",
    "text": "29.1.1 Lowercase\n\nehr %&gt;% \n  arrange(name) %&gt;% \n  pull(name) %&gt;% \n  str_to_lower()\n\n [1] \"arabella george\"  \"arabella george\"  \"charlee carroll\"  \"emma medrano\"    \n [5] \"ivy   mccann\"     \"ivy mccann\"       \"jasper decker\"    \"kane martin\"     \n [9] \"ryan edwards\"     \"ryan edwards  \"   \"tatum chavez\"     \"tatum s chavez\"  \n[13] \"weston fox\"       \"weston fox,\"      \"zariah hernandez\"\n\n\n\n\n29.1.2 Upper case\n\nehr %&gt;% \n  arrange(name) %&gt;% \n  pull(name) %&gt;% \n  str_to_upper()\n\n [1] \"ARABELLA GEORGE\"  \"ARABELLA GEORGE\"  \"CHARLEE CARROLL\"  \"EMMA MEDRANO\"    \n [5] \"IVY   MCCANN\"     \"IVY MCCANN\"       \"JASPER DECKER\"    \"KANE MARTIN\"     \n [9] \"RYAN EDWARDS\"     \"RYAN EDWARDS  \"   \"TATUM CHAVEZ\"     \"TATUM S CHAVEZ\"  \n[13] \"WESTON FOX\"       \"WESTON FOX,\"      \"ZARIAH HERNANDEZ\"\n\n\n\n\n29.1.3 Title case\n\nehr %&gt;% \n  arrange(name) %&gt;% \n  pull(name) %&gt;% \n  str_to_title()\n\n [1] \"Arabella George\"  \"Arabella George\"  \"Charlee Carroll\"  \"Emma Medrano\"    \n [5] \"Ivy   Mccann\"     \"Ivy Mccann\"       \"Jasper Decker\"    \"Kane Martin\"     \n [9] \"Ryan Edwards\"     \"Ryan Edwards  \"   \"Tatum Chavez\"     \"Tatum S Chavez\"  \n[13] \"Weston Fox\"       \"Weston Fox,\"      \"Zariah Hernandez\"\n\n\n\n\n29.1.4 Sentence case\n\nehr %&gt;% \n  arrange(name) %&gt;% \n  pull(name) %&gt;% \n  str_to_sentence()\n\n [1] \"Arabella george\"  \"Arabella george\"  \"Charlee carroll\"  \"Emma medrano\"    \n [5] \"Ivy   mccann\"     \"Ivy mccann\"       \"Jasper decker\"    \"Kane martin\"     \n [9] \"Ryan edwards\"     \"Ryan edwards  \"   \"Tatum chavez\"     \"Tatum s chavez\"  \n[13] \"Weston fox\"       \"Weston fox,\"      \"Zariah hernandez\"\n\n\nEach of the function above can come in handy from time-to-time. So, you may just want to keep them in your back pocket. Let’s go ahead and use the str_to_lower() function now as the first step in cleaning our data:\n\nehr &lt;- ehr %&gt;% \n  mutate(name = str_to_lower(name)) %&gt;% \n  print()\n\n# A tibble: 15 × 6\n   admit_date          name               dob        address      city  symptoms\n   &lt;dttm&gt;              &lt;chr&gt;              &lt;date&gt;     &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;   \n 1 2017-02-01 05:22:30 \"zariah hernandez\" 1944-09-27 3201 ORANGE… FORT… \"\\\"Pain…\n 2 2017-04-08 09:17:17 \"tatum chavez\"     1952-06-12 1117 richmo… Fort… \"Pain\"  \n 3 2017-04-18 09:17:17 \"tatum s chavez\"   1952-06-12 1117 richmo… Fort… \"Pain\"  \n 4 2017-08-31 18:29:34 \"arabella george\"  1966-06-15 357 Angle    FORT… \"\\\"Naus…\n 5 2017-09-13 06:27:07 \"jasper decker\"    1954-05-11 3612 LAURA … FORT… \"\\\"Pain…\n 6 2017-09-15 18:29:34 \"arabella george\"  1966-06-15 357 Angle    FORT… \"\\\"Naus…\n 7 2017-10-07 06:31:18 \"weston fox\"       2009-08-21 6433 HATCHE… City… \"Pain\"  \n 8 2017-10-08 23:17:18 \"ryan edwards\"     1917-12-10 3201 HORIZO… City…  &lt;NA&gt;   \n 9 2017-10-16 06:31:18 \"weston fox,\"      2009-08-21 6433 HATCHE… City… \"Pain\"  \n10 2017-10-26 23:17:18 \"ryan edwards  \"   1917-12-10 3201 HORIZO… City…  &lt;NA&gt;   \n11 2017-10-27 18:37:00 \"emma medrano\"     1975-05-01 6301 BEECHC… KELL… \"\\\"Naus…\n12 2017-12-18 20:47:48 \"ivy mccann\"       1911-06-21 5426 CHILDR… FORT… \"\\\"Head…\n13 2017-12-20 13:40:04 \"charlee carroll\"  1908-07-22 8190 DUCK C… City… \"Headac…\n14 2017-12-26 20:47:48 \"ivy   mccann\"     1911-06-21 5426 CHILDR… FORT… \"\\\"Head…\n15 2018-01-28 08:49:38 \"kane martin\"      1939-10-27 4929 asbury  FORT…  &lt;NA&gt;   \n\n\n👆Here’s what we did above:\n\nwe used stringr’s str_to_lower() function to coerce all the letters in the name column to lowercase.\n\nNow, let’s check and see how many unique people R finds in our data?\n\nehr %&gt;% \n  group_by(name) %&gt;% \n  mutate(dup = row_number() &gt; 1) %&gt;% \n  arrange(name) %&gt;% \n  select(name, dup, dob, address, city)\n\n# A tibble: 15 × 5\n# Groups:   name [14]\n   name               dup   dob        address             city              \n   &lt;chr&gt;              &lt;lgl&gt; &lt;date&gt;     &lt;chr&gt;               &lt;chr&gt;             \n 1 \"arabella george\"  FALSE 1966-06-15 357 Angle           FORT WORTH        \n 2 \"arabella george\"  TRUE  1966-06-15 357 Angle           FORT WORTH        \n 3 \"charlee carroll\"  FALSE 1908-07-22 8190 DUCK CREEK CT  City of Fort Worth\n 4 \"emma medrano\"     FALSE 1975-05-01 6301 BEECHCREEK DR  KELLER            \n 5 \"ivy   mccann\"     FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 6 \"ivy mccann\"       FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 7 \"jasper decker\"    FALSE 1954-05-11 3612 LAURA ANNE CT. FORT WORTH        \n 8 \"kane martin\"      FALSE 1939-10-27 4929 asbury         FORT WORTH        \n 9 \"ryan edwards\"     FALSE 1917-12-10 3201 HORIZON PL     City of Saginaw   \n10 \"ryan edwards  \"   FALSE 1917-12-10 3201 HORIZON PL     City of Saginaw   \n11 \"tatum chavez\"     FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n12 \"tatum s chavez\"   FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n13 \"weston fox\"       FALSE 2009-08-21 6433 HATCHER ST     City of Fort Worth\n14 \"weston fox,\"      FALSE 2009-08-21 6433 HATCHER ST     City of Fort Worth\n15 \"zariah hernandez\" FALSE 1944-09-27 3201 ORANGE AVE     FORT WORTH        \n\n\nIn the output above, there are 15 rows. R has identified 1 row with a duplicate name (dup == TRUE), which results in a count of 14 unique people. So, simply coercing all the letters to lower case alone helped R figure out that there was a duplicate name value for arabella george. Next, let’s go ahead and remove the trailing space from Ryan Edwards’ name.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Working with Character Strings</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_character_strings/working_with_character_strings.html#trim-white-space",
    "href": "chapters/working_with_character_strings/working_with_character_strings.html#trim-white-space",
    "title": "29  Working with Character Strings",
    "section": "29.2 Trim white space",
    "text": "29.2 Trim white space\nwe can use stringr’s str_trim() function to “trim” white space from the beginning and end of character strings. For example:\n\nstr_trim(\"Ryan Edwards  \")\n\n[1] \"Ryan Edwards\"\n\n\nLet’s go ahead and use the str_trim() function now as the next step in cleaning our data:\n\nehr &lt;- ehr %&gt;% \n  mutate(name = str_trim(name))\n\nNow, let’s check and see how many unique people R finds in our data?\n\nehr %&gt;% \n  group_by(name) %&gt;% \n  mutate(dup = row_number() &gt; 1) %&gt;% \n  arrange(name) %&gt;% \n  select(name, dup, dob, address, city)\n\n# A tibble: 15 × 5\n# Groups:   name [13]\n   name             dup   dob        address             city              \n   &lt;chr&gt;            &lt;lgl&gt; &lt;date&gt;     &lt;chr&gt;               &lt;chr&gt;             \n 1 arabella george  FALSE 1966-06-15 357 Angle           FORT WORTH        \n 2 arabella george  TRUE  1966-06-15 357 Angle           FORT WORTH        \n 3 charlee carroll  FALSE 1908-07-22 8190 DUCK CREEK CT  City of Fort Worth\n 4 emma medrano     FALSE 1975-05-01 6301 BEECHCREEK DR  KELLER            \n 5 ivy   mccann     FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 6 ivy mccann       FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 7 jasper decker    FALSE 1954-05-11 3612 LAURA ANNE CT. FORT WORTH        \n 8 kane martin      FALSE 1939-10-27 4929 asbury         FORT WORTH        \n 9 ryan edwards     FALSE 1917-12-10 3201 HORIZON PL     City of Saginaw   \n10 ryan edwards     TRUE  1917-12-10 3201 HORIZON PL     City of Saginaw   \n11 tatum chavez     FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n12 tatum s chavez   FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n13 weston fox       FALSE 2009-08-21 6433 HATCHER ST     City of Fort Worth\n14 weston fox,      FALSE 2009-08-21 6433 HATCHER ST     City of Fort Worth\n15 zariah hernandez FALSE 1944-09-27 3201 ORANGE AVE     FORT WORTH        \n\n\nIn the output above, there are 15 rows. R has identified 2 rows with a duplicate name (dup == TRUE), which results in a count of 13 unique people. We’re getting closer. 👏 However, the rest of the discrepancies in the name column that we want to address are a little more complicated. There isn’t a pre-made base R or stringr function that will fix them. Instead, we’ll need to learn how to use something called regular expressions.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Working with Character Strings</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_character_strings/working_with_character_strings.html#regular-expressions",
    "href": "chapters/working_with_character_strings/working_with_character_strings.html#regular-expressions",
    "title": "29  Working with Character Strings",
    "section": "29.3 Regular expressions",
    "text": "29.3 Regular expressions\nRegular expressions, also called regex or regexps, can be really intimidating at first. In fact, I debated whether or not to even include a discussion of regular expressions at this point in the book. However, regular expressions are the most powerful and flexible tool for manipulating character strings that I are aware of. So, I think it’s important for you to get a little exposure to regular expressions, even if you aren’t a regular expressions expert by the end of this chapter.\nThe first time you see regular expressions, you will probably think they look like gibberish. For example, here’s a regular expression that I recently used to clean a data set (\\d{1,2}\\/\\d{1,2}\\/\\d{2}). You can think of regular expressions as an entirely different programming language that the R interpreter can also understand. Regular expressions aren’t unique to R. Many programming languages can accept regular expressions as a way to manipulate character strings.\nIn the examples that follow, we hope\n1. To give you a feel for how regular expression can be useful.\n2. Provide you with some specific regular expressions that you may want to save for your epi work (or your class assignments).\n3. Provide you with some resources to help you take your regular expression skills to the next level when you are ready.\n\n29.3.1 Remove the comma\nFor our first example, let’s remove the comma from Weston Fox’s last name.\n\nstr_replace(\n  string      = \"weston fox,\", \n  pattern     = \",\",\n  replacement = \"\"\n)\n\n[1] \"weston fox\"\n\n\n👆Here’s what we did above:\n\nwe used stringr’s str_replace() function remove the comma from the character string “weston fox,”.\nThe first argument to the str_replace() function is string. The value passed the string argument should be the character string, or vector of character strings, we want to manipulate.\nThe second argument to the str_replace() function is pattern. The value passed the pattern argument should be regular expression. It should tell the str_replace() function what part of the character string we want to replace. In this case, it is a comma (\",\"). We are telling the str_replace() function that we want it to replace the first comma it sees in the character string “weston fox,” with the value we pass to the replacement argument.\nThe third argument to the str_replace() function is replacement. The value passed the replacement argument should also be regular expression. It should tell the str_replace() function to what replace the value identified in the pattern argument with. In this case, it is nothing (\"\") – two double quotes with nothing in-between. We are telling the str_replace() function that we want it to replace the first comma it sees in the character string “weston fox,” with nothing. This is sort of a long-winded way of saying, “delete the comma.”\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice that our regular expressions above are wrapped in quotes. Regular expressions should always be wrapped in quotes.\n\n\nLet’s go ahead and use the str_replace() function now as the next step in cleaning our data:\n\nehr &lt;- ehr %&gt;% \n  mutate(name = str_replace(name, \",\", \"\"))\n\nNow, let’s check and see how many unique people R finds in our data?\n\nehr %&gt;% \n  group_by(name) %&gt;% \n  mutate(dup = row_number() &gt; 1) %&gt;% \n  arrange(name) %&gt;% \n  select(name, dup, dob, address, city)\n\n# A tibble: 15 × 5\n# Groups:   name [12]\n   name             dup   dob        address             city              \n   &lt;chr&gt;            &lt;lgl&gt; &lt;date&gt;     &lt;chr&gt;               &lt;chr&gt;             \n 1 arabella george  FALSE 1966-06-15 357 Angle           FORT WORTH        \n 2 arabella george  TRUE  1966-06-15 357 Angle           FORT WORTH        \n 3 charlee carroll  FALSE 1908-07-22 8190 DUCK CREEK CT  City of Fort Worth\n 4 emma medrano     FALSE 1975-05-01 6301 BEECHCREEK DR  KELLER            \n 5 ivy   mccann     FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 6 ivy mccann       FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 7 jasper decker    FALSE 1954-05-11 3612 LAURA ANNE CT. FORT WORTH        \n 8 kane martin      FALSE 1939-10-27 4929 asbury         FORT WORTH        \n 9 ryan edwards     FALSE 1917-12-10 3201 HORIZON PL     City of Saginaw   \n10 ryan edwards     TRUE  1917-12-10 3201 HORIZON PL     City of Saginaw   \n11 tatum chavez     FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n12 tatum s chavez   FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n13 weston fox       FALSE 2009-08-21 6433 HATCHER ST     City of Fort Worth\n14 weston fox       TRUE  2009-08-21 6433 HATCHER ST     City of Fort Worth\n15 zariah hernandez FALSE 1944-09-27 3201 ORANGE AVE     FORT WORTH        \n\n\nIn the output above, there are 15 rows. R has identified 3 rows with a duplicate name (dup == TRUE), which results in a count of 12 unique people.\n\n\n29.3.2 Remove middle initial\nNext, let’s remove the middle initial from Tatum Chavez’s name.\n\nstr_replace(\n  string      = \"tatum s chavez\",\n  pattern     = \" \\\\w \",\n  replacement = \" \"\n)\n\n[1] \"tatum chavez\"\n\n\n👆Here’s what we did above:\n\nwe used stringr’s str_replace() function remove the “s” from the character string “tatum s chavez”.\nThe first argument to the str_replace() function is string. The value passed the string argument should be the character string, or vector of character strings, we want to manipulate.\nThe second argument to the str_replace() function is pattern. The value passed the pattern argument should be regular expression. It should tell the str_replace() function what part of the character string we want to replace. In this case, it is \" \\\\w \". That is a space, two backslashes, a “w,” and a space. This regular expression looks a little stranger than the last one we saw.\n\nThe \\w is called a token in regular expression lingo. The \\w token means “Any word character.” Any word character includes all the letters of the alphabet upper and lowercase (i.e., [a-zA-Z]), all numbers (i.e., [0-9]), and the underscore character (_).\nWhen passing regular expression to R, we must always add an additional backslash in front of any other backslash in the regular expression. In this case, \\\\w instead of \\w.\nIf we had stopped here (\"\\\\w\"), this regular expression would have told the str_replace() function that we want it to replace the first word character it sees in the character string “tatum s chavez” with the value we pass to the replacement argument. In this case, that would have been the “t” at the beginning of “tatum s chavez”.\nThe final component of the regular expression we passed to the pattern argument is spaces on both sides of the \\\\w token. The complete regular expression, \" \\\\w \", tells the str_replace() function that we want it to replace the first time it sees a space, followed by any word character, followed by another space in the character string “tatum s chavez” with the value we pass to the replacement argument. The first section of the character string above that matches that pattern is the ” s ” in “tatum s chavez”.\n\nThe third argument to the str_replace() function is replacement. The value passed the replacement argument should also be regular expression. It should tell the str_replace() function what to replace the value identified in the pattern argument with. In this case, it is a single space (\" \").\n\nLet’s go ahead and use the str_replace() function now as the next step in cleaning our data:\n\nehr &lt;- ehr %&gt;% \n  mutate(name = str_replace(name, \" \\\\w \", \" \"))\n\nAnd, let’s once again check and see how many unique people R finds in our data?\n\nehr %&gt;% \n  group_by(name) %&gt;% \n  mutate(dup = row_number() &gt; 1) %&gt;% \n  arrange(name) %&gt;% \n  select(name, dup, dob, address, city)\n\n# A tibble: 15 × 5\n# Groups:   name [11]\n   name             dup   dob        address             city              \n   &lt;chr&gt;            &lt;lgl&gt; &lt;date&gt;     &lt;chr&gt;               &lt;chr&gt;             \n 1 arabella george  FALSE 1966-06-15 357 Angle           FORT WORTH        \n 2 arabella george  TRUE  1966-06-15 357 Angle           FORT WORTH        \n 3 charlee carroll  FALSE 1908-07-22 8190 DUCK CREEK CT  City of Fort Worth\n 4 emma medrano     FALSE 1975-05-01 6301 BEECHCREEK DR  KELLER            \n 5 ivy   mccann     FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 6 ivy mccann       FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 7 jasper decker    FALSE 1954-05-11 3612 LAURA ANNE CT. FORT WORTH        \n 8 kane martin      FALSE 1939-10-27 4929 asbury         FORT WORTH        \n 9 ryan edwards     FALSE 1917-12-10 3201 HORIZON PL     City of Saginaw   \n10 ryan edwards     TRUE  1917-12-10 3201 HORIZON PL     City of Saginaw   \n11 tatum chavez     FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n12 tatum chavez     TRUE  1952-06-12 1117 richmond ave   Fort Worth        \n13 weston fox       FALSE 2009-08-21 6433 HATCHER ST     City of Fort Worth\n14 weston fox       TRUE  2009-08-21 6433 HATCHER ST     City of Fort Worth\n15 zariah hernandez FALSE 1944-09-27 3201 ORANGE AVE     FORT WORTH        \n\n\nIn the output above, there are 15 rows. R has identified 4 rows with a duplicate name (dup == TRUE), which results in a count of 11 unique people.\n\n\n29.3.3 Remove double spaces\nFinally, let’s remove the double space from Ivy Mccann’s name.\n\nstr_replace(\n  string      = \"Ivy   Mccann\",\n  pattern     = \"\\\\s{2,}\",\n  replacement = \" \"\n)\n\n[1] \"Ivy Mccann\"\n\n\n👆Here’s what we did above:\n\nwe used stringr’s str_replace() function remove the double space from the character string “Ivy Mccann”.\nThe first argument to the str_replace() function is string. The value passed the string argument should be the character string, or vector of character strings, we want to manipulate.\nThe second argument to the str_replace() function is pattern. The value passed the pattern argument should be regular expression. It should tell the str_replace() function what part of the character string we want to replace. In this case, it is \\\\s{2,}. This regular expression looks even more strange than the last one we saw.\n\nThe \\s is another token. The \\s token means “Any whitespace character.”\nWhen passing regular expression to R, we must always add an additional backslash in front of any other backslash in the regular expression. In this case, \\\\s instead of \\s.\nThe curly braces with numbers inside is called a quantifier in regular expression lingo. The first number inside the curly braces tells str_replace() to look for at least this many occurrences of whatever is immediately before the curly braces in the regular expression. The second number inside the curly braces tells str_replace() to look for no more than this many occurrences of whatever is immediately before the curly braces in the regular expression. When there is no number in the first position, that means that there is no minimum number of occurrences that count. When there is no number is the second position, that means that there is no upper limit of occurrences that count. In this case, the thing immediately before the curly braces in the regular expression was a whitespace (\\\\s), and the {2,} tells str_replace() to look for between 2 and unlimited consecutive occurrences of whitespace.\n\nThe third argument to the str_replace() function is replacement. The value passed the replacement argument should also be regular expression. It should tell the str_replace() function what to replace the value identified in the pattern argument with. In this case, it is a single space (\" \").\n\nLet’s go ahead and use the str_replace() function now as the final step in cleaning our name column:\n\nehr &lt;- ehr %&gt;% \n  mutate(name = str_replace(name, \"\\\\s{2,}\", \" \"))\n\nLet’s check one final time to see how many unique people R finds in our data.\n\nehr %&gt;% \n  group_by(name) %&gt;% \n  mutate(dup = row_number() &gt; 1) %&gt;% \n  arrange(name) %&gt;% \n  select(name, dup, dob, address, city)\n\n# A tibble: 15 × 5\n# Groups:   name [10]\n   name             dup   dob        address             city              \n   &lt;chr&gt;            &lt;lgl&gt; &lt;date&gt;     &lt;chr&gt;               &lt;chr&gt;             \n 1 arabella george  FALSE 1966-06-15 357 Angle           FORT WORTH        \n 2 arabella george  TRUE  1966-06-15 357 Angle           FORT WORTH        \n 3 charlee carroll  FALSE 1908-07-22 8190 DUCK CREEK CT  City of Fort Worth\n 4 emma medrano     FALSE 1975-05-01 6301 BEECHCREEK DR  KELLER            \n 5 ivy mccann       FALSE 1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 6 ivy mccann       TRUE  1911-06-21 5426 CHILDRESS ST   FORT WORTH        \n 7 jasper decker    FALSE 1954-05-11 3612 LAURA ANNE CT. FORT WORTH        \n 8 kane martin      FALSE 1939-10-27 4929 asbury         FORT WORTH        \n 9 ryan edwards     FALSE 1917-12-10 3201 HORIZON PL     City of Saginaw   \n10 ryan edwards     TRUE  1917-12-10 3201 HORIZON PL     City of Saginaw   \n11 tatum chavez     FALSE 1952-06-12 1117 richmond ave   Fort Worth        \n12 tatum chavez     TRUE  1952-06-12 1117 richmond ave   Fort Worth        \n13 weston fox       FALSE 2009-08-21 6433 HATCHER ST     City of Fort Worth\n14 weston fox       TRUE  2009-08-21 6433 HATCHER ST     City of Fort Worth\n15 zariah hernandez FALSE 1944-09-27 3201 ORANGE AVE     FORT WORTH        \n\n\nIn the output above, there are 15 rows. R has identified 5 rows with a duplicate name (dup == TRUE), which results in a count of 10 unique people. This is the answer we wanted! 👏\nIf our data frame was too big to count unique people manually, we could have R calculate the number of unique people for us like this:\n\nehr %&gt;% \n  group_by(name) %&gt;% \n  filter(row_number() == 1) %&gt;% \n  ungroup() %&gt;% \n  summarise(`Unique People` = n())\n\n# A tibble: 1 × 1\n  `Unique People`\n            &lt;int&gt;\n1              10\n\n\n👆Here’s what we did above:\n\nWith the exception of filter(row_number() == 1), you should have seen all of the elements in the code above before.\nwe saw the row_number() function used before inside of mutate() to sequentially count the number of rows that belong to each group created with group_by(). We could have done that in the code above. The filter(row_number() == 1) code is really just a shorthand way to write mutate(row = row_number()) %&gt;% filter(row == 1). It has the effect of telling R to just keep the first row for each group created by group_by(). In this case, just keep the first row for each name in the data frame.\n\nNow that we know how many unique people are in our data, let’s say we want to know how many of them live in each city that our data contains.\nFirst, we will subset our data to include one row only for each person:\n\nehr_unique &lt;- ehr %&gt;% \n  group_by(name) %&gt;% \n  filter(row_number() == 1) %&gt;% \n  ungroup() %&gt;% \n  print()\n\n# A tibble: 10 × 6\n   admit_date          name             dob        address        city  symptoms\n   &lt;dttm&gt;              &lt;chr&gt;            &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   \n 1 2017-02-01 05:22:30 zariah hernandez 1944-09-27 3201 ORANGE A… FORT… \"\\\"Pain…\n 2 2017-04-08 09:17:17 tatum chavez     1952-06-12 1117 richmond… Fort… \"Pain\"  \n 3 2017-08-31 18:29:34 arabella george  1966-06-15 357 Angle      FORT… \"\\\"Naus…\n 4 2017-09-13 06:27:07 jasper decker    1954-05-11 3612 LAURA AN… FORT… \"\\\"Pain…\n 5 2017-10-07 06:31:18 weston fox       2009-08-21 6433 HATCHER … City… \"Pain\"  \n 6 2017-10-08 23:17:18 ryan edwards     1917-12-10 3201 HORIZON … City…  &lt;NA&gt;   \n 7 2017-10-27 18:37:00 emma medrano     1975-05-01 6301 BEECHCRE… KELL… \"\\\"Naus…\n 8 2017-12-18 20:47:48 ivy mccann       1911-06-21 5426 CHILDRES… FORT… \"\\\"Head…\n 9 2017-12-20 13:40:04 charlee carroll  1908-07-22 8190 DUCK CRE… City… \"Headac…\n10 2018-01-28 08:49:38 kane martin      1939-10-27 4929 asbury    FORT…  &lt;NA&gt;   \n\n\nLet’s go ahead and get an initial count of how many people live in each city:\n\nehr %&gt;% \n  group_by(city) %&gt;% \n  summarise(n = n())\n\n# A tibble: 5 × 2\n  city                   n\n  &lt;chr&gt;              &lt;int&gt;\n1 City of Fort Worth     3\n2 City of Saginaw        2\n3 FORT WORTH             7\n4 Fort Worth             2\n5 KELLER                 1\n\n\nI’m sure you saw this coming, but we have more data entry discrepancies that are preventing us from completing our analysis. Now that you’ve gotten your feet wet with character string manipulation and regular expressions, what do we need to do in order to complete our analysis?\nHopefully, your first instinct by now is to coerce all the letters to lowercase. In fact, one of the first things we typically do is coerce all character columns to lowercase. Let’s do that now.\n\nehr &lt;- ehr %&gt;% \n  mutate(\n    address = tolower(address),\n    city    = tolower(city)\n  )\n\nNow how many people live in each city?\n\nehr %&gt;% \n  group_by(city) %&gt;% \n  summarise(n = n())\n\n# A tibble: 4 × 2\n  city                   n\n  &lt;chr&gt;              &lt;int&gt;\n1 city of fort worth     3\n2 city of saginaw        2\n3 fort worth             9\n4 keller                 1\n\n\nwe’re getting closer to the right answer, but we still need to remove “city of” from some of the values. This sounds like another job for str_replace().\n\nstr_replace(\n  string      = \"city of fort worth\",\n  pattern     = \"city of \",\n  replacement = \"\"\n)\n\n[1] \"fort worth\"\n\n\nThat regular expression looks like it will work. Let’s go ahead and use it to remove “city of” from the values in the address_city column now.\n\nehr &lt;- ehr %&gt;% \n  mutate(city = str_replace(city, \"city of \", \"\"))\n\nOne last time, how many people live in each city?\n\nehr %&gt;% \n  group_by(city) %&gt;% \n  summarise(n = n())\n\n# A tibble: 3 × 2\n  city           n\n  &lt;chr&gt;      &lt;int&gt;\n1 fort worth    12\n2 keller         1\n3 saginaw        2",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Working with Character Strings</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_character_strings/working_with_character_strings.html#separate-values-into-component-parts",
    "href": "chapters/working_with_character_strings/working_with_character_strings.html#separate-values-into-component-parts",
    "title": "29  Working with Character Strings",
    "section": "29.4 Separate values into component parts",
    "text": "29.4 Separate values into component parts\nAnother common task that I perform on character strings is to separate the strings into multiple parts. For example, sometimes we may want to separate full names into two columns. One for fist name and one for last name. To complete this task, we will once again use regular expressions. We will also learn how to use the str_extract() function to pull values out of a character string when the match a pattern we create with a regular expression.\n\nstr_extract(\"zariah hernandez\", \"^\\\\w+\")\n\n[1] \"zariah\"\n\n\n👆Here’s what we did above:\n\nwe used stringr’s str_extract() function pull the first name out of the full name “zariah hernandez”.\nThe first argument to the str_extract() function is string. The value passed the string argument should be the character string, or vector of character strings, we want to manipulate.\nThe second argument to the str_extract() function is pattern. The value passed the pattern argument should be regular expression. It should tell the str_extract() function what part of the character string we want to pull out of the character string. In this case, it is ^\\\\w+.\n\nwe’ve already seen that the \\w token means “Any word character.”\nWhen passing regular expression to R, we must always add an additional backslash in front of any other backslash in the regular expression. In this case, \\\\w instead of \\w.\nThe carrot (^) is a type of anchor in regular expression lingo. It tells the str_extract() function to look for the pattern at the start of the character sting only.\nThe plus sign (+) is another quantifier. It means, “match the pattern one or more times.”\nTaken together, ^\\\\w+ tells the str_extract() function to look for one or more consecutive word characters beginning at the start of the character string and extract them.\nThe first word character at the start of the string is “z”, then “a”, then “riah”. Finally, R gets to the space between “zariah” and “hernandez”, which isn’t a word character, and stops the extraction. The result is “zariah”.\n\n\nwe can pull the last name from the character string in a similar way:\n\nstr_extract(\"zariah hernandez\", \"\\\\w+$\")\n\n[1] \"hernandez\"\n\n\n👆Here’s what we did above:\n\nwe used stringr’s str_extract() function pull the last name out of the full name “zariah hernandez”.\nThe first argument to the str_extract() function is string. The value passed the string argument should be the character string, or vector of character strings, we want to manipulate.\nThe second argument to the str_extract() function is pattern. The value passed the pattern argument should be regular expression. It should tell the str_extract() function what part of the character string we want to pull out of the character string. In this case, it is \\\\w+$.\n\nwe’ve already seen that the \\w token means “Any word character.”\nWhen passing regular expression to R, we must always add an additional backslash in front of any other backslash in the regular expression. In this case, \\\\w instead of \\w.\nThe dollar sign ($) is another type of anchor. It tells the str_extract() function to look for the pattern at the end of the string only.\nwe’ve already seen that the plus sign (+) is a quantifier that means, “match the pattern one or more times.”\n\n-Taken together, \\\\w+$ tells the str_extract() function to look for one or more consecutive word characters beginning at the end of the string and extract them.\n\nThe first word character at the end of the string is “z”, then “e”, then “dnanreh”. Finally, R gets to the space between “zariah” and “hernandez”, which isn’t a word character, and stops the extraction. The result is “hernandez”.\n\n\nNow, let’s use str_extract() to separate full name into name_first and name_last.\n\nehr &lt;- ehr %&gt;% \n  mutate(\n    # Separate name into first name and last name\n    name_first = str_extract(name, \"^\\\\w+\"),\n    name_last  = str_extract(name, \"\\\\w+$\")\n  ) \n\n\nehr %&gt;% \n  select(name, name_first, name_last)\n\n# A tibble: 15 × 3\n   name             name_first name_last\n   &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;    \n 1 zariah hernandez zariah     hernandez\n 2 tatum chavez     tatum      chavez   \n 3 tatum chavez     tatum      chavez   \n 4 arabella george  arabella   george   \n 5 jasper decker    jasper     decker   \n 6 arabella george  arabella   george   \n 7 weston fox       weston     fox      \n 8 ryan edwards     ryan       edwards  \n 9 weston fox       weston     fox      \n10 ryan edwards     ryan       edwards  \n11 emma medrano     emma       medrano  \n12 ivy mccann       ivy        mccann   \n13 charlee carroll  charlee    carroll  \n14 ivy mccann       ivy        mccann   \n15 kane martin      kane       martin   \n\n\nThe regular expressions we used in the examples above weren’t super complex. We hope that leaves you feeling like you can use regular expression to complete data cleaning tasks that are actually useful, even if you haven’t totally mastered them yet (I haven’t totally mastered them either).\nBefore moving on, we want to introduce you to a free tool I use when I have to do more complex character string manipulations with regular expressions. It is the regular expressions 101 online regex tester and debugger.\n\n\n\n\n\n\n\n\n\nIn the screenshot above, I highlight some of the really cool features of the regex tester and debugger.\n\nFirst, you can use the regex tester without logging in. However, I typically do log in because that allows me to save regular expressions and use them again later.\nThe top input box on the screen corresponds to what you would type into the pattern argument of the str_replace() function.\nThe middle input box on the screen corresponds to what you would type into the string argument of the str_replace() function.\nThe third input box on the screen corresponds to what you would type into the replacement argument of the str_replace() function, and the results are presented below.\nIn addition, the regex tester and debugger has a quick reference pane that allows you to lookup different elements you might want to use in your regular expression. It also has an explanation pane that tells you what each of the elements in the current regular expression you typed out mean.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Working with Character Strings</span>"
    ]
  },
  {
    "objectID": "chapters/working_with_character_strings/working_with_character_strings.html#dummy-variables",
    "href": "chapters/working_with_character_strings/working_with_character_strings.html#dummy-variables",
    "title": "29  Working with Character Strings",
    "section": "29.5 Dummy variables",
    "text": "29.5 Dummy variables\nData collection tools in epidemiology often include “check all that apply” questions. In our ehr example data, patients were asked about what symptoms they were experiencing at admission. The choices were pain, headache, and nausea. They were allowed to check any combination of the three that they wanted. That results in a symptoms column in our data frame that looks like this:\n\n\n\n\n\n\nNote\n\n\n\nAny categorical variable can be transformed into dummy variables. Not just the variables that result from “check all that apply” survey questions. However, the “check all that apply” survey questions often require extra data cleaning steps relative to categorical variables that can only take a single value in each row.\n\n\n\nehr %&gt;% \n  select(name_first, name_last, symptoms)\n\n# A tibble: 15 × 3\n   name_first name_last symptoms                            \n   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;                               \n 1 zariah     hernandez \"\\\"Pain\\\", \\\"Headache\\\", \\\"Nausea\\\"\"\n 2 tatum      chavez    \"Pain\"                              \n 3 tatum      chavez    \"Pain\"                              \n 4 arabella   george    \"\\\"Nausea\\\", \\\"Headache\\\"\"          \n 5 jasper     decker    \"\\\"Pain\\\", \\\"Headache\\\"\"            \n 6 arabella   george    \"\\\"Nausea\\\", \\\"Headache\\\"\"          \n 7 weston     fox       \"Pain\"                              \n 8 ryan       edwards    &lt;NA&gt;                               \n 9 weston     fox       \"Pain\"                              \n10 ryan       edwards    &lt;NA&gt;                               \n11 emma       medrano   \"\\\"Nausea\\\", \\\"Headache\\\"\"          \n12 ivy        mccann    \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\"\n13 charlee    carroll   \"Headache\"                          \n14 ivy        mccann    \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\"\n15 kane       martin     &lt;NA&gt;                               \n\n\nNotice that some people didn’t report their symptoms (NA), some people reported only one symptom, and some people reported multiple symptoms. The way the data is currently formatted is not ideal for analysis. For example, if I asked you to tell me how many people ever came in complaining of headache, how would you do that? Maybe like this:\n\nehr %&gt;% \n  group_by(symptoms) %&gt;% \n  summarise(n = n())\n\n# A tibble: 7 × 2\n  symptoms                                 n\n  &lt;chr&gt;                                &lt;int&gt;\n1 \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\"     2\n2 \"\\\"Nausea\\\", \\\"Headache\\\"\"               3\n3 \"\\\"Pain\\\", \\\"Headache\\\"\"                 1\n4 \"\\\"Pain\\\", \\\"Headache\\\", \\\"Nausea\\\"\"     1\n5 \"Headache\"                               1\n6 \"Pain\"                                   4\n7  &lt;NA&gt;                                    3\n\n\nIn this case, you could probably count manually and get the right answer. But what if we had many more possible symptoms and many more rows. Counting would quickly become tedious and error prone. The solution is to create dummy variables. We can create dummy variables like this:\n\nehr &lt;- ehr %&gt;% \n  mutate(\n    pain     = str_detect(symptoms, \"Pain\"),\n    headache = str_detect(symptoms, \"Headache\"),\n    nausea   = str_detect(symptoms, \"Nausea\")\n  )\n\n\nehr %&gt;% \n  select(symptoms, pain, headache, nausea)\n\n# A tibble: 15 × 4\n   symptoms                             pain  headache nausea\n   &lt;chr&gt;                                &lt;lgl&gt; &lt;lgl&gt;    &lt;lgl&gt; \n 1 \"\\\"Pain\\\", \\\"Headache\\\", \\\"Nausea\\\"\" TRUE  TRUE     TRUE  \n 2 \"Pain\"                               TRUE  FALSE    FALSE \n 3 \"Pain\"                               TRUE  FALSE    FALSE \n 4 \"\\\"Nausea\\\", \\\"Headache\\\"\"           FALSE TRUE     TRUE  \n 5 \"\\\"Pain\\\", \\\"Headache\\\"\"             TRUE  TRUE     FALSE \n 6 \"\\\"Nausea\\\", \\\"Headache\\\"\"           FALSE TRUE     TRUE  \n 7 \"Pain\"                               TRUE  FALSE    FALSE \n 8  &lt;NA&gt;                                NA    NA       NA    \n 9 \"Pain\"                               TRUE  FALSE    FALSE \n10  &lt;NA&gt;                                NA    NA       NA    \n11 \"\\\"Nausea\\\", \\\"Headache\\\"\"           FALSE TRUE     TRUE  \n12 \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\" TRUE  TRUE     TRUE  \n13 \"Headache\"                           FALSE TRUE     FALSE \n14 \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\" TRUE  TRUE     TRUE  \n15  &lt;NA&gt;                                NA    NA       NA    \n\n\n👆Here’s what we did above:\n\nwe used stringr’s str_detect() function create three new dummy variables in our data frame.\nThe first argument to the str_detect() function is string. The value passed the string argument should be the character string, or vector of character stings, we want to manipulate.\nThe second argument to the str_detect() function is pattern. The value passed the pattern argument should be regular expression. The str_detect() function returns TRUE if it finds the pattern in the string and FALSE if it does not find the pattern in the string.\nInstead of having a single symptoms column that can take different combinations of the values pain, headache, and nausea, we create a new column for each value – the so-called dummy variables.\nEach dummy variable can take the value TRUE, FALSE, or NA. The value for each dummy variable is TRUE in rows were that symptom was reported and FALSE in rows where the symptom was not reported. For example, the value in the first row of the pain column is TRUE because the value in the first row of symptoms column (“Pain”, “Headache”, “Nausea”) includes “Pain”. However, the value in the fourth row of the pain column is FALSE because the value in the fourth row of symptoms column (“Nausea”, “Headache”) does not include “Pain”.\n\nNow, we can much more easily figure out how many people had each symptom.\n\ntable(ehr$headache)\n\n\nFALSE  TRUE \n    4     8 \n\n\nwe should acknowledge that dummy variables typically take the values 0 and 1 instead of FALSE and TRUE. We can easily coerce our dummy variable values to 0/1 using the as.numeric() function. For example:\n\nehr %&gt;% \n  select(pain) %&gt;% \n  mutate(pain_01 = as.numeric(pain))\n\n# A tibble: 15 × 2\n   pain  pain_01\n   &lt;lgl&gt;   &lt;dbl&gt;\n 1 TRUE        1\n 2 TRUE        1\n 3 TRUE        1\n 4 FALSE       0\n 5 TRUE        1\n 6 FALSE       0\n 7 TRUE        1\n 8 NA         NA\n 9 TRUE        1\n10 NA         NA\n11 FALSE       0\n12 TRUE        1\n13 FALSE       0\n14 TRUE        1\n15 NA         NA\n\n\nHowever, this step is sort of unnecessary in most cases because R treats TRUE and FALSE as 1 and 0 respectively when logical (i.e., TRUE/FALSE) vectors are passed to functions or operators that perform a mathematical operation.\nThat concludes the chapter on working with character strings. Don’t beat yourself up if you’re feeling confused about regular expressions. They are really tough to wrap your head around at first! But, at least now you know they exist and can be useful for manipulating character strings. If you come across more complicated situations in the future, we suggest starting by checking out the stringr cheat sheet and practicing with the regular expressions 101 online regex tester and debugger before writing any R code.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Working with Character Strings</span>"
    ]
  },
  {
    "objectID": "chapters/conditional_operations/conditional_operations.html",
    "href": "chapters/conditional_operations/conditional_operations.html",
    "title": "30  Conditional Operations",
    "section": "",
    "text": "30.1 Operands and operators\nThere will often be times that we want to modify the values in one column of our data based on the values in one or more other columns in our data. For example, maybe we want to create a column that contains the region of the country someone is from, based on another column that contains the state they are from.\nwe don’t really have a way to do this with the tools we currently have in our toolbox. We can manually type out all the region values, but that isn’t very scalable. Wouldn’t it be nice if we could just give R some rules, or conditions (e.g., TX is in the South, CA is in the West), and have R fill in the region values for us? Well, that’s exactly what we are going to learn how to do in this chapter.\nThese kinds of operations are called conditional operations because we type in a set of conditions, R evaluates those conditions, and then executes a different process or procedure based on whether or not the condition is met.\nAs a silly example, let’s say that we want our daughters to wear a raincoat if it’s raining outside, but we don’t want them to wear a raincoat if it is not raining outside. So, we give them a conditional request: “If it’s raining outside, then make sure to wear your raincoat, please. Otherwise, please don’t wear your raincoat.”\nIn this hypothetical scenario, they say, “yes, dad,” and then go to the window to see if it’s raining. Then, they choose their next action (i.e., raincoat wearing) depending on whether the condition (raining) is met or not.\nJust like we have to ask our daughters to put on a raincoat using conditional logic, we sometimes have to ask R to execute commands using conditional logic. Additionally, we have to do so in a way that R understands. For example, we can use dplyr’s if_else() function to ask R to execute commands conditionally. Let’s go ahead and take a look at an example now:\n👆Here’s what we did above:\nNow, let’s say that we want to create a new column in our data frame called raincoat. We want the value of raincoat to be wear on rainy days and no wear on days when it isn’t raining. Here’s how we can do that with the if_else() function:\n👆Here’s what we did above:\nBefore moving on, let’s dive into this a little further. R must always be able to reduce whatever value we pass to the condition argument of if_else() to TRUE or FALSE. That’s how R views any expression we pass to the condition argument. We can literally even pass the value TRUE or the value FALSE (not that doing so has much practical application):\nBecause the value passed to the condition argument is TRUE (in this case, literally), the if_else() function returns the value wear. What happens if we use this code to assign values to the raincoat column?\nAgain, the if_else() function returns the value wear because the value passed to the condition argument is TRUE. Then, R uses its recycling rules to copy the value wear to every row of the raincoat column. What would do you think will happen if we pass the value FALSE to the condition argument instead?\nHopefully, that was the result you expected. The if_else() function returns the value no wear because the value passed to the condition argument is FALSE. Then, R uses its recycling rules to copy the value no wear to every row of the raincoat column.\nwe can take this a step further and actually pass a vector of logical (TRUE/FALSE) values to the condition argument. For example:\nIn reality, that’s sort of what we did in the very first if_else() example above. But, instead of typing the values manually, we used an expression that returned a vector of logical values. Specifically, we used the equality operator (==) to check whether or not each value in the weather column was equal to the value “rain” or not.\nThat pretty much covers the basics of how the if_else() function works. Next, let’s take a look at some of the different combinations of operands and operators that we can combine and pass to the condition argument of the if_else() function.\nLet’s start by taking a look at some commonly used operands:\nAs we can see in the table above, operands are the values we want to check, or test. Operands can be variables or they can be individual values (also called constants). The example above (weather == \"rain\") contained two operands; the variable weather and the character constant \"rain\". The operator we used in this case was the equality operator (==). Next, let’s take a look at some other commonly used operators.\nwe think that most of the operators above will be familiar, or a least intuitive, for most of you. However, we do want to provide a little bit of commentary for a few of them.\ndf &lt;- tibble(\n  id        = c(1, 1, 2, 2),\n  outcome   = c(0, 1, 1, 1)\n) %&gt;% \n  print()\n\n# A tibble: 4 × 2\n     id outcome\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     1       0\n2     1       1\n3     2       1\n4     2       1\ndf %&gt;% \n  mutate(\n    # Odd rows are treatment A\n    # Even rows are treatment B\n    treatment = if_else(row_number() %% 2 == 1, \"A\", \"B\")\n  )\n\n# A tibble: 4 × 3\n     id outcome treatment\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    \n1     1       0 A        \n2     1       1 B        \n3     2       1 A        \n4     2       1 B\ndf &lt;- tibble(\n  name1 = c(\"Jon\", \"John\", NA),\n  name2 = c(\"Jon\", \"Jon\", \"Jon\")\n)\ndf %&gt;% \n  mutate(\n    name_match = name1 == name2\n  )\n\n# A tibble: 3 × 3\n  name1 name2 name_match\n  &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt;     \n1 Jon   Jon   TRUE      \n2 John  Jon   FALSE     \n3 &lt;NA&gt;  Jon   NA\nMany of us would expect the third value of the name_match column to be FALSE instead of NA. There are a couple of different ways we can get FALSE in the third row instead of NA. One way, although not necessarily the best way, is to use the if_else() function:\ndf %&gt;% \n  mutate(\n    name_match = name1 == name2,\n    name_match = if_else(is.na(name_match), FALSE, name_match)\n  )\n\n# A tibble: 3 × 3\n  name1 name2 name_match\n  &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt;     \n1 Jon   Jon   TRUE      \n2 John  Jon   FALSE     \n3 &lt;NA&gt;  Jon   FALSE\n👆Here’s what we did above:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Conditional Operations</span>"
    ]
  },
  {
    "objectID": "chapters/conditional_operations/conditional_operations.html#operands-and-operators",
    "href": "chapters/conditional_operations/conditional_operations.html#operands-and-operators",
    "title": "30  Conditional Operations",
    "section": "",
    "text": "we haven’t seen the %in% operator before, but we will wait to discuss it below.\nSome of you may have been a little surprised by the results we get from using less than (&lt;) and greater than (&gt;) with characters. It’s basically just testing alphabetical order. A comes before B in the alphabet, so A is less than B. Additionally, when two letters are the same, the upper-case letter is considered greater than the lowercase letter. However, alphabetical order takes precedence over case. So, b is still greater than A even though b is lowercase and A is upper case.\nMany of you may not have seen the modulus operator (%%) before. The modulus operator returns the remainder that is left after dividing two numbers. For example, 4 divided by 2 is 2 with a remainder of 0 because 2 goes into 4 exactly two times. Said another way, 2 * 2 = 4 and 4 - 4 = 0. So, 4 %% 2 = 0. However, 3 divided by 2 is 1 with a remainder of 1 because 2 goes into 3 one time with 1 left over. Said another way, 2 * 1 = 2 and 3 - 2 = 1. So, 3 %% 2 = 1. How is this useful? Well, the only times we can remember using the modulus operator have been when we needed to separate even and odd rows of a data frame. For example, let’s say that we have a data frame where each person has two rows. The first row always corresponds to treatment A and the second row always corresponds to treatment B. However, for some reason (maybe blinding?), there was no treatment column in the data when we received it. We could use the modulus operator to add a treatment column like this:\n\n\n\n\nwe also want to remind you that we should always use the is.na() function to check for missing values. Not the equality operator (==). Using the equality operator when there are missing values can give results that may be unexpected. For example:\n\n\n\n\n\n\n\nwe used dplyr’s if_else() function to assign the value FALSE to the column name_match where the original value of name_match was NA.\nThe value we passed to the condition argument was is.na(name_match). In doing so, we asked R to check each value of the name_match column and see if it was NA.\nIf it was NA, then we wanted to return the value that we passed to the true argument. Somewhat confusingly, the value we passed to the true argument was FALSE. All that means is that we wanted if_else() to return the literal value FALSE when the value for name_match was NA.\nIf the value in name_match was NOT NA, then we wanted to return the value that we passed to the false argument. In this case, we asked R to return the value that already exists in the name_match column.\nIn more informal language, we asked R to replace missing values in the name_match column with FALSE and leave the rest of the values unchanged.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Conditional Operations</span>"
    ]
  },
  {
    "objectID": "chapters/conditional_operations/conditional_operations.html#testing-multiple-conditions-simultaneously",
    "href": "chapters/conditional_operations/conditional_operations.html#testing-multiple-conditions-simultaneously",
    "title": "30  Conditional Operations",
    "section": "30.2 Testing multiple conditions simultaneously",
    "text": "30.2 Testing multiple conditions simultaneously\nSo far, we have only ever passed one condition to the condition argument of the if_else() function. However, we can pass as many conditions as we want. Having said that, more than 2, or maybe 3, gets very convoluted. Let’s go ahead and take a look at a couple of examples now. We’ll start by simulating some blood pressure data:\n\nblood_pressure &lt;- tibble(\n  id     = 1:10,\n  sysbp  = c(152, 120, 119, 123, 135, 83, 191, 147, 209, 166),\n  diasbp = c(78, 60, 88, 76, 85, 54, 116, 95, 100, 106)\n) %&gt;% \n  print()\n\n# A tibble: 10 × 3\n      id sysbp diasbp\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1   152     78\n 2     2   120     60\n 3     3   119     88\n 4     4   123     76\n 5     5   135     85\n 6     6    83     54\n 7     7   191    116\n 8     8   147     95\n 9     9   209    100\n10    10   166    106\n\n\nA person may be categorized as having normal blood pressure when their systolic blood pressure is less than 120 mmHG AND their diastolic blood pressure is less than 80 mmHG. We can use this information and the if_else() function to create a new column in our data frame that contains information about whether each person in our simulated data frame has normal blood pressure or not:\n\nblood_pressure %&gt;% \n  mutate(bp = if_else(sysbp &lt; 120 & diasbp &lt; 80, \"Normal\", \"Not Normal\"))\n\n# A tibble: 10 × 4\n      id sysbp diasbp bp        \n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1     1   152     78 Not Normal\n 2     2   120     60 Not Normal\n 3     3   119     88 Not Normal\n 4     4   123     76 Not Normal\n 5     5   135     85 Not Normal\n 6     6    83     54 Normal    \n 7     7   191    116 Not Normal\n 8     8   147     95 Not Normal\n 9     9   209    100 Not Normal\n10    10   166    106 Not Normal\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s if_else() function to create a new column in our data frame (bp) that contains information about whether each person has normal blood pressure or not.\nwe actually passed two conditions to the condition argument. The first condition was that the value of sysbp had to be less than 120. The second condition was that the value of diasbp had to be less than 80.\nBecause we separated these conditions with the AND operator (&), both conditions had to be true in order for the if_else() function to return the value we passed to the true argument – Normal. Otherwise, the if_else() function returned the value we passed to the false argument – Not Normal.\nParticipant 2 had a systolic blood pressure of 120 and a diastolic blood pressure of 60. Although 60 is less than 80 (condition number 2), 120 is not less than 120 (condition number 1). So, the value returned by the if_else() function was Not Normal.\nParticipant 3 had a systolic blood pressure of 119 and a diastolic blood pressure of 88 Although 119 is less than 120 (condition number 1), 88 is not less than 80 (condition number 2). So, the value returned by the if_else() function was Not Normal.\nParticipant 6 had a systolic blood pressure of 83 and a diastolic blood pressure of 54. In this case, conditions 1 and 2 were met. So, the value returned by the if_else() function was Normal.\n\nThis is useful! However, in some cases, we need to be able to test conditions sequentially, rather than simultaneously, and return a different value for each condition.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Conditional Operations</span>"
    ]
  },
  {
    "objectID": "chapters/conditional_operations/conditional_operations.html#testing-a-sequence-of-conditions",
    "href": "chapters/conditional_operations/conditional_operations.html#testing-a-sequence-of-conditions",
    "title": "30  Conditional Operations",
    "section": "30.3 Testing a sequence of conditions",
    "text": "30.3 Testing a sequence of conditions\nLet’s say that we wanted to create a new column in our blood_pressure data frame that contains each person’s blood pressure category according to the following scale:\n\n\n\n\n\n\n\n\n\nThis is the perfect opportunity to use dplyr’s case_when() function. Take a look:\n\nblood_pressure %&gt;% \n  mutate(\n    bp = case_when(\n      sysbp &lt; 120 & diasbp &lt; 80                               ~ \"Normal\",\n      sysbp &gt;= 120 & sysbp &lt; 130 & diasbp &lt; 80                ~ \"Elevated\",\n      sysbp &gt;= 130 & sysbp &lt; 140 | diasbp &gt;= 80 & diasbp &lt; 90 ~ \"Hypertension Stage 1\",\n      sysbp &gt;= 140 | diasbp &gt;= 90                             ~ \"Hypertension Stage 2\"\n    )\n  )\n\n# A tibble: 10 × 4\n      id sysbp diasbp bp                  \n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;               \n 1     1   152     78 Hypertension Stage 2\n 2     2   120     60 Elevated            \n 3     3   119     88 Hypertension Stage 1\n 4     4   123     76 Elevated            \n 5     5   135     85 Hypertension Stage 1\n 6     6    83     54 Normal              \n 7     7   191    116 Hypertension Stage 2\n 8     8   147     95 Hypertension Stage 2\n 9     9   209    100 Hypertension Stage 2\n10    10   166    106 Hypertension Stage 2\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s case_when() function to create a new column in our data frame (bp) that contains information about each person’s blood pressure category.\nYou can type ?case_when into our R console to view the help documentation for this function and follow along with the explanation below.\nThe case_when() function only has a single argument – the ... argument. You should pass one or more two-sided formulas separated by commas to this argument. What in the heck does that mean?\n\nWhen the help documentation refers to a two-sided formula, it means this: LHS ~ RHS. Here, LHS means left-hand side and RHS means right-hand side.\nThe LHS should be the condition or conditions that we want to test. You can think of this as being equivalent to the condition argument of the if_else() function.\nThe RHS should be the value we want the case_when() function to return when the condition on the left-hand side is met. You can think of this as being equivalent to the true argument of the if_else() function.\nThe tilde symbol (~) is used to separate the conditions on the left-hand side and the return values on the right-hand side.\n\nThe case_when() function doesn’t have a direct equivalent to the if_else() function’s false argument. Instead, it evaluates each two-sided formula sequentially until if finds a condition that is met. If it never finds a condition that is met, then it returns an NA. We will expand on this more below.\nFinally, we assigned all the values returned by the case_when() function to a new column that we named bp.\n\n\n\n\n\n\n\nNote\n\n\n\nTraditionally, the tilde symbol (~) is used to represent relationships in a statistical model. Here, it doesn’t have that meaning. We assume this symbol was picked somewhat out of necessity. Remember, any of the comparison operators, arithmetic operators, and logical operators may be used to define a condition in the left-hand side, and commas are used to separated multiple two-sided formulas. Therefore, there aren’t very many symbols left to choose from. Therefore, tilde it is. That’s our guess anyway.\n\n\nThe case_when() function was really useful for creating the bp column above, but there was also a lot going on there. Next, we’ll take a look at a slightly less complex example and clarify a few things along the way.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Conditional Operations</span>"
    ]
  },
  {
    "objectID": "chapters/conditional_operations/conditional_operations.html#recoding-variables",
    "href": "chapters/conditional_operations/conditional_operations.html#recoding-variables",
    "title": "30  Conditional Operations",
    "section": "30.4 Recoding variables",
    "text": "30.4 Recoding variables\nIn epidemiology, recoding variables is really common. For example, we may collect information about people’s ages as a continuous variable, but decide that it makes more sense to collapse age into age categories for our analysis. Let’s say that our analysis plan calls for assigning each of our participants to one of the following age categories:\n1 = child when the participant is less than 12 years old\n2 = adolescent when the participant is between the ages of 12 and less than 18\n3 = adult when the participant is 18 years old or older\n\n\n\n\n\n\nNote\n\n\n\nYou may not have ever heard of collapsing variables before. It simply means combing two or more values of our variable. We can collapse continuous variables into categories, as we discussed in the example above, or we can collapse categories into broader categories (as we will see with the race category example below). After we collapse a variable, it always contains fewer (and broader) possible values than it contained before we collapsed it.\n\n\nwe’re going to show you how to do this below using the case_when() function. However, we’re going to do it piecemeal so that we can highlight a few important concepts. First, let’s simulate some data that includes 10 participant’s ages.\n\n# Simulate some age data\nset.seed(123)\nages &lt;- tibble(\n  id  = 1:10,\n  age = c(sample(1:30, 9, TRUE), NA)\n) %&gt;% \n  print()\n\n# A tibble: 10 × 2\n      id   age\n   &lt;int&gt; &lt;int&gt;\n 1     1    15\n 2     2    19\n 3     3    14\n 4     4     3\n 5     5    10\n 6     6    18\n 7     7    22\n 8     8    11\n 9     9     5\n10    10    NA\n\n\nThen, let’s start the process of collapsing the age column into a new column called age_3cat that contains the 3 age categories we discussed above:\n\nages %&gt;% \n  mutate(\n    age_3cat = case_when(\n      age &lt; 12 ~ 1\n    )\n  )\n\n# A tibble: 10 × 3\n      id   age age_3cat\n   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1    15       NA\n 2     2    19       NA\n 3     3    14       NA\n 4     4     3        1\n 5     5    10        1\n 6     6    18       NA\n 7     7    22       NA\n 8     8    11        1\n 9     9     5        1\n10    10    NA       NA\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s case_when() function to create a new column in our data frame (age_3cat) that will eventually categorize each participant into one of 3 categories depending on their continuous age value.\nNotice that we only passed one two-sided formula to the case_when() function – age &lt; 12 ~ 1.\n\nThe RHS of the two-sided formula is age &lt; 12. This tells the case_when() function to check whether or not every value in the age column is less than 12 or not.\nThe LHS of the two-sided formula is 1. This tells the case_when() function what value to return each time it finds a value less than 12 in the age column.\nThe tilde symbol is used to separate the RHS and the LHS of the two-sided formula.\n\nHere is how the case_when() function basically works. It will test the condition on the left-hand side for each value of the variable, or variables, passed to the left-hand side (i.e., age). If the condition is met (i.e., &lt; 12), then it will return the value on the right-hand side of the tilde (i.e., 1). If the condition is not met, it will test the condition in the next two-sided formula. When there are no more two-sided formulas, then it will return an NA.\n\nAbove, the first value in age is 15. 15 is NOT less than 12. So, case_when() tries to move on to the next two-sided formula. However, there is no next two-sided formula. So, the first value returned by the case_when() function is NA. The same is true for the next two values of age.\nThe fourth value in age is 3. 3 is less than 12. So, the fourth value returned by the case_when() function is 1. And so on…\nFinally, after the case_when() function has tested all conditions, the returned values are assigned to a new column that we named age_3cat.\n\nNotice that we named the new variable age_3cat. We’re not sure where we picked up this naming convention, but we use it a lot when we collapse variables. The basic format is the name of variable we’re collapsing, an underscore, and the number of categories in the collapsed variable. We like using this convention for two reasons. First, the resulting column names are meaningful and informative. Second, we don’t have to spend any time trying to think of a different meaningful or informative name for my new variable. It’s totally fine if you don’t adopt this naming convention, but we would recommend that you try to use names that are more informative than age2 or something like that.\nNotice that we used a number (1) on the right-hand side of the two-sided formula above. We could have used a character value instead (i.e., child); however, for reasons we discussed in the section on factor variables, we prefer to recode my variables using numeric categories and then later creating a factor version of the variable using the _f naming convention.\n\nNow, let’s add a second two-sided formula to our case_when() function.\n\nages %&gt;% \n  mutate(\n    age_3cat = case_when(\n      age &lt; 12             ~ 1,\n      age &gt;= 12 & age &lt; 18 ~ 2\n    )\n  )\n\n# A tibble: 10 × 3\n      id   age age_3cat\n   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1    15        2\n 2     2    19       NA\n 3     3    14        2\n 4     4     3        1\n 5     5    10        1\n 6     6    18       NA\n 7     7    22       NA\n 8     8    11        1\n 9     9     5        1\n10    10    NA       NA\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s case_when() function to create a new column in our data frame (age_3cat) that will eventually categorize each participant into one of 3 categories depending on their continuous age value.\nNotice that this time we passed two two-sided formulas to the case_when() function – age &lt; 12 ~ 1 and age &gt;= 12 & age &lt; 18 ~ 2.\n\nNotice that we separated the two two-sided formulas with a comma (i.e., immediately after the 1 in age &lt; 12 ~ 1.\nNotice that the second two-sided formula is actually testing two conditions. First, it tests whether or not the value of age is greater than or equal to 12. Then, it tests whether or not the value of age is less than 18.\nBecause we separated the two conditions with the and operator (&), both must be TRUE for case_when() to return the value 2. Otherwise, it will move on to the next two-sided formula.\nAbove, the first value in age is 15. 15 is NOT less than 12. So, case_when() moves on to evaluate the next two-sided formula. 15 is greater than or equal to 12 AND 15 is less than 18. Because both conditions of the second two-sided formula were met, case-when() returns the value on the right-hand side of the second two-sided formula – 2. So, the first value returned by the case_when() function is 2.\nThe second value in age is 19. 19 is NOT less than 12. So, case_when() moves on to evaluate the next two-sided formula. 19 is greater than or equal to 12, but 19 is NOT less than 18. So, case_when() tries to move on to the next two-sided formula. However, there is no next two-sided formula. So, the second value returned by the case_when() function is NA.\nThe fourth value in age is 3. 3 is less than 12. So, the fourth value returned by the case_when() function is 1. At this point, because a condition was met, case_when() does not continue checking the current value of age against the remaining two-sided formulas. It returns a 1 and moves on to the next value of age.\nFinally, after the case_when() function has tested all conditions, the returned values are assigned to a new column that we named age_3cat.\n\nIn everyday speech, we may express the second two-sided condition above as “categorize all people between the ages of 12 and 18 as an adolescent.” we want to make two points about that before moving on.\n\nFirst, while that statement may be totally reasonable in everyday speech, it isn’t quite specific enough for what we are trying to do here. “Between 12 and 18” is a little bit ambiguous. What category is a person put in if they are exactly 12? What category are they put in if they are exactly 18? So, clearly we need to be more precise. We’re not aware of any hard and fast rules for making these kinds of decisions about categorization, but we tend to include the lower end of the range in the current category and exclude the value on the upper end of the range in the current category. So, in the example above, we would say, “categorize all people between the ages of 12 and less than 18 as an adolescent.”\nSecond, when we are testing for a “between” condition like this one, we often see students write code like this: age &gt;= 12 & &lt; 18. R won’t understand that. We have to use the column name in each condition to be tested (i.e., age &gt;= 12 & age &lt; 18), even though it doesn’t change. Otherwise, we get an error that looks something like this:\n\n\n\nages %&gt;% \n  mutate(\n    age_3cat = case_when(\n      age &lt; 12             ~ 1,\n      age &gt;= 12 & &lt; 18     ~ 2\n    )\n  )\n\nError in parse(text = input): &lt;text&gt;:5:19: unexpected '&lt;'\n4:       age &lt; 12             ~ 1,\n5:       age &gt;= 12 & &lt;\n                     ^\n\n\nOk, let’s go ahead and wrap up this age category variable:\n\nages %&gt;% \n  mutate(\n    age_3cat = case_when(\n      age &lt; 12             ~ 1,\n      age &gt;= 12 & age &lt; 18 ~ 2,\n      age &gt;= 18            ~ 3\n    )\n  )\n\n# A tibble: 10 × 3\n      id   age age_3cat\n   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1    15        2\n 2     2    19        3\n 3     3    14        2\n 4     4     3        1\n 5     5    10        1\n 6     6    18        3\n 7     7    22        3\n 8     8    11        1\n 9     9     5        1\n10    10    NA       NA\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s case_when() function to create a new column in our data frame (age_3cat) that categorized each participant into one of 3 categories depending on their continuous age value.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Conditional Operations</span>"
    ]
  },
  {
    "objectID": "chapters/conditional_operations/conditional_operations.html#case_when-is-lazy",
    "href": "chapters/conditional_operations/conditional_operations.html#case_when-is-lazy",
    "title": "30  Conditional Operations",
    "section": "30.5 case_when() is lazy",
    "text": "30.5 case_when() is lazy\nWhat do we mean when we say that case_when() is lazy? Well, it may not have registered when we mentioned it above, but case_when() stops evaluating two-sided functions for a value as soon as it finds one that is TRUE. For example:\n\ndf &lt;- tibble(\n  number = c(1, 2, 3)\n) %&gt;% \n  print()\n\n# A tibble: 3 × 1\n  number\n   &lt;dbl&gt;\n1      1\n2      2\n3      3\n\n\n\ndf %&gt;% \n  mutate(\n    size = case_when(\n      number &lt; 2 ~ \"Small\",\n      number &lt; 3 ~ \"Medium\",\n      number &lt; 4 ~ \"Large\"\n    )\n  )\n\n# A tibble: 3 × 2\n  number size  \n   &lt;dbl&gt; &lt;chr&gt; \n1      1 Small \n2      2 Medium\n3      3 Large \n\n\nWhy wasn’t the value for the size column Large in every row of the data frame? After all, 1, 2, and 3 are all less than 4, and number &lt; 4 was the final possible two-sided formula that could have been evaluated for each value of number. The answer is that case_when() is lazy. The first value in number is 1. 1 is less than 2. So, the condition in the first two-sided formula evaluates to TRUE. So, case_when() immediately returns the value on the right-hand side (Small) and does not continue checking two-sided formulas. It moves on to the next value of number.\nThe fact that case_when() is lazy isn’t a bad thing. It’s just something to be aware of. In fact, we can often use it to our advantage. For example, we can use case_when()’s laziness to rewrite the age_3cat code from above a little more succinctly:\n\nages %&gt;% \n  mutate(\n    age_3cat = case_when(\n      age &lt; 12  ~ 1,\n      age &lt; 18  ~ 2,\n      age &gt;= 18 ~ 3\n    )\n  )\n\n# A tibble: 10 × 3\n      id   age age_3cat\n   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1    15        2\n 2     2    19        3\n 3     3    14        2\n 4     4     3        1\n 5     5    10        1\n 6     6    18        3\n 7     7    22        3\n 8     8    11        1\n 9     9     5        1\n10    10    NA       NA\n\n\n👆Here’s what we did above:\n\nBecause case_when() is lazy, we were able to omit the age &gt;= 12 condition from the second two-sided formula. It’s unnecessary because the value 1 is immediately returned for every person with an age value less than 12. By definition, any value being evaluated in the second two-sided function (age &lt; 18) has an age value greater than or equal to 12.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Conditional Operations</span>"
    ]
  },
  {
    "objectID": "chapters/conditional_operations/conditional_operations.html#recode-missing",
    "href": "chapters/conditional_operations/conditional_operations.html#recode-missing",
    "title": "30  Conditional Operations",
    "section": "30.6 Recode missing",
    "text": "30.6 Recode missing\nWe’ve already talked about how R uses the special NA value to represent missing data. We’ve also learned how to convert other representations of missing data (e.g., “.”) to NA when we are importing data. However, It is extremely common for data sets that we use in epidemiology to include “don’t know” and “refused” answer options in addition to true “missing”. By convention, those options are often coded as 7 and 9. For questions that include 7 or more response options (e.g., month), then 77 and 99 are commonly used to represent “don’t know” and “refused”. For questions that include 77 or more response options (e.g., age), then 777 and 999 are commonly used to represent “don’t know” and “refused”.\nDifferentiating between true missing (i.e., the respondent was never asked the question or just left the response blank on a written questionnaire), don’t know (i.e., the respondent doesn’t know the answer), and refused (i.e., the respondent knows the answer, but doesn’t want to reveille it – possibly out of shame, fear, or embarrassment) can be of some interest for survey design purposes. However, all three of the values described above typically just amount to missing data by the time we get around to the substantive analyses. In other words, knowing that a person refused to give their age doesn’t help me figure out how old they are any more than if they had never been asked at all. Therefore, we commonly use conditional operations in epidemiology to recode these kinds of values to explicitly missing values (NA).\nWe’ll walk through an example below, but first we will simulate some additional data. Specifically, we’ll add a race column and a hispanic column to our ages data frame, and name the new data frame demographics.\nLet’s assume that we have a survey that asks people what race they most identify with. For the moment, let’s assume that they can only select one race. Further, let’s say that the options they are given to select from are:\n1 = White\n2 = Black or African American\n3 = American Indian or Alaskan Native\n4 = Asian\n5 = Pacific Islander\n7 = Don’t know\n9 = Refused\nLet’s say that we also ask if they self-identify their ethnicity as Hispanic or not. The options they are given to select from are:\n0 = No, not Hispanic\n1 = Yes, Hispanic\n7 = Don’t know\n9 = Refused\n\ndemographics &lt;- ages %&gt;% \n  mutate(\n    race     = c(1, 2, 1, 4, 7, 1, 2, 9, 1, 3),\n    hispanic = c(7, 0, 1, 0, 1, 0, 1, 9, 0, 1)\n  ) %&gt;% \n  print()\n\n# A tibble: 10 × 4\n      id   age  race hispanic\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     1    15     1        7\n 2     2    19     2        0\n 3     3    14     1        1\n 4     4     3     4        0\n 5     5    10     7        1\n 6     6    18     1        0\n 7     7    22     2        1\n 8     8    11     9        9\n 9     9     5     1        0\n10    10    NA     3        1\n\n\nA very common way that we may want to transform data like this is to collapse race and ethnicity into as single combined race and ethnicity column. Further, notice that American Indian or Alaskan Native race and Asian race are only observed once each, and Pacific Islander race is not observed at all. When values are observed very few times in the data like this, it is common to collapse them into an “other” category. Therefore, our new combined race and ethnicity column will have the following possible values:\n1 = White, non-Hispanic\n2 = Black, non-Hispanic\n3 = Hispanic, any race\n4 = Other race, non-Hispanic\nThere are multiple ways that we can create this new column. We could start by using if_else() to recode 7 and 9 to missing:\n\ndemographics %&gt;% \n  mutate(\n    # Recode 7 and 9 to missing\n    race_recode = if_else(race == 7 | race == 9, NA, race),\n    hispanic_recode = if_else(hispanic == 7 | hispanic == 9, NA, hispanic)\n  )\n\n# A tibble: 10 × 6\n      id   age  race hispanic race_recode hispanic_recode\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1     1    15     1        7           1              NA\n 2     2    19     2        0           2               0\n 3     3    14     1        1           1               1\n 4     4     3     4        0           4               0\n 5     5    10     7        1          NA               1\n 6     6    18     1        0           1               0\n 7     7    22     2        1           2               1\n 8     8    11     9        9          NA              NA\n 9     9     5     1        0           1               0\n10    10    NA     3        1           3               1\n\n\nWe intentionally made this error because it’s a really easy one to make, and you will probably make it too. If we look back to the Let’s get programming chapter, we will see that we briefly discussed the NA value being type logical by default. We also talked about “type coercion” and how most of the time we don’t have to worry about it. We said that R generally coerces NA to NA_character, NA_double, or whatever specific version of NA is most appropriate for the data type automatically. We also said that there are some exceptions. Notably, when using the if_else() and case_when() functions, R will throw an error instead of automatically type coercing. Finally, we said we would discuss it later. It’s later now. Long story short, the developers of the if_else() function do this on purpose to make the function’s returned result more predictable and slightly faster.\nFor us, this just means that we have to remember to use NA_character, NA_integer, or NA_real as appropriate. For example, the error message above says, “false must be a logical vector, not a double vector.” This means that the value we passed to the false argument was type double, but if_else() was expecting it to be type logical. Why? Well, if_else() was expecting it to be type logical because the value we passed to the true argument (NA) is type logical, and vectors can only ever have one type. To fix this error, we simply need to change the value we are passing to the true argument from logical (NA) to double (NA_real) so that it matches the values we are passing to the false argument.\nLet’s try again using NA_real instead of NA.\n\ndemographics %&gt;% \n  mutate(\n    # Recode 7 and 9 to missing\n    race_recode = if_else(race == 7 | race == 9, NA_real_, race),\n    hispanic_recode = if_else(hispanic == 7 | hispanic == 9, NA_real_, hispanic)\n  )\n\n# A tibble: 10 × 6\n      id   age  race hispanic race_recode hispanic_recode\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1     1    15     1        7           1              NA\n 2     2    19     2        0           2               0\n 3     3    14     1        1           1               1\n 4     4     3     4        0           4               0\n 5     5    10     7        1          NA               1\n 6     6    18     1        0           1               0\n 7     7    22     2        1           2               1\n 8     8    11     9        9          NA              NA\n 9     9     5     1        0           1               0\n10    10    NA     3        1           3               1\n\n\nGreat! We can move on with creating our new race and ethnicity column now that we’ve explicitly transformed 7’s and 9’s to NA. There’s nothing “new” in the code below, so we’re not going to explain it line-by-line. However, it’s a little bit dense, so we recommend that you take a few minutes to review it thoroughly and make sure you understand what each line is doing.\n\ndemographics %&gt;% \n  mutate(\n    # Recode 7 and 9 to missing\n    race_recode     = if_else(race == 7 | race == 9, NA_real_, race),\n    hispanic_recode = if_else(hispanic == 7 | hispanic == 9, NA_real_, hispanic),\n    race_eth_4cat   = case_when(\n      # White, non-Hispanic\n      race_recode == 1 & hispanic_recode == 0 ~ 1,\n      # Black, non-Hispanic\n      race_recode == 2 & hispanic_recode == 0 ~ 2,\n      # American Indian or Alaskan Native to Other race, non-Hispanic\n      race_recode == 3 & hispanic_recode == 0 ~ 4,\n      # Asian to Other race, non-Hispanic\n      race_recode == 4 & hispanic_recode == 0 ~ 4,\n      # Pacific Islander to Other race, non-Hispanic\n      race_recode == 4 & hispanic_recode == 0 ~ 4,\n      # Hispanic, any race\n      hispanic_recode == 1                    ~ 3\n    )\n  )\n\n# A tibble: 10 × 7\n      id   age  race hispanic race_recode hispanic_recode race_eth_4cat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1     1    15     1        7           1              NA            NA\n 2     2    19     2        0           2               0             2\n 3     3    14     1        1           1               1             3\n 4     4     3     4        0           4               0             4\n 5     5    10     7        1          NA               1             3\n 6     6    18     1        0           1               0             1\n 7     7    22     2        1           2               1             3\n 8     8    11     9        9          NA              NA            NA\n 9     9     5     1        0           1               0             1\n10    10    NA     3        1           3               1             3\n\n\nThe code above works, and it is very explicit. However, we can definitely make it more succinct and easier to read. For example:\n\ndemographics %&gt;% \n  mutate(\n    race_eth_4cat = case_when(\n      is.na(hispanic) | hispanic %in% c(7, 9) ~ NA_real_, # Unknown ethnicity\n      hispanic == 1                           ~ 3,        # Hispanic, any race\n      is.na(race) | race %in% c(7, 9)         ~ NA_real_, # non-Hispanic, unknown race\n      race == 1                               ~ 1,        # White, non-Hispanic\n      race == 2                               ~ 2,        # Black, non-Hispanic\n      TRUE                                    ~ 4         # Other race, non-Hispanic\n    )\n  )\n\n# A tibble: 10 × 5\n      id   age  race hispanic race_eth_4cat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n 1     1    15     1        7            NA\n 2     2    19     2        0             2\n 3     3    14     1        1             3\n 4     4     3     4        0             4\n 5     5    10     7        1             3\n 6     6    18     1        0             1\n 7     7    22     2        1             3\n 8     8    11     9        9            NA\n 9     9     5     1        0             1\n10    10    NA     3        1             3\n\n\n👆Here’s what we did above:\n\nWe used dplyr’s case_when() function to create a new column in our data frame (race_eth_4cat) that categorized each participant into one of 4 race and ethnicity categories depending on their values in the race column and the hispanic column.\nCompared to the first method we used, the second method doesn’t explicitly create new race and hispanic columns with the 7’s and 9’s recoded to NA. In the second method, those columns aren’t needed.\nThe very first two-sided formula tells case_when() to set the value of race_eth_4cat to NA_real_ when the value of hispanic is missing.\n\nWe put this two-sided formula first because if we don’t know a person’s Hispanic ethnicity, then we can’t put them into any category of race_eth_4cat. All categories of race_eth_4cat are dependent on a known value for hispanic. For example, look at participant number 1. They reported being white, but they don’t give their ethnicity. Which category do we put them in? We can’t put them in White, non-Hispanic because they very well could be Hispanic. We can’t put them in Hispanic, any race because they very well could be non-Hispanic. We don’t know. We never will. We code them as missing and don’t evaluate any further. And because case_when() is lazy, any other participants with a missing value for hispanic would also only have this first two-sided formula evaluated.\nThere were no actual NA values in the hispanic column, but we put it in the code for completeness. There will be some true missing (NA) values in most real-world data sets.\nNotice that we finally used the %in% operator above (hispanic %in% c(7, 9)). This is equivalent to typing hispanic == 7 | hispanic == 9. Notice that’s an OR. In this case, it doesn’t save us a ton of typing and visual clutter, but in many cases it can.\n\nThe second two-sided formula tells case_when() to set the value of race_eth_4cat to 3 (i.e., Hispanic any-race) when the value of hispanic is 1. Why did we put this second? If we know that someone is Hispanic, does it matter what race they reported? Nope! No matter what race they reported (even missing race) they get coded as Hispanic, any race. And because case_when() is lazy, putting this two-sided formula second has two advantages:\n\nAny other participants with a value of 1 for hispanic would only have the first two two-sided formulas evaluated. In other words, for each Hispanic participant, R would only evaluate 2 two-sided formulas instead of the 6 we used in the first method. With only 10 participants in the data, we won’t notice any performance improvement. But, this performance improvement can add up when we have thousands or millions of rows.\nIt allows us to remove the hispanic == 0 from the remaining two-sided formulas. Think about it. All participants with a missing value for hispanic were accounted for in the first two-sided formula. All participants with a 1 for hispanic were accounted for in the second two-sided formula. By definition, any participant left in the data must have a value of 0 for hispanic. There’s no need to write that code and there’s no need for R to evaluate that condition. Less typing for us and further performance improvements to boot.\n\nThe third two-sided formula tells case_when() to set the value of race_eth_4cat to NA_real_ when the value of race is missing. At this point in the code, there are no participants left with a value of 1 for hispanic. Therefore, if they are missing a value for race we won’t be able to assign them a value for race_eth_4cat. We code them as missing and don’t evaluate any further.\nThe fourth and fifth two-sided formulas tell case_when() to set the value of race_eth_4cat to 1 and 2 respectively when the value of race is 1 and 2.\nThe final two-sided formula is simply TRUE ~ 4. This tells case_when() to set the value of race_eth_4cat to 4 when none of the other two-sided formulas above evaluated to TRUE. Why did we do this? Well, every participant with missing data has been accounted for, every Hispanic participant has been accounted for, every White, non-Hispanic participant has been accounted for, and every Black, non-Hispanic participant has been accounted for. Because case_when() is lazy, we know that any participant that makes it to this part of the code must fall into the Other race, non-Hispanic category.\n\nNotice that there is nothing at all about race or hispanic in this two-sided formula. It just says TRUE. What does case_when() do when a condition on the left-hand side evaluates to TRUE? It returns the value on the right-hand side. In this case 4.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSometimes, adding an a final TRUE condition like the one above can be really useful. However, we have to be really careful. We can easily get unintended results if we aren’t absolutely sure that we’ve already accounted for every possible combination of relevant conditions in the two-sided formulas that come before.\n\n\nLet’s go ahead and wrap up this chapter with one consolidated code chunk that cleans our demographics data:\n\ndemographics %&gt;% \n  # Recode variables\n  mutate(\n    # Collapse continuous age into 3 categories\n    age_3cat = case_when(\n      age &lt; 12  ~ 1, # child\n      age &lt; 18  ~ 2, # adolescent\n      age &gt;= 18 ~ 3  # adult\n    ),\n    age_3cat_f = factor(\n      age_3cat, \n      labels = c(\"child\", \"adolescent\", \"adult\")\n    ),\n    # Combine race and ethnicity\n    race_eth_4cat = case_when(\n      is.na(hispanic) | hispanic %in% c(7, 9) ~ NA_real_, # Unknown ethnicity\n      hispanic == 1                           ~ 3,        # Hispanic, any race\n      is.na(race) | race %in% c(7, 9)         ~ NA_real_, # non-Hispanic, unknown race\n      race == 1                               ~ 1,        # White, non-Hispanic\n      race == 2                               ~ 2,        # Black, non-Hispanic\n      TRUE                                    ~ 4         # Other race, non-Hispanic\n    ),\n    race_eth_4cat_f = factor(\n      race_eth_4cat,\n      labels = c(\n        \"White, non-Hispanic\", \"Black, non-Hispanic\", \"Hispanic, any race\",\n        \"Other race, non-Hispanic\"\n      )\n    )\n  )\n\n# A tibble: 10 × 8\n      id   age  race hispanic age_3cat age_3cat_f race_eth_4cat race_eth_4cat_f \n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt; &lt;fct&gt;           \n 1     1    15     1        7        2 adolescent            NA &lt;NA&gt;            \n 2     2    19     2        0        3 adult                  2 Black, non-Hisp…\n 3     3    14     1        1        2 adolescent             3 Hispanic, any r…\n 4     4     3     4        0        1 child                  4 Other race, non…\n 5     5    10     7        1        1 child                  3 Hispanic, any r…\n 6     6    18     1        0        3 adult                  1 White, non-Hisp…\n 7     7    22     2        1        3 adult                  3 Hispanic, any r…\n 8     8    11     9        9        1 child                 NA &lt;NA&gt;            \n 9     9     5     1        0        1 child                  1 White, non-Hisp…\n10    10    NA     3        1       NA &lt;NA&gt;                   3 Hispanic, any r…\n\n\nNow that we’ve mastered conditional operations, we can use them to help us navigate another common data collection technique in epidemiology – skip patterns.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Conditional Operations</span>"
    ]
  },
  {
    "objectID": "chapters/multiple_data_frames/multiple_data_frames.html",
    "href": "chapters/multiple_data_frames/multiple_data_frames.html",
    "title": "31  Working with Multiple Data Frames",
    "section": "",
    "text": "31.1 Combining data frames vertically: Adding rows\nUp to this point, the data we’ve needed has always been stored in a single data frame. However, that won’t always be the case. At times we may need to combine data from multiple agencies in order to complete your analysis.\nAdditionally, large studies often gather data at multiple sites.\nOr, data is sometimes gathered over long periods of time. When this happens, it is not uncommon for observations across the study sites or times to be stored as separate data sets.\nAnother common scenario in which you end up with multiple data sets for the same study is when researchers use different data sets to record the results of different survey instruments or groups of similar instruments.\nIn any of these cases, you may need to combine data from across data sets in order to complete your analysis.\nThis combining of data comes in two basic forms: combining vertically and combining horizontally. First we’ll learn about combining vertically, or adding rows. Later, we’ll learn about combining horizontally, or adding columns.\nBelow we have two separate data frames - data frame one and data frame two. In this case both data frames contain the exact same variables: Var1, Var2, and Var3. However, they aren’t identical because they contain different observations.\nNow, you want to combine these two data frames and end up with one data frame that includes the observations from data frame two listed directly below the observations from data frame one. This is a situation where we want to combine data frames vertically.\nWhen combining data frames vertically, one of the most important questions to ask is, “do the data frames have variables in common?” Just by examining data frame one and data frame two, you can see that the variables have the same names. How can you check to make sure that the variables also contain the same type of data? Well, you can use the str() or glimpse() functions to compare the details of the columns in the two data frames.\nSometimes, you might find that columns that have different names across data frames contain the same data. For example, suppose that data frame one has a variable named ID and data frame two has a variable named subject ID. In this situation you might want R to combine these two variables when you combine data frames.\nOn the other hand, you may find that variables that have the same name across data frames, actually contain different data. For example, both data frames may contain the variable date. But, one date variable might store birth date and the other might store date of admission. You would not want to combine these two variables.\nAs you may have guessed, when combining data frames vertically, it’s easiest to combine data frames that have identical variables. However, you will also learn how to combine data frames that have different variables.\nSuppose you are working on a multisite clinical trial recruiting participants over multiple years. You have a data frame named Trial, that stores the number of participants recruited each year, as well as the number of participants who experienced the outcome of interest. Another data frame named Trial_2020 was just sent to you with the recruitment numbers for the year 2020.\nYou want to add the observations about the participants recruited in 2020 to the master data frame so that it contains the information about all years. To do this, you bind the rows in the trial_2020 data frame to the trial data frame.\nLet’s go ahead and load dplyr:\nlibrary(dplyr)\nAnd simulate our data frames:\ntrial &lt;- tibble(\n  year    = c(2016, 2017, 2018, 2019),\n  n       = c(501, 499, 498, 502),\n  outcome = c(51, 52, 49, 50) \n) %&gt;% \n  print()\n\n# A tibble: 4 × 3\n   year     n outcome\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  2016   501      51\n2  2017   499      52\n3  2018   498      49\n4  2019   502      50\ntrial_2020 &lt;- tibble(\n  year    = 2020,\n  n       = 500,\n  outcome = 48 \n) %&gt;% \n  print()\n\n# A tibble: 1 × 3\n   year     n outcome\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  2020   500      48\nwe can see above that column names and types in both data frames are identical. In this case, we can easily bind them together vertically with dplyr’s bind_rows() function:\ntrial %&gt;% \n  bind_rows(trial_2020)\n\n# A tibble: 5 × 3\n   year     n outcome\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  2016   501      51\n2  2017   499      52\n3  2018   498      49\n4  2019   502      50\n5  2020   500      48\n👆Here’s what we did above:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Working with Multiple Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/multiple_data_frames/multiple_data_frames.html#combining-data-frames-vertically-adding-rows",
    "href": "chapters/multiple_data_frames/multiple_data_frames.html#combining-data-frames-vertically-adding-rows",
    "title": "31  Working with Multiple Data Frames",
    "section": "",
    "text": "we used dplyr’s bind_rows() function to vertically stack, or bind, the rows in trial_2020 to the rows in trials.\nYou can type ?bind_rows into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the bind_rows() function is the ... argument. Typically, we will pass one or more data frames that we want to combine to the ... argument.\n\n\n31.1.1 Combining more than 2 data frames\nWhat if we want to vertically combine more than two data frames? This isn’t a problem. Thankfully, bind_rows() lets us pass as many data frames as we want to the ... argument. For example:\n\ntrial_2021 &lt;- tibble(\n  year      = 2021,\n  n         = 598,\n  outcome   = 57\n) %&gt;% \n  print()\n\n# A tibble: 1 × 3\n   year     n outcome\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  2021   598      57\n\n\n\ntrial %&gt;% \n  bind_rows(trial_2020, trial_2021)\n\n# A tibble: 6 × 3\n   year     n outcome\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  2016   501      51\n2  2017   499      52\n3  2018   498      49\n4  2019   502      50\n5  2020   500      48\n6  2021   598      57\n\n\n\n\n31.1.2 Adding rows with differing columns\nWhat happens when the data frames we want to combine don’t have identical sets of columns? For example, let’s say that we started collecting data on adverse events for the first time in 2020. In this case, trials_2020 would contain a column that trials doesn’t contain. Can we still row bind our two data frames? Let’s see:\n\ntrial_2020 &lt;- tibble(\n  year      = 2020,\n  n         = 500,\n  outcome   = 48,\n  adv_event = 3 # Here is the new column\n) %&gt;% \n  print()\n\n# A tibble: 1 × 4\n   year     n outcome adv_event\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1  2020   500      48         3\n\n\n\ntrial %&gt;% \n  bind_rows(trial_2020)\n\n# A tibble: 5 × 4\n   year     n outcome adv_event\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1  2016   501      51        NA\n2  2017   499      52        NA\n3  2018   498      49        NA\n4  2019   502      50        NA\n5  2020   500      48         3\n\n\nwe sure can! R just sets the value of adv_event to NA in the rows that came from the trial data frame.\n\n\n31.1.3 Differing column positions\nNext, let’s say that the person doing data entry accidently put the columns in a different order in 2020. Is bind_rows() able to figure out which columns go together?\n\ntrial_2020 &lt;- tibble(\n  year      = 2020,\n  n         = 500,\n  adv_event = 3, # This was previously the fourth column\n  outcome   = 48 # This is the thrid column in trial\n) %&gt;% \n  print()\n\n# A tibble: 1 × 4\n   year     n adv_event outcome\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1  2020   500         3      48\n\n\n\ntrial %&gt;% \n  bind_rows(trial_2020)\n\n# A tibble: 5 × 4\n   year     n outcome adv_event\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1  2016   501      51        NA\n2  2017   499      52        NA\n3  2018   498      49        NA\n4  2019   502      50        NA\n5  2020   500      48         3\n\n\nYes! The bind_rows() function binds the data frames together based on column names. So, having our columns in a different order in the two data frames isn’t a problem. But, what happens when we have different column names?\n\n\n31.1.4 Differing column names\nAs a final wrinkle, let’s say that the person doing data entry started using different column names in 2020 as well. For example, below, the n column is now named count and the outcome column is now named outcomes. Will bind_rows() still be able to vertically combine these data frames?\n\ntrial_2020 &lt;- tibble(\n  year      = 2020,\n  count     = 500,\n  adv_event = 3,\n  outcomes  = 48\n) %&gt;% \n  print()\n\n# A tibble: 1 × 4\n   year count adv_event outcomes\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1  2020   500         3       48\n\n\n\ntrial %&gt;% \n  bind_rows(trial_2020)\n\n# A tibble: 5 × 6\n   year     n outcome count adv_event outcomes\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1  2016   501      51    NA        NA       NA\n2  2017   499      52    NA        NA       NA\n3  2018   498      49    NA        NA       NA\n4  2019   502      50    NA        NA       NA\n5  2020    NA      NA   500         3       48\n\n\nIn this case, bind_rows() plays it safe and doesn’t make any assumptions about whether columns with different names belong together or not. However, we only need to rename the columns in one data frame or the other to fix this problem. We could do this in separate steps like this:\n\ntrial_2020_rename &lt;- trial_2020 %&gt;% \n  rename(\n    n = count,\n    outcome = outcomes\n  )\n\ntrial %&gt;% \n  bind_rows(trial_2020_rename)\n\n# A tibble: 5 × 4\n   year     n outcome adv_event\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1  2016   501      51        NA\n2  2017   499      52        NA\n3  2018   498      49        NA\n4  2019   502      50        NA\n5  2020   500      48         3\n\n\nOr, we could rename and bind in a single step by nesting functions like this:\n\ntrial %&gt;% \n  bind_rows(\n    trial_2020 %&gt;% \n      rename(\n        n = count,\n        outcome = outcomes\n      )\n  )\n\n# A tibble: 5 × 4\n   year     n outcome adv_event\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1  2016   501      51        NA\n2  2017   499      52        NA\n3  2018   498      49        NA\n4  2019   502      50        NA\n5  2020   500      48         3\n\n\n👆Here’s what we did above:\n\nwe nested the code that we previously used to create the trial_2020_rename data frame inside of the bind_rows() function instead creating the actual trial_2020_rename data frame and passing it to bind_rows().\nI don’t think you can really say that one method is “better” or “worse”. The first method requires two steps and creates a data frame in our global environment that we may or may not ever need again (i.e., potentially just clutter). However, one could make an argument that the first method is also easier to glance at and read. I would typically use the second method, but this is really just a personal preference in this case.\n\nAnd that’s pretty much it. The bind_rows() function makes it really easy to combine R data frames vertically. Next, let’s learn how to combine data frames horizontally.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Working with Multiple Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/multiple_data_frames/multiple_data_frames.html#combining-data-frames-horizontally-adding-columns",
    "href": "chapters/multiple_data_frames/multiple_data_frames.html#combining-data-frames-horizontally-adding-columns",
    "title": "31  Working with Multiple Data Frames",
    "section": "31.2 Combining data frames horizontally: Adding columns",
    "text": "31.2 Combining data frames horizontally: Adding columns\nIn this section we will once again begin with two separate data frames - data frame one and data frame two. But, unlike before, these data frames share only one variable in common. And, the data contained in both data frames pertains to the same observations.\n\n\n\n\n\n\n\n\n\nOur goal is once again to combine these data frames. But, this time we want to combine them horizontally. In other words, we want a combined data frame that combines all the columns from data frame one and data frame two.\n\n\n\n\n\n\n\n\n\nCombining data frames horizontally can be slightly more complicated than combining them vertically. As shown in the following flow chart, we can either match the rows of our two data frames up by position or by key values.\n\n\n\n\n\n\n\n\n\n\n31.2.1 Combining data frames horizontally by position\nIn the simplest case, we match the rows in our data frames up by position. In other words, row 1 in data frame one is matched up with row 1 in data frame two, row 2 in data frame one is matched up with row 2 in data frame two, and so on. Row n (meaning, any number) in data frame one always gets matched to row n in data frame two, regardless of the values in any column of those rows.\n\n\n\n\n\n\n\n\n\nCombining data frames horizontally by position is very easy in R. We just use dplyr’s bind_cols() function similarly to the way used bind_rows() above. Just remember that when we horizontally combine data frames by position both data frames must have the same number of rows. For example:\n\ndf1 &lt;- tibble(\n  color = c(\"red\", \"green\", \"blue\"),\n  size  = c(\"small\", \"medium\", \"large\")\n) %&gt;% \n  print()\n\n# A tibble: 3 × 2\n  color size  \n  &lt;chr&gt; &lt;chr&gt; \n1 red   small \n2 green medium\n3 blue  large \n\n\n\ndf2 &lt;- tibble(\n  amount = c(1, 4, 3),\n  dose   = c(10, 20, 30)\n) %&gt;% \n  print()\n\n# A tibble: 3 × 2\n  amount  dose\n   &lt;dbl&gt; &lt;dbl&gt;\n1      1    10\n2      4    20\n3      3    30\n\n\n\ndf1 %&gt;% \n  bind_cols(df2)\n\n# A tibble: 3 × 4\n  color size   amount  dose\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 red   small       1    10\n2 green medium      4    20\n3 blue  large       3    30\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s bind_cols() function to horizontally bind the columns in df1 to the columns in df2.\nYou can type ?bind_cols into your R console to view the help documentation for this function and follow along with the explanation below.\nThe only argument to the bind_cols() function is the ... argument. Typically, we will pass one or more data frames that we want to combine to the ... argument.\n\nIn general, it’s a bad idea to combine data frames that contain different kinds of information (i.e., variables) about the same set of people (or places or things) in this way. It’s difficult to ensure that the information in row n in both data frames is really about the same person (or place or thing). However, we do sometimes find bind_cols() to be useful when we’re writing our own functions in R. We haven’t quite learned how to do that yet, but we will soon.\n\n\n31.2.2 Combining data frames horizontally by key values\nIn all the examples from here on out we will match the rows of our data frames by one or more key values.\n\n\n\n\n\n\n\n\n\nIn epidemiology, the term I most often hear used for combining data frames in this way is merging. So, I will mostly use that term below. However, in other disciplines it is common to use the term joining, or performing a data join, to mean the same thing. The dplyr package, in specific, refers to these as “mutating joins.”\n\n31.2.2.1 Relationship types\nWhen we merge data frames it’s important to ask ourselves, “what is the relationship between the observations in the original data frames?” The observations can be related in several different ways.\nIn a one-to-one relationship, a single observation in one data frame is related to no more than one observation in the other data frame. We know how to align, or connect, the rows in the two data frames based on the values of one or more common variables.\n\n\n\n\n\n\n\n\n\nThis common variable, or set of common variables, is also called a key. When we use the values in the key to match rows in our data frames, we can say that we are matching on key values.\nIn the example above, There is one key column – Var1. Both data frames contain the column named Var1, and the values of that column tell R how to align the rows in both data frames so that all the values in that row contain data are about the same person, place, or thing. In the example above, we know that the first row of data frame one goes with the second row of data frame two because both rows have the same key value – 1.\nIn a one-to-many relationship, a single observation in one data frame is related to multiple observations in the other data frame.\n\n\n\n\n\n\n\n\n\nAnd finally, in a many-to-many relationship, multiple observations in one data frame are related to multiple observations in the other data frame.\n\n\n\n\n\n\n\n\n\nMany-to-many relationships are messy and are generally best avoided, if possible. In practice, we’re not sure that we’ve ever merged two data frames that had a true many-to-many relationship. We emphasize true because we have definitely merged data frames that had a many-to-many relationship when matching on a single key column. However, after matching on multiple key columns (e.g., study id and date instead of just study id), the relationship became one-to-one or one-to-many. We’ll see an example of matching on multiple key columns later.\n\n\n31.2.2.2 dplyr join types\nIn this chapter, we will merge data frames using one of dplyr’s four mutating join functions.\nThe first three arguments to all four of dplyr’s mutating join functions are: x, y, and by. You should pass the names of the data frames you want to merge to the x and y arguments respectively. You should pass the name(s) of the key column(s) to the by argument. In many cases, you will get a different merge result depending on which data frame you pass to the x and y arguments, and which mutating join function you use. Below, we will give you a brief overview of each of the mutating join functions, and then we will jump into some examples.\nThe four mutating join functions are:\n\nleft_join(). This is probably the join function that you will use the most. It’s important to remember that left_join() keeps all the rows from the x data frame in the resulting combined data frame. However, it only keeps the rows from the y data frame that have a key value match in the x data frame. The values for columns with no key value match in the opposite data frame are set to NA.\n\n\n\n\n\n\n\n\n\n\n\nright_join(). This is just the mirror opposite of left_join(). Accordingly, right_join() keeps all the rows from the y data frame in the resulting combined data frame, and only keep the rows from the x data frame that have a key value match in the y data frame. The values for columns with no key value match in the opposite data frame are set to NA.\n\n\n\n\n\n\n\n\n\n\n\nfull_join(). Full join keeps all the rows from both data frames in the resulting combined data frame. The values for columns with no key value match in the opposite data frame are set to NA.\n\n\n\n\n\n\n\n\n\n\n\ninner_join(). Inner join keeps only the rows from both data frames that have a key value match in the opposite data frame in the resulting combined data frame.\n\n\n\n\n\n\n\n\n\n\nNow that we have a common vocabulary, let’s take a look at some more concrete examples.\nSuppose we are analyzing data from a study of aging and functional ability. At baseline, we assigned a study id to each of our participants. We then ask them their date of birth and their race and ethnicity. We saved that information in a data frame called demographics.\n\ndemographics &lt;- tibble(\n  id       = c(\"1001\", \"1002\", \"1003\", \"1004\"),\n  dob      = as.Date(c(\"1968-12-14\", \"1952-08-03\", \"1949-05-27\", \"1955-03-12\")),\n  race_eth = c(1, 2, 2, 4)\n) %&gt;% \n  print()\n\n# A tibble: 4 × 3\n  id    dob        race_eth\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;\n1 1001  1968-12-14        1\n2 1002  1952-08-03        2\n3 1003  1949-05-27        2\n4 1004  1955-03-12        4\n\n\nThen, we asked our participants to do a series of functional tests. The functional tests included measuring grip strength in their right hand (grip_r) and grip strength in their left hand (grip_l). We saved each measure, along with their study id, in a separate data frame called grip_strength.\n\ngrip_strength &lt;- tibble(\n  id     = c(\"1002\", \"1001\", \"1003\", \"1004\"),\n  grip_r = c(32, 28, 32, 22),\n  grip_l = c(30, 30, 28, 22)\n) %&gt;% \n  print()\n\n# A tibble: 4 × 3\n  id    grip_r grip_l\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1002      32     30\n2 1001      28     30\n3 1003      32     28\n4 1004      22     22\n\n\nNow, we want to merge these two data frames together so that we can include age, race/ethnicity, and grip strength in our analysis.\nLet’s first ask ourselves, “what is the relationship between the observations in demographics and the observations in grip_strength?”\n\n\n31.2.2.3 One-to-one relationship merge\nIt’s a one-to-one relationship because each participant in demographics has no more than one corresponding row in grip_strength. Since both data frames have exactly four rows, we can go ahead hand combine them horizontally using bind_cols() like this:\n\ndemographics %&gt;% \n  bind_cols(grip_strength)\n\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...4`\n\n\n# A tibble: 4 × 6\n  id...1 dob        race_eth id...4 grip_r grip_l\n  &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 1001   1968-12-14        1 1002       32     30\n2 1002   1952-08-03        2 1001       28     30\n3 1003   1949-05-27        2 1003       32     28\n4 1004   1955-03-12        4 1004       22     22\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s bind_cols() function to horizontally bind the columns in demographics to the columns in grip_strength. This was a bad idea!\nNotice the message that bind_cols() gave us this time: New names: * id -&gt; id...1 * id -&gt; id...2. This is telling us that both data frames had a column named id. If bind_cols() had left the column names as-is, then the resulting combined data frame would have had two columns named id, which isn’t allowed.\nMore importantly, notice the demographic data for participant 1001 is now aligned with the grip strength data for participant 1002, and vice versa. The grip strength data was recorded in the order that participants came in to have their grip strength measured. In this case, participant 1002 came in before 1001. Remember that bind_cols() matches rows by position, which results in mismatched data in this case.\n\nNow, let’s learn a better way to merge these two data frames – dplyr’s left_join() function:\n\ndemographics %&gt;% \n  left_join(grip_strength, by = \"id\")\n\n# A tibble: 4 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n\n\n👆Here’s what we did above:\n\nwe used dplyr’s left_join() function to perform a one-to-one merge of the demographics data frame with the grip_strength data frame.\nYou can type ?left_join into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the left_join() function is the x argument. You should pass a data frame to the x argument.\nThe second argument to the left_join() function is the y argument. You should pass a data frame to the y argument.\nThe third argument to the left_join() function is the by argument. You should pass the name of the column, or columns, that contain the key values. The column name should be wrapped in quotes.\nNotice that the demographics and grip strength data are now correctly aligned for participants 1001 and 1002 even though they were still misaligned in the original data frames. That’s because row position is irrelevant when we match by key values.\nNotice that the result above only includes a single id column. This is because we aren’t simply smooshing two data frames together, side-by-side. We are integrating information from across the two data frames based on the value of the key column – id.\n\nThe merge we did above is about as simple as it gets. It was a one-to-one merge where every key value in the x data frame had one, and only one, matching key value in the y data frame. Therefore, in this simple case, all four join types give us the same result:\n\n# Right join\ndemographics %&gt;% \n  right_join(grip_strength, by = \"id\")\n\n# A tibble: 4 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n\n\n\n# Full join\ndemographics %&gt;% \n  full_join(grip_strength, by = \"id\")\n\n# A tibble: 4 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n\n\n\n# Inner join\ndemographics %&gt;% \n  inner_join(grip_strength, by = \"id\")\n\n# A tibble: 4 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n\n\nAdditionally, aside from the order of the rows and columns in the resulting combined data frame, it makes no difference which data frame you pass to the x and y arguments in this case:\n\n# Switching order\ngrip_strength %&gt;% \n  left_join(demographics, by = \"id\")\n\n# A tibble: 4 × 5\n  id    grip_r grip_l dob        race_eth\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;        &lt;dbl&gt;\n1 1002      32     30 1952-08-03        2\n2 1001      28     30 1968-12-14        1\n3 1003      32     28 1949-05-27        2\n4 1004      22     22 1955-03-12        4\n\n\nAs our merges get more complex, we will get different results depending on which join function we choose and the ordering in which we pass our data frames to the x and y arguments. We’re not going to attempt to cover every possible combination. But, we are going to try to give you a flavor for some of the scenarios we believe you are most likely to encounter in practice.\n\n\n31.2.2.4 Differing rows\nIn the real world, participants don’t always attend scheduled visits. Let’s suppose that there was actually a fifth participant that we collected baseline data from:\n\ndemographics &lt;- tibble(\n  id       = c(\"1001\", \"1002\", \"1003\", \"1004\", \"1005\"),\n  dob      = as.Date(c(\n    \"1968-12-14\", \"1952-08-03\", \"1949-05-27\", \"1955-03-12\", \"1942-06-07\"\n  )),\n  race_eth = c(1, 2, 2, 4, 3)\n) %&gt;% \n  print()\n\n# A tibble: 5 × 3\n  id    dob        race_eth\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;\n1 1001  1968-12-14        1\n2 1002  1952-08-03        2\n3 1003  1949-05-27        2\n4 1004  1955-03-12        4\n5 1005  1942-06-07        3\n\n\nHowever, participant 1005 never made it back in for a grip strength test. Now, what do you think will happen when we merge demographics and grip_strength using left_join()?\n\ndemographics %&gt;% \n  left_join(grip_strength, by = \"id\")\n\n# A tibble: 5 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n5 1005  1942-06-07        3     NA     NA\n\n\nThe resulting data frame includes all rows from the demographics data frame and all the rows from the grip_strength data frame. Because participant 1005 never had their grip strength measured, and therefore, had no rows in the grip_strength data frame, their values for grip_r and grip_l are set to missing.\nThis scenario is a little a different than the one above. It’s still a one-to-one relationship because each participant in demographics has no more than one corresponding row in grip_strength. However, every key value in the x data frame no longer has one, and only one, matching key value in the y data frame. Therefore, we will now get different results depending on which join function we choose, and the order in which we pass our data frames to the x and y arguments. Before reading further, think about what you expect the results from each join function to look like. Think about what you expect the results of switching the data frame order to look like.\n\n# Right join\ndemographics %&gt;% \n  right_join(grip_strength, by = \"id\")\n\n# A tibble: 4 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n\n\n\n# Full join\ndemographics %&gt;% \n  full_join(grip_strength, by = \"id\")\n\n# A tibble: 5 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n5 1005  1942-06-07        3     NA     NA\n\n\n\n# Inner join\ndemographics %&gt;% \n  inner_join(grip_strength, by = \"id\")\n\n# A tibble: 4 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n\n\n\n# Switching order\ngrip_strength %&gt;% \n  left_join(demographics, by = \"id\")\n\n# A tibble: 4 × 5\n  id    grip_r grip_l dob        race_eth\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;        &lt;dbl&gt;\n1 1002      32     30 1952-08-03        2\n2 1001      28     30 1968-12-14        1\n3 1003      32     28 1949-05-27        2\n4 1004      22     22 1955-03-12        4\n\n\nWell, were those the results you expected? In practice, the “correct” result depends on what we are trying to do. In the scenario above, we would probably tend to want the result from left_join() or full_join() in most cases. The reason is that it’s much harder to add data into our analysis that never made it into our combined data frame than it is to drop rows from our results data frame that we don’t need for our analysis.\n\n\n31.2.2.5 Differing key column names\nSometimes the key columns will have different names across data frames. For example, let’s imagine that the team collecting the grip strength data named the participant id column pid instead of id:\n\ngrip_strength &lt;- tibble(\n  pid    = c(\"1002\", \"1001\", \"1003\", \"1004\"),\n  grip_r = c(32, 28, 32, 22),\n  grip_l = c(30, 30, 28, 22)\n) %&gt;% \n  print()\n\n# A tibble: 4 × 3\n  pid   grip_r grip_l\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1002      32     30\n2 1001      28     30\n3 1003      32     28\n4 1004      22     22\n\n\nIf we try to merge demographics and grip_strength as we did before, we will get an error.\n\ndemographics %&gt;% \n  left_join(grip_strength, by = \"id\")\n\nError in `left_join()`:\n! Join columns in `y` must be present in the data.\n✖ Problem with `id`.\n\n\nThis error is left_join() telling us that it couldn’t find a column named id in both data frames. To get around this error, we can simply tell left_join() which column is the matching key column in the opposite data frame using a named vector like this:\n\ndemographics %&gt;% \n  left_join(grip_strength, by = c(\"id\" = \"pid\"))\n\n# A tibble: 5 × 5\n  id    dob        race_eth grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1     28     30\n2 1002  1952-08-03        2     32     30\n3 1003  1949-05-27        2     32     28\n4 1004  1955-03-12        4     22     22\n5 1005  1942-06-07        3     NA     NA\n\n\nJust make sure that the first column name you pass to the named vector (i.e., \"id\") is the name of the key column in the x data frame and that the second column name you pass to the named vector (i.e., \"pid\") is the name of the key column in the y data frame.\n\n\n31.2.2.6 One-to-many relationship merge\nNow suppose that our grip strength study has a longitudinal design. The demographics data was only collected at enrollment into the study. After all, race and dob don’t change. There’s no need to ask our participants about them at every follow-up interview.\n\ndemographics\n\n# A tibble: 5 × 3\n  id    dob        race_eth\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt;\n1 1001  1968-12-14        1\n2 1002  1952-08-03        2\n3 1003  1949-05-27        2\n4 1004  1955-03-12        4\n5 1005  1942-06-07        3\n\n\nGrip strength, however, was measured pre and post some intervention.\n\ngrip_strength &lt;- tibble(\n  id     = rep(c(\"1001\", \"1002\", \"1003\", \"1004\"), each = 2),\n  visit  = rep(c(\"pre\", \"post\"), 4),\n  grip_r = c(32, 33, 28, 27, 32, 34, 22, 27),\n  grip_l = c(30, 32, 30, 30, 28, 30, 22, 26)\n) %&gt;% \n  print()\n\n# A tibble: 8 × 4\n  id    visit grip_r grip_l\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  pre       32     30\n2 1001  post      33     32\n3 1002  pre       28     30\n4 1002  post      27     30\n5 1003  pre       32     28\n6 1003  post      34     30\n7 1004  pre       22     22\n8 1004  post      27     26\n\n\nNow what is the relationship of these two data frames?\nThese data frames have a one-to-many relationship because at least one observation in one data frame is related to multiple observations in the other data frame. The demographics data frame has one observation for each value of id. The grip_strength data frame has two observations for each value of the id’s 1001 through 1004.\nNow, to conduct our analysis, we need to combine the data in demographics with the data in the longitudinal grip_strength data frame. And how will we ask R to merge these two data frames? Well, here is some good news. To perform a one-to-many or many-to-many merge, we use the exact same syntax that we used to perform a one-to-one merge. R will figure out the relationship between the data frames automatically. Take a look:\n\ndemographics %&gt;% \n  left_join(grip_strength, by = \"id\")\n\n# A tibble: 9 × 6\n  id    dob        race_eth visit grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1 pre       32     30\n2 1001  1968-12-14        1 post      33     32\n3 1002  1952-08-03        2 pre       28     30\n4 1002  1952-08-03        2 post      27     30\n5 1003  1949-05-27        2 pre       32     28\n6 1003  1949-05-27        2 post      34     30\n7 1004  1955-03-12        4 pre       22     22\n8 1004  1955-03-12        4 post      27     26\n9 1005  1942-06-07        3 &lt;NA&gt;      NA     NA\n\n\n\n\n31.2.2.7 Multiple key columns\nLet’s throw one more little wrinkle into our analysis. Let’s say that each participant had a medical exam prior to being sent into the gym to do their functional assessments. The results of that medical exam, along with the participant’s study id, were recorded in the university hospital system’s electronic medical records. As part of that medical exam, each participant’s weight was recorded. Luckily, we were given access to the electronic medical records, which look like this:\n\nemr &lt;- tibble(\n  id     = rep(c(\"1001\", \"1002\", \"1003\", \"1004\"), each = 2),\n  visit  = rep(c(\"pre\", \"post\"), 4),\n  weight = c(105, 99, 200, 201, 136, 133, 170, 175)\n) %&gt;% \n  print()\n\n# A tibble: 8 × 3\n  id    visit weight\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 1001  pre      105\n2 1001  post      99\n3 1002  pre      200\n4 1002  post     201\n5 1003  pre      136\n6 1003  post     133\n7 1004  pre      170\n8 1004  post     175\n\n\nNow, we would like to add participant weight to our analysis. Our first attempt might look something like this:\n\ndemographics %&gt;% \n  left_join(grip_strength, emr, by = \"id\")\n\n# A tibble: 9 × 6\n  id    dob        race_eth visit grip_r grip_l\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1 pre       32     30\n2 1001  1968-12-14        1 post      33     32\n3 1002  1952-08-03        2 pre       28     30\n4 1002  1952-08-03        2 post      27     30\n5 1003  1949-05-27        2 pre       32     28\n6 1003  1949-05-27        2 post      34     30\n7 1004  1955-03-12        4 pre       22     22\n8 1004  1955-03-12        4 post      27     26\n9 1005  1942-06-07        3 &lt;NA&gt;      NA     NA\n\n\nOf course, that doesn’t work because left_join() can only merge two data frames at a time – x and y. The emr data frame was ignored. Then we think, “hmmm, maybe we should try merging them sequentially.” In other words, merge demographics and grip_strength first. Then merge the combined demographics/grip_strength data frame with emr. So, our next attempt might look like this:\n\ndemographics %&gt;% \n  left_join(grip_strength, by = \"id\") %&gt;% \n  left_join(emr, by = \"id\")\n\nWarning in left_join(., emr, by = \"id\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 17 × 8\n   id    dob        race_eth visit.x grip_r grip_l visit.y weight\n   &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1 1001  1968-12-14        1 pre         32     30 pre        105\n 2 1001  1968-12-14        1 pre         32     30 post        99\n 3 1001  1968-12-14        1 post        33     32 pre        105\n 4 1001  1968-12-14        1 post        33     32 post        99\n 5 1002  1952-08-03        2 pre         28     30 pre        200\n 6 1002  1952-08-03        2 pre         28     30 post       201\n 7 1002  1952-08-03        2 post        27     30 pre        200\n 8 1002  1952-08-03        2 post        27     30 post       201\n 9 1003  1949-05-27        2 pre         32     28 pre        136\n10 1003  1949-05-27        2 pre         32     28 post       133\n11 1003  1949-05-27        2 post        34     30 pre        136\n12 1003  1949-05-27        2 post        34     30 post       133\n13 1004  1955-03-12        4 pre         22     22 pre        170\n14 1004  1955-03-12        4 pre         22     22 post       175\n15 1004  1955-03-12        4 post        27     26 pre        170\n16 1004  1955-03-12        4 post        27     26 post       175\n17 1005  1942-06-07        3 &lt;NA&gt;        NA     NA &lt;NA&gt;        NA\n\n\nBut, if you look closely, that isn’t what we want either. Each participant didn’t have four visits. They only had two. Here’s the problem. Each participant in the combined demographics/grip_strength data frame has two rows (i.e., one for pre and one for post). Each participant in the emr data frame also has two rows (i.e., one for pre and one for post). Above, we told left_join() to join by id. So, left_join() aligns all rows with matching key values – id’s.\nFor example, row one in the combined demographics/grip_strength data frame has the key value 1001. So, left_join() aligns row one in the combined demographics/grip_strength data frame with rows one and two in the emr data frame. Next, row two in the combined demographics/grip_strength data frame has the key value 1001. So, left_join() aligns row two in the combined demographics/grip_strength data frame with rows one and two in the emr data frame. This results in 2 * 2 = 4 rows for each id - a many-to-many merge.\n\n\n\n\n\n\n\n\n\nBut in reality, study id alone no longer uniquely identifies observations in our data. Now, observations are uniquely identified by study id and visit. For example, 1001 and pre are a unique observation, 1001 and post are a unique observation, 1002 and pre are a unique observation, and so on. We now have two key columns that identify unique observations. And once we give that information to left_join, the relationship between the data frames becomes a one-to-one relationship. In other words, each observation (defined by id and visit) in one data frame is related to no more than one observation (defined by id and visit) in the other data frame.\n\n\n\n\n\n\n\n\n\nHere is how we tell left_join() to merge our data frames by id and visit:\n\ndemographics %&gt;% \n  left_join(grip_strength, by = \"id\") %&gt;% \n  left_join(emr, by = c(\"id\", \"visit\"))\n\n# A tibble: 9 × 7\n  id    dob        race_eth visit grip_r grip_l weight\n  &lt;chr&gt; &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1001  1968-12-14        1 pre       32     30    105\n2 1001  1968-12-14        1 post      33     32     99\n3 1002  1952-08-03        2 pre       28     30    200\n4 1002  1952-08-03        2 post      27     30    201\n5 1003  1949-05-27        2 pre       32     28    136\n6 1003  1949-05-27        2 post      34     30    133\n7 1004  1955-03-12        4 pre       22     22    170\n8 1004  1955-03-12        4 post      27     26    175\n9 1005  1942-06-07        3 &lt;NA&gt;      NA     NA     NA\n\n\n👆Here’s what we did above:\n\nWe used dplyr’s left_join() function to perform a one-to-many merge of the demographics data frame with the grip_strength data frame. Then, we used left_join() again to perform a one-to-one merge of the combined demographics/grip_strength data frame with the emr data frame.\nWe told left_join() that it needed to match the values in the id key column and the values in the visit key column in order to align the rows in the combined demographics/grip_strength data frame with the emr data frame.\n\nWe now have a robust set of tools we can use to work with data that is stored in more than one data frame – a common occurrence in epidemiology!",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Working with Multiple Data Frames</span>"
    ]
  },
  {
    "objectID": "chapters/restructuring_data_frames/restructuring_data_frames.html",
    "href": "chapters/restructuring_data_frames/restructuring_data_frames.html",
    "title": "32  Restructuring Data frames",
    "section": "",
    "text": "32.1 The tidyr package\nwe’ve already seen data frames with a couple of different structures, but we haven’t explicitly discussed those structures yet. When we say structure, we basically mean the way the data is organized into columns and rows. Traditionally, data are described as being organized in one of two ways:\nIn some cases, only the person-level data structure will practically make sense. For example, the table below contains the sex, weight, length, head circumference, and abdominal circumference for eight newborn babies measured cross-sectionally (i.e., at one point in time) at birth.\nIn this table, each baby has one observation (row) and a separate column contains data for each measurement. Further, each measurement is only taken on one occasion. There really is no other structure that makes sense for this data.\nFor contrast, the next table below is also person-level data. It contains the weight in pounds for eight babies at ages 3 months, 6 months, 9 months, and 12 months.\nNotice that each baby still has one, and only one, row. This time, however, there are only 2 measurements – sex and weight. Sex is measured on one occasion, but weight is measured on four occasions, and a new column is created in the data frame for each subsequent measure of weight. So, although each baby has a single row in the data, they really have four observations (i.e., measurement occasions). Notice that this is the first time that we’ve explicitly drawn a distinction between a row and an observation. Further, unlike the first table we saw, this table could actually be structured in a different way.\nAn alternative, and often preferable, data structure for data with repeated measures is the person-period, or long, data structure. Below, we look at the baby weights again. In the interest of saving space, we’re only looking at the first two babies from the previous table of data.\nNotice that each baby in the person-period table has four rows – one for each weight measurement. Also notice that there is a new variable in the person-period data that explicitly records time (i.e., months).\nBelow, we can compare the person-level version of the baby weight data to the person-period version of the baby weight data. we are only including babies 1001 and 1002 in the interest of saving space. As you can see, given the same data, the person-level structure is wider (i.e., more columns) than the person-period data and the person-period structure is longer (i.e., more rows) than the person-level data. That’s why the two structures are sometimes referred to as wide and long respectively.\nOk, so this data can be structured in either a person-level or a person-period format, but which structure should we use?\nWell, in general, we are going to suggest that you use the person-period structure for the kind of longitudinal data we have above for the following reasons:\nNotice all the missing data in this format – even with only two babies. For example, baby 1001 had her first check-up at 36 days old. She was 9 lbs. Baby 1002, however, didn’t have her first checkup until she was 84 days old. So, baby 1002 has a missing value for weight_36. That pattern continues throughout the data. Now, just try to imagine what this would look like for tens, hundreds, or thousands of babies. It would be a mess! By contrast, the person-period version of this data is much more efficient. In fact, it looks almost identical to the first person-period version of this data:\nSo, does this mean that we should never organize our data frames in a person-level format? Of course not! There are going to be some occasions when there are advantages to organizing our data frames in a person-level format. For example:\nLuckily, we rarely have to choose one structure or the other in an absolute sense. The tidyr package generally makes it very easy for us to restructure (“reshape” is another commonly used term) our data frames from wide to long and back again. This allows us to organize our data in the manner that is best suited for the particular task at hand. Let’s go ahead and take a look at some examples.\nThe tools we will use for restructuring our data will primarily come from a package we haven’t used before in this book – tidyr. If you haven’t already done so, and you’d like to follow along, please install and load tidyr, dplyr, and ggplot2 now.\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Restructuring Data frames</span>"
    ]
  },
  {
    "objectID": "chapters/restructuring_data_frames/restructuring_data_frames.html#pivoting-longer",
    "href": "chapters/restructuring_data_frames/restructuring_data_frames.html#pivoting-longer",
    "title": "32  Restructuring Data frames",
    "section": "32.2 Pivoting longer",
    "text": "32.2 Pivoting longer\nIn epidemiology, it’s common for data that we analyze to be measured on multiple occasions. It’s also common for repeated measures data to be entered into a spreadsheet or database in such a way that each new measure is a new column. We saw an example of this above:\n\n\n\n\n\nBaby weights at 3, 6, 9, and 12 months\n\n\n\n\nwe already concluded that this data has a person-level (wide) structure. As discussed above, many techniques that we may want to use to analyze this data will require us to restructure it to a person-period format. Let’s go ahead and walk through a demonstration of how do that. We will start by simulating this data in R:\n\nbabies &lt;- tibble(\n  id       = 1001:1008,\n  sex      = c(\"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\", \"F\"),\n  weight_3  = c(9, 11, 17, 16, 11, 17, 16, 15),\n  weight_6  = c(13, 16, 20, 18, 15, 21, 17, 16),\n  weight_9  = c(16, 17, 23, 21, 16, 25, 19, 18),\n  weight_12 = c(17, 20, 24, 22, 18, 26, 21, 19)\n) %&gt;% \n  print()\n\n# A tibble: 8 × 6\n     id sex   weight_3 weight_6 weight_9 weight_12\n  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  1001 F            9       13       16        17\n2  1002 F           11       16       17        20\n3  1003 M           17       20       23        24\n4  1004 F           16       18       21        22\n5  1005 M           11       15       16        18\n6  1006 M           17       21       25        26\n7  1007 M           16       17       19        21\n8  1008 F           15       16       18        19\n\n\nNow, let’s use the pivot_longer() function to restructure the babies data frame to a person-period format:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols         = starts_with(\"weight\"),\n    names_to     = \"months\",\n    names_prefix = \"weight_\",\n    values_to    = \"weight\"\n  ) %&gt;% \n  print()\n\n# A tibble: 32 × 4\n      id sex   months weight\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1  1001 F     3           9\n 2  1001 F     6          13\n 3  1001 F     9          16\n 4  1001 F     12         17\n 5  1002 F     3          11\n 6  1002 F     6          16\n 7  1002 F     9          17\n 8  1002 F     12         20\n 9  1003 M     3          17\n10  1003 M     6          20\n# ℹ 22 more rows\n\n\n👆Here’s what we did above:\n\nwe used tidyr’s pivot_longer() function to restructure the babies data frame from person-level (wide) to person-period (long).\nYou can type ?pivot_longer into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the pivot_longer() function is the data argument. You should pass the name of the data frame you want to restructure to the data argument. Above, we passed the babies data frame to the data argument using a pipe operator.\nThe second argument to the pivot_longer() function is the cols argument. You should pass the name of the columns you want to make longer to the cols argument. Above, we passed the names of the four weight columns to the cols argument. The cols argument actually accepts tidy-select argument modifiers. We first discussed tidy-select argument modifiers in the chapter on subsetting data frames. In the example above, we used the starts_with() tidy-select modifier to simplify our code. Instead of passing each column name directly to the cols argument, we asked starts_with() to pass the name of any column that has a column name that starts with the word “weight” to the cols argument.\nThe third argument to the pivot_longer() function is the names_to argument. You should pass the names_to argument a character string or character vector that tells pivot_longer() what you want to name the column that will contain the previous column names that were pivoted. By default, the value passed to the names_to argument is \"name\". We passed the value \"months\" to the names_to argument. This tells pivot_longer() what to name the column that contains the names of the previous column names. If that seems really confusing, I’m with you. Unfortunately, we don’t currently know a better way to write it, but we will show you what the names_to argument does below.\nThe fourth argument to the pivot_longer() function is the names_prefix argument. You should pass the names_prefix argument a regular expression that tells pivot_longer() what to remove from the start of each of the previous column names that we pivoted. By default, the value passed to the names_prefix argument is NULL (i.e., it doesn’t remove anything). We passed the value \"weight_\" to the names_prefix argument. This tells pivot_longer() that we want to remove the character string “weight_” from the start of each of the previous column names that we pivoted. For example, removing “weight_” from “weight_3” results in the value “3”, removing “weight_” from “weight_6” results in the value “6”, and so on. Again, we will show you what the names_prefix argument does below.\nThe eighth argument (we left the 5th, 6th, and 7th arguments at their default values) to the pivot_longer() function is the values_to argument. You should pass the values_to argument a character string or character vector that tells pivot_longer() what you want to name the column that will contain the values from the columns that were pivoted. By default, the value passed to the values_to argument is \"value\". We passed the value \"weight\" to the values_to argument. This tells pivot_longer() what to name the column that contains values from the columns that were pivoted. we will demonstrate what the values_to argument does below as well.\n\n\n32.2.1 The names_to argument\nThe official help documentation for pivot_longer() says that the value passed to the names_to argument should be “a string specifying the name of the column to create from the data stored in the column names of data.” we don’t blame you if you feel like that’s a little bit difficult to wrap your head around. Let’s take a look at the result we get when we don’t adjust the value passed to the names_to argument:\n\nbabies %&gt;% \n  pivot_longer(\n    cols = starts_with(\"weight\")\n  )\n\n# A tibble: 32 × 4\n      id sex   name      value\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1  1001 F     weight_3      9\n 2  1001 F     weight_6     13\n 3  1001 F     weight_9     16\n 4  1001 F     weight_12    17\n 5  1002 F     weight_3     11\n 6  1002 F     weight_6     16\n 7  1002 F     weight_9     17\n 8  1002 F     weight_12    20\n 9  1003 M     weight_3     17\n10  1003 M     weight_6     20\n# ℹ 22 more rows\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, when we only pass a value to the cols argument, pivot_longer() creates a new column that contains the column names from the data frame passed to the data argument, that are being pivoted into long format. By default, pivot_longer() names that column name. However, that name isn’t very informative. We will go ahead and change the column name to “months” because we know that this column will eventually contain month values. We do so by passing the value \"months\" to the names_to argument like this:\n\nbabies %&gt;% \n  pivot_longer(\n    cols     = starts_with(\"weight\"),\n    names_to = \"months\"\n  )\n\n# A tibble: 32 × 4\n      id sex   months    value\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1  1001 F     weight_3      9\n 2  1001 F     weight_6     13\n 3  1001 F     weight_9     16\n 4  1001 F     weight_12    17\n 5  1002 F     weight_3     11\n 6  1002 F     weight_6     16\n 7  1002 F     weight_9     17\n 8  1002 F     weight_12    20\n 9  1003 M     weight_3     17\n10  1003 M     weight_6     20\n# ℹ 22 more rows\n\n\n\n\n32.2.2 The names_prefix argument\nThe official help documentation for pivot_longer() says that the value passed to the names_prefix argument should be “a regular expression used to remove matching text from the start of each variable name.” Passing a value to this argument can be really useful when column names actually contain data values, which was the case above. Take the column name “weight_3” for example. The “weight” part is truly a column name – it tells us what the values in that column are. They are weights. The “3” part is actually a separate data value meaning “3 months.” If we can remove the “weight_” part of the column name, then what remains is a useful column of information – time measured in months. Passing the value “weight_” to the names_prefix argument does exactly that.\n\nbabies %&gt;% \n  pivot_longer(\n    cols         = starts_with(\"weight\"),\n    names_to     = \"months\",\n    names_prefix = \"weight_\"\n  )\n\n# A tibble: 32 × 4\n      id sex   months value\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  1001 F     3          9\n 2  1001 F     6         13\n 3  1001 F     9         16\n 4  1001 F     12        17\n 5  1002 F     3         11\n 6  1002 F     6         16\n 7  1002 F     9         17\n 8  1002 F     12        20\n 9  1003 M     3         17\n10  1003 M     6         20\n# ℹ 22 more rows\n\n\nNow, the value passed to the names_prefix argument can be any regular expression. So, we could have written a more complicated, and flexible, regular expression like this:\n\nbabies %&gt;% \n  pivot_longer(\n    cols         = starts_with(\"weight\"),\n    names_to     = \"months\",\n    names_prefix = \"\\\\w+_\"\n  )\n\n# A tibble: 32 × 4\n      id sex   months value\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  1001 F     3          9\n 2  1001 F     6         13\n 3  1001 F     9         16\n 4  1001 F     12        17\n 5  1002 F     3         11\n 6  1002 F     6         16\n 7  1002 F     9         17\n 8  1002 F     12        20\n 9  1003 M     3         17\n10  1003 M     6         20\n# ℹ 22 more rows\n\n\nThe regular expression above would have removed any word characters followed by an underscore. However, in this case, the value \"weight_\" is straightforward and gets the job done.\n\n\n32.2.3 The values_to argument\nThe official help documentation for pivot_longer() says that the value passed to the values_to argument should be “a string specifying the name of the column to create from the data stored in cell values.” All that means is that we use this argument to name the column that contains the values that were pivoted.\n\n\n\n\n\n\n\n\n\nBy default, pivot_longer() names that column “value.” However, we will once again want a more informative column name in our new data frame. So, we’ll go ahead and change the column name to “weight” because that’s what the values in that column are – weights. We do so by passing the value \"weight\" to the values_to argument like this:\n\nbabies %&gt;% \n  pivot_longer(\n    cols         = starts_with(\"weight\"),\n    names_to     = \"months\",\n    names_prefix = \"weight_\",\n    values_to    = \"weight\"\n  )\n\n# A tibble: 32 × 4\n      id sex   months weight\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1  1001 F     3           9\n 2  1001 F     6          13\n 3  1001 F     9          16\n 4  1001 F     12         17\n 5  1002 F     3          11\n 6  1002 F     6          16\n 7  1002 F     9          17\n 8  1002 F     12         20\n 9  1003 M     3          17\n10  1003 M     6          20\n# ℹ 22 more rows\n\n\n\n\n32.2.4 The names_transform argument\nAs one little final touch on the data restructuring at hand, it would be nice to coerce the months column from type character to type integer. We already know how to do this with mutate():\n\nbabies %&gt;% \n  pivot_longer(\n    cols         = starts_with(\"weight\"),\n    names_to     = \"months\",\n    names_prefix = \"weight_\",\n    values_to    = \"weight\"\n  ) %&gt;% \n  mutate(months = as.integer(months))\n\n# A tibble: 32 × 4\n      id sex   months weight\n   &lt;int&gt; &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;\n 1  1001 F          3      9\n 2  1001 F          6     13\n 3  1001 F          9     16\n 4  1001 F         12     17\n 5  1002 F          3     11\n 6  1002 F          6     16\n 7  1002 F          9     17\n 8  1002 F         12     20\n 9  1003 M          3     17\n10  1003 M          6     20\n# ℹ 22 more rows\n\n\nHowever, we can also do this directly inside the pivot_longer() function by passing a list of column names paired with type coercion functions. For example:\n\nbabies %&gt;% \n  pivot_longer(\n    cols            = starts_with(\"weight\"),\n    names_to        = \"months\",\n    names_prefix    = \"weight_\",\n    names_transform = list(months = as.integer),\n    values_to       = \"weight\"\n  )\n\n# A tibble: 32 × 4\n      id sex   months weight\n   &lt;int&gt; &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;\n 1  1001 F          3      9\n 2  1001 F          6     13\n 3  1001 F          9     16\n 4  1001 F         12     17\n 5  1002 F          3     11\n 6  1002 F          6     16\n 7  1002 F          9     17\n 8  1002 F         12     20\n 9  1003 M          3     17\n10  1003 M          6     20\n# ℹ 22 more rows\n\n\n👆Here’s what we did above:\n\nwe coerced the months column from type character to type integer by passing the value list(months = as.integer) to the names_transform argument. The list passed to names_transform should contain one or more column names paired with a type coercion function. The column name and type coercion function should be paired using an equal sign. Multiple pairs should be separated by commas.\n\n\n\n32.2.5 Pivoting multiple sets of columns\nLet’s add a little layer of complexity to our situation. Let’s say that our babies data frame also includes each baby’s length in inches measured at each visit:\n\nset.seed(123)\nbabies &lt;- tibble(\n  id       = 1001:1008,\n  sex      = c(\"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\", \"F\"),\n  weight_3  = c(9, 11, 17, 16, 11, 17, 16, 15),\n  weight_6  = c(13, 16, 20, 18, 15, 21, 17, 16),\n  weight_9  = c(16, 17, 23, 21, 16, 25, 19, 18),\n  weight_12 = c(17, 20, 24, 22, 18, 26, 21, 19),\n  length_3  = c(17, 19, 23, 20, 18, 22, 21, 18),\n  length_6  = round(length_3 + rnorm(8, 2, 1)),\n  length_9  = round(length_6 + rnorm(8, 2, 1)),\n  length_12 = round(length_9 + rnorm(8, 2, 1)),\n) %&gt;% \n  print()\n\n# A tibble: 8 × 10\n     id sex   weight_3 weight_6 weight_9 weight_12 length_3 length_6 length_9\n  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1  1001 F            9       13       16        17       17       18       19\n2  1002 F           11       16       17        20       19       21       23\n3  1003 M           17       20       23        24       23       27       30\n4  1004 F           16       18       21        22       20       22       24\n5  1005 M           11       15       16        18       18       20       22\n6  1006 M           17       21       25        26       22       26       28\n7  1007 M           16       17       19        21       21       23       24\n8  1008 F           15       16       18        19       18       19       23\n# ℹ 1 more variable: length_12 &lt;dbl&gt;\n\n\nHere is what we want our final data frame to look like:\n\nbabies %&gt;% \n  pivot_longer(\n    cols      = c(-id, -sex),\n    names_to  = c(\".value\", \"months\"),\n    names_sep = \"_\"\n  )\n\n# A tibble: 32 × 5\n      id sex   months weight length\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  1001 F     3           9     17\n 2  1001 F     6          13     18\n 3  1001 F     9          16     19\n 4  1001 F     12         17     21\n 5  1002 F     3          11     19\n 6  1002 F     6          16     21\n 7  1002 F     9          17     23\n 8  1002 F     12         20     23\n 9  1003 M     3          17     23\n10  1003 M     6          20     27\n# ℹ 22 more rows\n\n\nNext, we’ll walk through getting to this result step-by-step.\nwe are once again starting with a person-level data frame, and we once again want to restructure it to a person-period data frame. This is the result we get if we use the same code we previously used to restructure the data frame that didn’t include each baby’s length:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols         = starts_with(\"weight\"),\n    names_to     = \"months\",\n    names_prefix = \"weight_\",\n    values_to    = \"weight\"\n  ) %&gt;% \n  print()\n\n# A tibble: 32 × 8\n      id sex   length_3 length_6 length_9 length_12 months weight\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1  1001 F           17       18       19        21 3           9\n 2  1001 F           17       18       19        21 6          13\n 3  1001 F           17       18       19        21 9          16\n 4  1001 F           17       18       19        21 12         17\n 5  1002 F           19       21       23        23 3          11\n 6  1002 F           19       21       23        23 6          16\n 7  1002 F           19       21       23        23 9          17\n 8  1002 F           19       21       23        23 12         20\n 9  1003 M           23       27       30        33 3          17\n10  1003 M           23       27       30        33 6          20\n# ℹ 22 more rows\n\n\nBecause we aren’t passing any of the length_ columns to the cols argument, pivot_longer() is treating them like the other time-invariant variables (i.e., id and sex). Their values are just being recycled across every row within each id. So, let’s add the length_ columns to the cols argument and see what happens:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols         = c(-id, -sex),\n    names_to     = \"months\",\n    names_prefix = \"weight_\",\n    values_to    = \"weight\"\n  ) %&gt;% \n  print()\n\n# A tibble: 64 × 4\n      id sex   months    weight\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1  1001 F     3              9\n 2  1001 F     6             13\n 3  1001 F     9             16\n 4  1001 F     12            17\n 5  1001 F     length_3      17\n 6  1001 F     length_6      18\n 7  1001 F     length_9      19\n 8  1001 F     length_12     21\n 9  1002 F     3             11\n10  1002 F     6             16\n# ℹ 54 more rows\n\n\n👆Here’s what we did above:\n\nwe passed the weight_ and length_ columns to the cols argument indirectly by passing the value c(-id, -sex). Basically, this tells pivot_longer() that we would like to pivot every column except id and sex.\n\nNow, we are pivoting both the weight_ columns and the length_ columns. That’s an improvement. However, we obviously still don’t have the result we want.\nRemember that the value passed to the names_prefix argument is used to remove matching text from the start of each variable name. Passing the value \"weight_\" to the names_prefix argument made sense when all of our pivoted columns began with the character sting “weight_”. Now, however, some of our pivoted columns begin with the character string “length_”. That’s why we are still seeing values in the months column like length_3, length_6, and so on.\nNow, your first instinct might be to just add \"length_\" to the names_prefix argument. Unfortunately, that doesn’t work:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols         = c(-id, -sex),\n    names_to     = \"months\",\n    names_prefix = c(\"weight_\", \"length_\"),\n    values_to    = \"weight\"\n  ) %&gt;% \n  print()\n\nWarning in gsub(vec_paste0(\"^\", names_prefix), \"\", cols): argument 'pattern'\nhas length &gt; 1 and only the first element will be used\n\n\n# A tibble: 64 × 4\n      id sex   months    weight\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1  1001 F     3              9\n 2  1001 F     6             13\n 3  1001 F     9             16\n 4  1001 F     12            17\n 5  1001 F     length_3      17\n 6  1001 F     length_6      18\n 7  1001 F     length_9      19\n 8  1001 F     length_12     21\n 9  1002 F     3             11\n10  1002 F     6             16\n# ℹ 54 more rows\n\n\nInstead, we need to drop the names_prefix argument altogether before we can move forward to the correct solution:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols      = c(-id, -sex),\n    names_to  = \"months\",\n    values_to = \"weight\"\n  ) %&gt;% \n  print()\n\n# A tibble: 64 × 4\n      id sex   months    weight\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1  1001 F     weight_3       9\n 2  1001 F     weight_6      13\n 3  1001 F     weight_9      16\n 4  1001 F     weight_12     17\n 5  1001 F     length_3      17\n 6  1001 F     length_6      18\n 7  1001 F     length_9      19\n 8  1001 F     length_12     21\n 9  1002 F     weight_3      11\n10  1002 F     weight_6      16\n# ℹ 54 more rows\n\n\nAdditionally, not all the values in the third column (i.e., weight) are weights. Half of those values are lengths. So, we also need to drop the values_to argument:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols     = c(-id, -sex),\n    names_to = \"months\"\n  ) %&gt;% \n  print()\n\n# A tibble: 64 × 4\n      id sex   months    value\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1  1001 F     weight_3      9\n 2  1001 F     weight_6     13\n 3  1001 F     weight_9     16\n 4  1001 F     weight_12    17\n 5  1001 F     length_3     17\n 6  1001 F     length_6     18\n 7  1001 F     length_9     19\n 8  1001 F     length_12    21\n 9  1002 F     weight_3     11\n10  1002 F     weight_6     16\n# ℹ 54 more rows\n\n\nBelieve it or not, we are actually pretty close to accomplishing our goal. Next, we need to somehow tell pivot_longer() that the column names we are pivoting contain a description of the values (i.e., heights and weights) and time values (i.e., 3, 6, 9, and 12 months). Notice that in all cases, the description and the time value are separated by an underscore. It turns out that we can use the names_sep argument to give pivot_longer() this information.\n\n\n32.2.6 The names_sep argument\nLet’s start by simply passing the adding the names_sep argument to the pivot_longer() function and pass it the value that separates our description and our time value:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols      = c(-id, -sex),\n    names_to  = \"months\",\n    names_sep = \"_\"\n  ) %&gt;% \n  print()\n\nError in `pivot_longer()`:\n! `names_sep` can't be used with a length 1 `names_to`.\n\n\nAnd we get an error. The reason we get an error can be seen in the following figure:\n\n\n\n\n\n\n\n\n\nwe are asking pivot_longer() to break up each column name (e.g., weight_3) at the underscore. That results in creating two separate character strings. In this case, the character string “weight” and the character string “3”. However, we only passed one value to the names_to argument – \"months\". So, which character string should pivot_longer() put in the months column? Of course, we know that the answer is “3”, but pivot_longer() doesn’t know that.\nSo, we have to pass two values to the names_to argument. But, what values should we pass?\n\n\n\n\n\n\n\n\n\nwe obviously want to character string that comes after the underscore to be called “months”. However, we can’t call the character string in front of the underscore “weight” because this column isn’t just identifying rows that contain weights. Similarly, we can’t call the character string in front of the underscore “length” because this column isn’t just identifying rows that contain lengths. For lack of a better idea, let’s just call it “measure”.\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols      = c(-id, -sex),\n    names_to  = c(\"measure\", \"months\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  print()\n\n# A tibble: 64 × 5\n      id sex   measure months value\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n 1  1001 F     weight  3          9\n 2  1001 F     weight  6         13\n 3  1001 F     weight  9         16\n 4  1001 F     weight  12        17\n 5  1001 F     length  3         17\n 6  1001 F     length  6         18\n 7  1001 F     length  9         19\n 8  1001 F     length  12        21\n 9  1002 F     weight  3         11\n10  1002 F     weight  6         16\n# ℹ 54 more rows\n\n\nThat sort of works. Except, what we really want is one row for each combination of id and months, each containing a value for weight and length. Instead, we have two rows for each combination of id and months. One set of rows contains weights and the other set of rows contains lengths.\nWhat we really need is for pivot_longer() to make weight one column and length a separate column, and then put the appropriate values from value under each. We can do this with the .value special value.\n\n\n32.2.7 The .value special value\nThe official help documentation for pivot_longer() says that the .value special value “indicates that [the] component of the name defines the name of the column containing the cell values, overriding values_to.” Said another way, .value tells pivot_longer() the character string in front of the underscore is the value description. Further, .value tells pivot_longer() to create a new column for each unique character string that is in front of the underscore.\n\n\n\n\n\n\n\n\n\nNow, let’s add the .value special value to our code:\n\nbabies_long &lt;- babies %&gt;% \n  pivot_longer(\n    cols      = c(-id, -sex),\n    names_to  = c(\".value\", \"months\"),\n    names_sep = \"_\",\n    names_transform = list(months = as.integer)\n  ) %&gt;% \n  print()\n\n# A tibble: 32 × 5\n      id sex   months weight length\n   &lt;int&gt; &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  1001 F          3      9     17\n 2  1001 F          6     13     18\n 3  1001 F          9     16     19\n 4  1001 F         12     17     21\n 5  1002 F          3     11     19\n 6  1002 F          6     16     21\n 7  1002 F          9     17     23\n 8  1002 F         12     20     23\n 9  1003 M          3     17     23\n10  1003 M          6     20     27\n# ℹ 22 more rows\n\n\nAnd that is exactly the result we wanted. However, there was one little detail we didn’t cover. How does .value know to create a new column for each unique character string that is in front of the underscore. Why didn’t it create a new column for each unique character string that is behind the underscore?\nThe answer is simple. It knows because of the ordering we used in the value we passed to the names_to argument. If we changed the order to c(\"months\", \".value\"), pivot_longer() would have created a new column for each unique character string that is behind the underscore. Take a look:\n\nbabies %&gt;% \n  pivot_longer(\n    cols      = c(-id, -sex),\n    names_to  = c(\"months\", \".value\"),\n    names_sep = \"_\"\n  )\n\n# A tibble: 16 × 7\n      id sex   months   `3`   `6`   `9`  `12`\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1001 F     weight     9    13    16    17\n 2  1001 F     length    17    18    19    21\n 3  1002 F     weight    11    16    17    20\n 4  1002 F     length    19    21    23    23\n 5  1003 M     weight    17    20    23    24\n 6  1003 M     length    23    27    30    33\n 7  1004 F     weight    16    18    21    22\n 8  1004 F     length    20    22    24    26\n 9  1005 M     weight    11    15    16    18\n10  1005 M     length    18    20    22    23\n11  1006 M     weight    17    21    25    26\n12  1006 M     length    22    26    28    30\n13  1007 M     weight    16    17    19    21\n14  1007 M     length    21    23    24    25\n15  1008 F     weight    15    16    18    19\n16  1008 F     length    18    19    23    24\n\n\nSo, be careful about the ordering of the values you pass to the names_to argument.\n\n\n32.2.8 Why person-period?\nWhy might we want the babies data in this person-period format? Well, as we discussed above, there are many analytic techniques that require our data to be in this format. Unfortunately, those techniques are beyond the scope of this chapter. However, this person-period format is still necessary for something as simple as plotting baby weight against baby height as we’ve done in the scatter plot below:\n\nbabies_long %&gt;% \n  mutate(months = factor(months, c(3, 6, 9, 12))) %&gt;% \n  ggplot() +\n    geom_point(aes(weight, length, color = months)) +\n    labs(\n      x = \"Weight (Pounds)\",\n      y = \"Length (Inches)\",\n      color = \"Age (Months)\"\n    ) +\n    theme_classic()",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Restructuring Data frames</span>"
    ]
  },
  {
    "objectID": "chapters/restructuring_data_frames/restructuring_data_frames.html#pivoting-wider",
    "href": "chapters/restructuring_data_frames/restructuring_data_frames.html#pivoting-wider",
    "title": "32  Restructuring Data frames",
    "section": "32.3 Pivoting wider",
    "text": "32.3 Pivoting wider\nAs previously discussed, the person-period, or long, data structure is usually preferable for longitudinal data analysis. However, there are times when the person-level data structure is preferable, or even necessary. Further, there are times when we have tables of analysis results, as opposed than actual data values, that we need to restructure for ease of interpretation. We will demonstrate how to do both below.\nWe’ll start by learning how to restructure, or reshape, our person-period babies_long data frame back to a person-level format. As a reminder, here is what our babies_long data frame currently looks like:\n\nbabies_long\n\n# A tibble: 32 × 5\n      id sex   months weight length\n   &lt;int&gt; &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  1001 F          3      9     17\n 2  1001 F          6     13     18\n 3  1001 F          9     16     19\n 4  1001 F         12     17     21\n 5  1002 F          3     11     19\n 6  1002 F          6     16     21\n 7  1002 F          9     17     23\n 8  1002 F         12     20     23\n 9  1003 M          3     17     23\n10  1003 M          6     20     27\n# ℹ 22 more rows\n\n\nAs you probably guessed, we will use tidyr’s pivot_wider() function to restructure the data:\n\nbabies &lt;- babies_long %&gt;% \n  pivot_wider(\n    names_from  = \"months\",\n    values_from = c(\"weight\", \"length\")\n  ) %&gt;% \n  print()\n\n# A tibble: 8 × 10\n     id sex   weight_3 weight_6 weight_9 weight_12 length_3 length_6 length_9\n  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1  1001 F            9       13       16        17       17       18       19\n2  1002 F           11       16       17        20       19       21       23\n3  1003 M           17       20       23        24       23       27       30\n4  1004 F           16       18       21        22       20       22       24\n5  1005 M           11       15       16        18       18       20       22\n6  1006 M           17       21       25        26       22       26       28\n7  1007 M           16       17       19        21       21       23       24\n8  1008 F           15       16       18        19       18       19       23\n# ℹ 1 more variable: length_12 &lt;dbl&gt;\n\n\n👆Here’s what we did above:\n\nWe used tidyr’s pivot_wider() function to restructure the babies_long data frame from person-period (long) to person-level (wide).\nYou can type ?pivot_wider into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the pivot_wider() function is the data argument. You should pass the name of the data frame you want to restructure to the data argument. Above, we passed the babies_long data frame to the data argument using a pipe operator.\nThe third argument (we left the second argument at its default value) to the pivot_wider() function is the names_from argument. You should pass this argument the name of a column, or columns, that exists in the data frame you passed to the data argument. The column(s) you choose should contain values that you want to become column names in the wide data frame. That’s a little be confusing, and our example above is sort of subtle, so here is a more obvious example:\n\n\ndf &lt;- tribble(\n  ~id, ~measure, ~lbs_inches,\n  1, \"weight\", 9,\n  1, \"length\", 17,\n  2, \"weight\", 11,\n  2, \"length\", 19 \n) %&gt;% \n  print()\n\n# A tibble: 4 × 3\n     id measure lbs_inches\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1     1 weight           9\n2     1 length          17\n3     2 weight          11\n4     2 length          19\n\n\n\nIn the data frame above, the values in the column named measure are what we want to use as column names in our wide data frame. Therefore, we would pass \"measure\" to the names_to argument of pivot_wider():\n\n\ndf %&gt;% pivot_wider(\n  names_from  = \"measure\",\n  values_from = \"lbs_inches\"\n)\n\n# A tibble: 2 × 3\n     id weight length\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1      9     17\n2     2     11     19\n\n\n\nOur babies example was more subtle in the sense that the long version of our data frame already had columns named weight and height. However, we essentially wanted to change those column names by adding the values from the column named months to the current column names. So, weight to weight_3, with the “3” coming from the column months.\nThe ninth argument (we left the fourth through eighth arguments at their default value) to the pivot_wider() function is the values_from argument. You should pass this argument the name of a column, or columns, that exists in the data frame you passed to the data argument. The column(s) you choose should contain values for the new columns you want to create in the new wide data frame. In our babies data frame, we wanted to pull the values from the weight and length columns respectively.\nThe combination of arguments (i.e., names_from  = \"months\" and values_from = c(\"weight\", \"length\")) that we passed to pivot_wider() above essentially said, “make new columns from each combination of the values in the column named months and the column names weight and length. So, weight_3, weight_6, etc. Then, the values you put in each column should come from the intersection of month and weight (for the weight_#) columns, or month and length (for the length_#) columns.\n\n\n32.3.1 Why person-level?\nWhy might we want the babies data in this person-level format? Well, as we discussed above, there are a handful analytic techniques that require our data to be in this format. Unfortunately, those techniques are beyond the scope of this chapter. However, this person-level format is still useful for something as simple as calculating descriptive statistics about time-invariant variables. For example, the number of female and male babies in our data frame:\n\nbabies %&gt;% \n  count(sex)\n\n# A tibble: 2 × 2\n  sex       n\n  &lt;chr&gt; &lt;int&gt;\n1 F         4\n2 M         4",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Restructuring Data frames</span>"
    ]
  },
  {
    "objectID": "chapters/restructuring_data_frames/restructuring_data_frames.html#pivoting-summary-statistics",
    "href": "chapters/restructuring_data_frames/restructuring_data_frames.html#pivoting-summary-statistics",
    "title": "32  Restructuring Data frames",
    "section": "32.4 Pivoting summary statistics",
    "text": "32.4 Pivoting summary statistics\nWhat do I mean by pivoting “summary statistics?” Well, in all the examples above we were manipulating the actual data values that were gathered about our observational units – babies. However, the ultimate goal of doing this kind of data management is typically to analyze it. In other words, we can often learn more from collapsing our data into a relatively small number of summary statistics than we can by viewing the actual data values themselves. Having said that, not all ways of organizing our summary statistics are equally informative. Or, perhaps it’s more accurate to say that not all ways of organizing our summary statistics convey the information with equal efficiency.\nThere are probably a near-infinite number of possible examples of manipulating summary statistics that we could discuss. Obviously, I can’t cover them all. However, I will walk through two examples below that are intended to give you a feel for what we are talking about.\n\n32.4.1 Pivoting summary statistics wide to long\nOur first example is a pretty simple one. Let’s say that we are working with our person-level babies data frame. In this scenario, we want to calculate the mean and standard deviation of weight at the 3, 6, 9, and 12-month follow-up visits. We might do the calculations like this:\n\nmean_weights &lt;- babies %&gt;% \n  summarise(\n    mean(weight_3),\n    sd(weight_3),\n    mean(weight_6),\n    sd(weight_6),\n    mean(weight_9),\n    sd(weight_9),\n    mean(weight_12),\n    sd(weight_12),\n  ) %&gt;% \n  print()\n\n# A tibble: 1 × 8\n  `mean(weight_3)` `sd(weight_3)` `mean(weight_6)` `sd(weight_6)`\n             &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1               14           3.16               17           2.62\n# ℹ 4 more variables: `mean(weight_9)` &lt;dbl&gt;, `sd(weight_9)` &lt;dbl&gt;,\n#   `mean(weight_12)` &lt;dbl&gt;, `sd(weight_12)` &lt;dbl&gt;\n\n\n\n🗒Side Note: This is not the most efficient way to do this analysis. We are only doing the analysis in this way to give us an excuse to use pivot_longer() to restructure some summary statistics.\n\nBy default, the mean and standard deviation are organized in a single row, side-by-side. One issue with organizing our results this way is that is that they don’t all fit on the screen at the same time. However, even if they did, it’s much more difficult for our brains to quickly scan the numbers and make comparisons across months when the summary statistics are organized this way than when they are stacked on top of each other. Take a look for yourself below:\n\nmean_weights %&gt;% \n  pivot_longer(\n    cols = everything(),\n    names_to = c(\".value\", \"measure\", \"months\"),\n    names_pattern = \"(\\\\w+)\\\\((\\\\w+)_(\\\\d+)\"\n  )\n\n# A tibble: 4 × 4\n  measure months  mean    sd\n  &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 weight  3       14    3.16\n2 weight  6       17    2.62\n3 weight  9       19.4  3.34\n4 weight  12      20.9  3.04\n\n\n👆Here’s what we did above:\n\nWe used tidyr’s pivot_longer() function to restructure our data frame of summary statistics from wide to long.\nThe only new argument above is the names_pattern argument. You should pass a regular expression to the names_pattern argument. This regular expression will tell pivot_longer() how to break up the original column names and repurpose them for the new column names. The regular expression we used above is not intended to be the main lesson here. But, I’m sure that some of you will be curious about how it works, so I will try to briefly explain it below. In a way, this is how R interprets the regular expression above (feel free to skip if you aren’t interested):\n\n\nstringr::str_match(\"mean(weight_3)\", \"(\\\\w+)\\\\((\\\\w+)_(\\\\d+)\")\n\n     [,1]            [,2]   [,3]     [,4]\n[1,] \"mean(weight_3\" \"mean\" \"weight\" \"3\" \n\n\n\nWe haven’t used parentheses yet in our regular expressions, but they create something called “capturing groups.” Instead of saying, “look for this one thing in the character string,” we say “look for these groups of things in this character string.”\nThe first capture group in the regular expression is (\\\\w+). This tells R to look for one or more word characters. The value that R grabs as part of this first capture group is given under the second result (i.e., [,2]) above – \"mean\".\nThen, the regular expression tells R to look for a literal open parenthesis \\\\(. However, this parenthesis is not included in a capture group. In this case, it’s really just used as landmark to tell R where the first capture group stops, and the second capture group starts.\nThe second capture group in the regular expression is another (\\\\w+). This again tells R to look for one or more word characters, but this time, R starts look for the word characters after the open parenthesis. The value that R grabs as part of the second capture group is given under the third result (i.e., [,3]) above – \"weight\".\nNext, the regular expression tells R to look for a literal underscore _. However, this underscore is not included in a capture group. In this case, it’s really just used as landmark to tell R where the second capture group stops, and the third capture group starts.\nThe third and final capture group in the regular expression is (\\\\d+). This tells R to look for one or more digits after the underscore. The value that R grabs as part of the third capture group is given under the third result (i.e., [,4]) above – \"3\".\nFinally, R matches the values it grabs in each of the three capture groups with the three values passed to the names_to argument, which are \".value\", \"measure\", and \"months\". We already discussed the .value special value above. Similar to before, .value will create a new column for each unique value captured in the first capture group. In this case, mean and sd. Next, the values captured in the second capture group are assigned to a column named measure. Finally, the values captured in the third capture group are assigned to a column named months.\n\n\n\n32.4.2 Pivoting summary statistics long to wide\nThis next example comes from an actual project I was involved with. As a part of this project, researchers asked the parents of elementary-aged children about series of sun protection behaviors. Below, I’m not simulating the data that was collected. Rather, I am simulating a small part of the results of one of the early descriptive analyses we conducted:\n\nsummary_stats &lt;- tribble(\n  ~period, ~behavior, ~value, ~n, ~n_total, ~percent,\n  \"School Year Weekends\", \"Long sleeve shirt\", \"Never\", 6, 78,  8,  \n  \"School Year Weekends\", \"Long sleeve shirt\", \"Seldom\", 16, 78,    21, \n  \"School Year Weekends\", \"Long sleeve shirt\", \"Sometimes\", 33, 78, 42, \n  \"School Year Weekends\", \"Long sleeve shirt\", \"Often\", 17, 78, 22, \n  \"School Year Weekends\", \"Long sleeve shirt\", \"Always\", 6, 78, 8,  \n  \"School Year Weekends\", \"Long Pants\", \"Never\", 5, 79, 6,  \n  \"School Year Weekends\", \"Long Pants\", \"Seldom\",   15, 79, 19, \n  \"School Year Weekends\", \"Long Pants\", \"Sometimes\", 32, 79, 41,    \n  \"School Year Weekends\", \"Long Pants\", \"Often\", 19, 79, 24,    \n  \"School Year Weekends\", \"Long Pants\", \"Always\",   8, 79, 10,  \n  \"Summer\", \"Long sleeve shirt\", \"Never\",   9, 80, 11,  \n  \"Summer\", \"Long sleeve shirt\", \"Seldom\", 18, 80, 22,  \n  \"Summer\", \"Long sleeve shirt\", \"Sometimes\", 31,   80, 39, \n  \"Summer\", \"Long sleeve shirt\", \"Often\",   14, 80, 18, \n  \"Summer\", \"Long sleeve shirt\", \"Always\", 8,   80, 10, \n  \"Summer\", \"Long Pants\", \"Never\", 7,   76, 9,  \n  \"Summer\", \"Long Pants\", \"Seldom\", 16, 76, 21, \n  \"Summer\", \"Long Pants\", \"Sometimes\", 27,  76, 36, \n  \"Summer\", \"Long Pants\", \"Often\", 18, 76,  24, \n  \"Summer\", \"Long Pants\", \"Always\", 8, 76,  11\n) %&gt;% \n  print()\n\n# A tibble: 20 × 6\n   period               behavior          value         n n_total percent\n   &lt;chr&gt;                &lt;chr&gt;             &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 School Year Weekends Long sleeve shirt Never         6      78       8\n 2 School Year Weekends Long sleeve shirt Seldom       16      78      21\n 3 School Year Weekends Long sleeve shirt Sometimes    33      78      42\n 4 School Year Weekends Long sleeve shirt Often        17      78      22\n 5 School Year Weekends Long sleeve shirt Always        6      78       8\n 6 School Year Weekends Long Pants        Never         5      79       6\n 7 School Year Weekends Long Pants        Seldom       15      79      19\n 8 School Year Weekends Long Pants        Sometimes    32      79      41\n 9 School Year Weekends Long Pants        Often        19      79      24\n10 School Year Weekends Long Pants        Always        8      79      10\n11 Summer               Long sleeve shirt Never         9      80      11\n12 Summer               Long sleeve shirt Seldom       18      80      22\n13 Summer               Long sleeve shirt Sometimes    31      80      39\n14 Summer               Long sleeve shirt Often        14      80      18\n15 Summer               Long sleeve shirt Always        8      80      10\n16 Summer               Long Pants        Never         7      76       9\n17 Summer               Long Pants        Seldom       16      76      21\n18 Summer               Long Pants        Sometimes    27      76      36\n19 Summer               Long Pants        Often        18      76      24\n20 Summer               Long Pants        Always        8      76      11\n\n\n\nThe period column contains the time frame the researchers were asking the parents about. It can take the values School Year Weekends or Summer.\nThe behavior column contains each of the specific behaviors that the researchers were interested in. Above, behavior takes only the values Long sleeve shirt and Long Pants.\nThe value column contains the possible answer choices that parents could select from.\nThe n column contains the number of parents who selected the response in value for the behavior in behavior and the time frame in period. For example, n = 6 in the first row indicates that six parents said that their child never wears long sleeve shirts on weekends during the school year.\nThe n_total column is the sum of n for each period/behavior combination.\nThe percent column contains the percentage of parents who selected the response in value for the behavior in behavior and the time frame in period. For example, percent = 8 in the first row indicates that 8 percent of parents said that their child never wears long sleeve shirts on weekends during the school year.\n\nThese results are relatively difficult to scan and get a feel for. In particular, these researchers were interested in whether or not engagement in these protective behaviors differed by period. In other words, were kids more likely to wear long sleeve shirts on weekends during the school year than they were during the summer? It’s difficult to answer that quickly with the way the summary statistics above are organized. We can improve the interpretability of our results by combining n and percent into a single character string, and pivoting them wider so that the two periods are presented side-by-side:\n\nsummary_stats %&gt;% \n  # Combine n and percent into a single character string\n  mutate(n_percent = paste0(n, \" (\", percent, \")\")) %&gt;% \n  # We no longer need n, n_total, percent\n  select(-n:-percent) %&gt;% \n  pivot_wider(\n    names_from = \"period\",\n    values_from = \"n_percent\"\n  )\n\n# A tibble: 10 × 4\n   behavior          value     `School Year Weekends` Summer \n   &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;                  &lt;chr&gt;  \n 1 Long sleeve shirt Never     6 (8)                  9 (11) \n 2 Long sleeve shirt Seldom    16 (21)                18 (22)\n 3 Long sleeve shirt Sometimes 33 (42)                31 (39)\n 4 Long sleeve shirt Often     17 (22)                14 (18)\n 5 Long sleeve shirt Always    6 (8)                  8 (10) \n 6 Long Pants        Never     5 (6)                  7 (9)  \n 7 Long Pants        Seldom    15 (19)                16 (21)\n 8 Long Pants        Sometimes 32 (41)                27 (36)\n 9 Long Pants        Often     19 (24)                18 (24)\n10 Long Pants        Always    8 (10)                 8 (11) \n\n\nThe layout of our summary statistics above is now much more compact. Further, it’s much easier to compare behaviors between the two time periods. For example, we can see that a slightly higher percentage of people (11%) reported that their child never wears a long sleeve shirt during the summer as compared to weekends during the school year (8%).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Restructuring Data frames</span>"
    ]
  },
  {
    "objectID": "chapters/restructuring_data_frames/restructuring_data_frames.html#tidy-data",
    "href": "chapters/restructuring_data_frames/restructuring_data_frames.html#tidy-data",
    "title": "32  Restructuring Data frames",
    "section": "32.5 Tidy data",
    "text": "32.5 Tidy data\nAs I said above, the person-level (wide) and person-period (long) data structures are the traditional way of classifying how longitudinal (or repeated measures) data are organized. In reality, however, structuring data in a way that is most conducive to analysis is often more complicated than the examples above would lead you to believe. Simply thinking about data structure in terms of wide and long sometimes leaves us with an incomplete model for how to take many real-world data sets and prepare them for conducting analysis in an efficient way. In his seminal paper on the topic, Hadley Wickham, provides us with a set of guidelines for systematically (re)structuring our data in a way that is consistent, and generally optimized for analysis. He refers to this process as “tidying” our data, and to the resulting data frame as “tidy data”.1\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested, you can download the entire article for free from the Journal of statistical Software here.\n\n\nThe three basic guidelines for tidy data are:\n\nEach variable (i.e., measurement or characteristic about the observational unit) must have its own column.\nEach observation (i.e. the people, places, or things we are interested in characterizing or comparing at a particular occasion) must have its own row.\nEach value must have its own cell.\n\nAccording to the tidy data philosophy, any data frame that does not conform to the guidelines above is considered “messy” data. In my opinion, it’s kind of hard to read the guidelines above and wrap your head around what tidy data is. I think it’s actually easier to get a feel for tidy data by looking at examples of data that are not tidy. Let’s go ahead and take a look at a few examples:\n\n32.5.1 Each variable must have its own column\nWhat does it mean for every variable to have its own column? Well, let’s say we interested the rate of neural tube defects by state. So, we pull some data from a government website that looks like this:\n\nbirths_ntd &lt;- tibble(\n  state   = rep(c(\"CA\", \"FL\", \"TX\"), each = 2),\n  outcome = rep(c(\"births\", \"neural tube defects\"), 3),\n  count   = c(454920, 318, 221542, 155, 378624, 265)\n) %&gt;% \n  print()\n\n# A tibble: 6 × 3\n  state outcome              count\n  &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;\n1 CA    births              454920\n2 CA    neural tube defects    318\n3 FL    births              221542\n4 FL    neural tube defects    155\n5 TX    births              378624\n6 TX    neural tube defects    265\n\n\nIn this case, there is only one count column, but that column really contains two variables: the count of live births and the count of neural tube defects. Further, the outcome column doesn’t really contain “data.” In this case, the values stored in the outcome column are really data labels. We can tidy this data using the pivot_wider() function:\n\nbirths_ntd %&gt;% \n  pivot_wider(\n    names_from  = \"outcome\",\n    values_from = \"count\"\n  )\n\n# A tibble: 3 × 3\n  state births `neural tube defects`\n  &lt;chr&gt;  &lt;dbl&gt;                 &lt;dbl&gt;\n1 CA    454920                   318\n2 FL    221542                   155\n3 TX    378624                   265\n\n\nNow, births and neural tube defects each have their own column. It might also be a good idea to remove the spaces from neural tube defects and make it clear that the values in each column are counts. But, I’m going to leave that to you.\nAnother common violation of the “each variable must have its own column” guideline is when column names contain data values. We already saw an example of this above. Our weight_ and length_ column names actually had time data embedded in them.\nIn the example below, each column name contains two data values (i.e., sex and year); however, neither variable currently has a column in the data:\n\nbirths_sex &lt;- tibble(\n  state  = c(\"CA\", \"FL\", \"TX\"),\n  f_2018 = c(222911, 108556, 185526),\n  m_2018 = c(232009, 112986, 193098)\n) %&gt;% \n  print()\n\n# A tibble: 3 × 3\n  state f_2018 m_2018\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 CA    222911 232009\n2 FL    108556 112986\n3 TX    185526 193098\n\n\nIn this case, we can tidy the data by giving sex and year a column, and giving the other data values (i.e., count of live births) a more informative column name. We can do so with the pivot_longer() function:\n\nbirths_sex %&gt;% \n  pivot_longer(\n    cols      = -state,\n    names_to  = c(\"sex\", \"year\"),\n    names_sep = \"_\",\n    values_to = \"births\"\n  )\n\n# A tibble: 6 × 4\n  state sex   year  births\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 CA    f     2018  222911\n2 CA    m     2018  232009\n3 FL    f     2018  108556\n4 FL    m     2018  112986\n5 TX    f     2018  185526\n6 TX    m     2018  193098\n\n\n\n\n32.5.2 Each observation must have its own row\nOur person-level babies data frame above also violated this guideline.\n\nbabies\n\n# A tibble: 8 × 10\n     id sex   weight_3 weight_6 weight_9 weight_12 length_3 length_6 length_9\n  &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1  1001 F            9       13       16        17       17       18       19\n2  1002 F           11       16       17        20       19       21       23\n3  1003 M           17       20       23        24       23       27       30\n4  1004 F           16       18       21        22       20       22       24\n5  1005 M           11       15       16        18       18       20       22\n6  1006 M           17       21       25        26       22       26       28\n7  1007 M           16       17       19        21       21       23       24\n8  1008 F           15       16       18        19       18       19       23\n# ℹ 1 more variable: length_12 &lt;dbl&gt;\n\n\nNotice that each baby in this data has one row, but that each row actually contains four unique observations – at 3, 6, 9, and 12 months. As another example, let’s say that we’ve once again downloaded birth count data from a government website. This time, we are interested in investigating the absolute change in live births over the decade between 2010 and 2020. That data may look like this:\n\nbirths_decade &lt;- tibble(\n  state  = c(\"CA\", \"FL\", \"TX\"),\n  `2010` = c(409428, 199388, 340762),\n  `2020` = c(454920, 221542, 378624)\n) %&gt;% \n  print()\n\n# A tibble: 3 × 3\n  state `2010` `2020`\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 CA    409428 454920\n2 FL    199388 221542\n3 TX    340762 378624\n\n\nIn this example, each state has a single row, but multiple observations. We can once again tidy this data using the pivot_longer() function:\n\nbirths_decade %&gt;% \n  pivot_longer(\n    cols      = -state,\n    names_to  = \"year\",\n    values_to = \"births\"\n  )\n\n# A tibble: 6 × 3\n  state year  births\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 CA    2010  409428\n2 CA    2020  454920\n3 FL    2010  199388\n4 FL    2020  221542\n5 TX    2010  340762\n6 TX    2020  378624\n\n\n\n\n32.5.3 Each value must have its own cell\nIn my personal experience, violations of this guideline are rarer than violations of the first two guidelines. However, let’s imagine a study where we are monitoring the sleeping habits of newborn babies. Specifically, we are interested in the range of lengths of time they sleep. That data could be recorded the following way:\n\nbaby_sleep &lt;- tibble(\n  id          = c(1001, 1002, 1003),\n  sleep_range = c(\".5-2\", \".75-2.4\", \"1.1-3.8\")\n) %&gt;% \n  print()\n\n# A tibble: 3 × 2\n     id sleep_range\n  &lt;dbl&gt; &lt;chr&gt;      \n1  1001 .5-2       \n2  1002 .75-2.4    \n3  1003 1.1-3.8    \n\n\nIn this case, we will use a new function to tidy our data. We will use tidyr’s separate() function to spread these values out across two columns:\n\nbaby_sleep %&gt;% \n  separate(\n    col     = sleep_range,\n    into    = c(\"min_hours\", \"max_hours\"),\n    sep     = \"-\",\n    convert = TRUE\n  )\n\n# A tibble: 3 × 3\n     id min_hours max_hours\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  1001      0.5        2  \n2  1002      0.75       2.4\n3  1003      1.1        3.8\n\n\n👆Here’s what we did above:\n\nWe used tidyr’s separate() function to tidy the baby_sleep data frame.\nYou can type ?separate into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the separate() function is the data argument. You should pass the name of the data frame you want to restructure to the data argument. Above, we passed the baby_sleep data frame to the data argument using a pipe operator.\nThe second argument to the separate() function is the col argument. You should pass the name of the column contain the data values that you want to split up to the col argument.\nThe third argument to the separate() function is the into argument. You should pass the into argument a character vector of column names you want to give the new columns that will be created when you break apart the values in the col column.\nThe fourth argument to the separate() function is the sep argument. You should pass the sep argument a character string that tells separate() what character separates the individual values in the col column.\nFinally, we passed the value TRUE to the convert argument. In doing so, we asked separate() to coerce the values in min_hours and max_hours from character type to numeric type.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Restructuring Data frames</span>"
    ]
  },
  {
    "objectID": "chapters/restructuring_data_frames/restructuring_data_frames.html#the-complete-function",
    "href": "chapters/restructuring_data_frames/restructuring_data_frames.html#the-complete-function",
    "title": "32  Restructuring Data frames",
    "section": "32.6 The complete() function",
    "text": "32.6 The complete() function\nThe final function we’re going to discuss in this chapter is tidyr’s complete() function. After we pivot data, we will sometimes notice “holes” in the data. This typically happens to me in the context of time data. When this happens, we can use the complete() function to fill-in the holes in our data.\nThis next example didn’t actually involve pivoting, but it did come from another actual project that I was involved with, and nicely demonstrates the importance of filling-in holes in the data. As a part of this project, researchers were interested in increasing the number of reports of elder mistreatment that were being made to Adult Protective Services (APS) by emergency medical technicians (EMTs) and paramedics. Each row in the raw data the researchers received from the emergency medical services provider represented a report to APS. Let’s say that the data from the week of October 28th, 2019 to November 3rd, 2019 looked something like this:\n\nreports &lt;- tibble(\n  date      = as.Date(c(\n    \"2019-10-29\", \"2019-10-29\", \"2019-10-30\", \"2019-11-02\", \"2019-11-02\"\n  )),\n  emp_id    = c(5123, 2224, 5153, 9876, 4030),\n  report_id = c(\"a8934\", \"af2as\", \"jzia3\", \"3293n\", \"dsf98\")\n) %&gt;% \n  print()\n\n# A tibble: 5 × 3\n  date       emp_id report_id\n  &lt;date&gt;      &lt;dbl&gt; &lt;chr&gt;    \n1 2019-10-29   5123 a8934    \n2 2019-10-29   2224 af2as    \n3 2019-10-30   5153 jzia3    \n4 2019-11-02   9876 3293n    \n5 2019-11-02   4030 dsf98    \n\n\nWhere:\n\ndate is the date the report was made to APS.\nemp_id is a unique identifier for each EMT or paramedic.\nreport_id is the unique identifier APS assigns to the incoming report.\n\nLet’s say that the researchers were interested in calculating the average number of reports per day. We would first need to count the number of reports made each day:\n\nreports %&gt;% \n  count(date)\n\n# A tibble: 3 × 2\n  date           n\n  &lt;date&gt;     &lt;int&gt;\n1 2019-10-29     2\n2 2019-10-30     1\n3 2019-11-02     2\n\n\nNext, we might naively go ahead and calculate the mean of n like this:\n\nreports %&gt;% \n  count(date) %&gt;% \n  summarise(mean_reports_per_day = mean(n))\n\n# A tibble: 1 × 1\n  mean_reports_per_day\n                 &lt;dbl&gt;\n1                 1.67\n\n\nAnd conclude that the mean number of reports made per day was 1.67. However, there is a problem with this strategy. Our study period wasn’t three days long. It was seven days long (i.e., October 28th, 2019 to November 3rd, 2019). Because there weren’t any reports made on 2019-10-28, 2019-10-31, 2019-11-01, or 2019-11-03 they don’t exist in our count data. But, their absence doesn’t represent a missing or unknown value. Their absence represents zero reports being made on that day. We need to explicitly encode that information in our count data if we want to accurately calculate the mean number of reports per day. In this tiny little simulated data frame, it’s trivial to do this calculation manually. However, the real data set was collected over a three-year period. That’s over 1,000 days that would have to be manually accounted for.\nLuckily, we can use tidyr’s complete() function, along with the seq.Date() function we learned in the chapter on working with date variables, to fill-in the holes in our count data in an automated way:\n\nreports %&gt;% \n  count(date) %&gt;% \n  complete(\n    date = seq.Date(\n      from = as.Date(\"2019-10-28\"), \n      to = as.Date(\"2019-11-03\"), \n      by = \"days\"\n    )\n  )\n\n# A tibble: 7 × 2\n  date           n\n  &lt;date&gt;     &lt;int&gt;\n1 2019-10-28    NA\n2 2019-10-29     2\n3 2019-10-30     1\n4 2019-10-31    NA\n5 2019-11-01    NA\n6 2019-11-02     2\n7 2019-11-03    NA\n\n\n👆Here’s what we did above:\n\nWe used tidyr’s complete() function to fill-in the holes in the dates between 2019-10-28 and 2019-11-03.\nYou can type ?complete into your R console to view the help documentation for this function and follow along with the explanation below.\nThe first argument to the complete() function is the data argument. You should pass the name of the data frame that contains the column you want to fill-in to the data argument. Above, we passed the reports data frame to the data argument using a pipe operator.\nThe second argument to the complete() function is the ... argument. This is where you tell the complete() function which column you want to fill-in, or expand, and give it instructions for doing so. Above, we asked complete() to make sure that each day between 2019-10-28 and 2019-11-03 was included in our date column. We did so by asking complete() to set the date column equal to the returned values from the seq.Date() function.\n\nNotice that all the days during our period of interest are now included in our count data. However, by default, the value for each new row of the n column is set to NA. But, as we already discussed, n isn’t missing for those days, it’s zero. We can change those values from NA to zero by adjusting the value we pass to the fill argument. We’ll do that next:\n\nreports %&gt;% \n  count(date) %&gt;% \n  complete(\n    date = seq.Date(\n      from = as.Date(\"2019-10-28\"), \n      to = as.Date(\"2019-11-03\"), \n      by = \"days\"\n    ),\n    fill = list(n = 0)\n  )\n\n# A tibble: 7 × 2\n  date           n\n  &lt;date&gt;     &lt;int&gt;\n1 2019-10-28     0\n2 2019-10-29     2\n3 2019-10-30     1\n4 2019-10-31     0\n5 2019-11-01     0\n6 2019-11-02     2\n7 2019-11-03     0\n\n\nNow, we can finally calculate the correct value for mean number of reports made per day during the week of October 28th, 2019 to November 3rd, 2019:\n\nreports %&gt;% \n  count(date) %&gt;% \n  complete(\n    date = seq.Date(\n      from = as.Date(\"2019-10-28\"), \n      to = as.Date(\"2019-11-03\"), \n      by = \"days\"\n    ),\n    fill = list(n = 0)\n  ) %&gt;% \n  summarise(mean_reports_per_day = mean(n))\n\n# A tibble: 1 × 1\n  mean_reports_per_day\n                 &lt;dbl&gt;\n1                0.714\n\n\nThat concludes the chapter on restructuring data. For now, it also concludes the part of this book devoted to the basics of data management. At this point, you should have the tools you need to tackle the majority of the common data management tasks that you will come across. Further, there’s a good chance that the packages we’ve used in this part of the book will contain a solution for the remaining data management challenges that we haven’t explicitly covered. In the next part of the book, we will dive into repeated operations.\n\n\n\n\n1. Wickham H. Tidy data. Journal of Statistical Software, Articles. 2014;59(10):1-23.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Restructuring Data frames</span>"
    ]
  },
  {
    "objectID": "chapters/intro_repeated_operations/intro_repeated_operations.html",
    "href": "chapters/intro_repeated_operations/intro_repeated_operations.html",
    "title": "33  Introduction to Repeated Operations",
    "section": "",
    "text": "33.1 Multiple methods for repeated operations in R\nThis part of the book is all about the DRY principle. We first discussed the DRY principle in the section on creating and modifying multiple columns. As a reminder, DRY is an acronym for “Don’t Repeat Yourself.” But, what does that mean?\nWell, think back to the conditional operations chapter. In that chapter, we compared conditional statements in R with asking our daughters to wear a raincoat if it’s raining. To extend the analogy, now imagine that we wake up one morning and say, “please wear your raincoat if it’s raining today - July 1st.” Then, we wake up the next morning and say, “please wear your raincoat if it’s raining today - July 2nd.” Then, we wake up the next morning and say, “please wear your raincoat if it’s raining today - July 3rd.” And, that pattern continues every morning until our daughters move out of the house. That’s a ton of repetition!! Alternatively, wouldn’t it be much more efficient to say, “please wear your raincoat on every day that it rains,” just once?\nThe same logic applies to our R code. We often want to do the same (or very similar) thing multiple times. This can result in many lines of code that are very similar and unnecessarily repetitive, and this unnecessary repetition can occur in all phases of our projects.\nFor example:\nIn all of these situations we are asking our R code to do something repeatedly, or iteratively, but with a slight change each time. We can write a separate chunk of code for each time we want to do that thing, or we can write one chunk of code that asks R to do that thing over and over. Writing code in the later way will often result in R programs that:\nSo, writing code that is highly repetitive is usually not a great idea, and this part of the book is all about teaching you to recognize and remove unnecessary repetition from your code. As is often the case with R, there are multiple different methods we can use.\nIn the chapters that follow, we will learn four different methods for removing unnecessary repetition from our code. They are:\nFour methods for removing unnecessary repetition\nIt’s also important to recognize that each of the methods above can be used independently or in combination with each other. We will see examples of both.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Repeated Operations</span>"
    ]
  },
  {
    "objectID": "chapters/intro_repeated_operations/intro_repeated_operations.html#multiple-methods-for-repeated-operations-in-r",
    "href": "chapters/intro_repeated_operations/intro_repeated_operations.html#multiple-methods-for-repeated-operations-in-r",
    "title": "33  Introduction to Repeated Operations",
    "section": "",
    "text": "Writing our own functions that can be reused throughout our code.\nUsing dplyr’s column-wise operations.\nUsing for loops.\nUsing the purrr package.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Repeated Operations</span>"
    ]
  },
  {
    "objectID": "chapters/intro_repeated_operations/intro_repeated_operations.html#tidy-evaluation",
    "href": "chapters/intro_repeated_operations/intro_repeated_operations.html#tidy-evaluation",
    "title": "33  Introduction to Repeated Operations",
    "section": "33.2 Tidy evaluation",
    "text": "33.2 Tidy evaluation\nIn case it isn’t obvious to you by now, we’re fans of the tidyverse packages (i.e., dplyr, ggplot2, tidyr, etc.). We use dplyr, in particular, in virtually every single one of our R programs. The use of non-standard evaluation is just one of the many aspects of the tidyverse packages that we’re fans of. As a reminder, among other things, non-standard evaluation is what allows us to refer to data frame columns without using dollar sign or bracket notation (i.e., data masking). However, non-standard evaluation will create some challenges for us when we try to use functions from tidyverse packages inside of functions and for loops that we write ourselves. Therefore, we will have to learn more about tidy evaluation if we want to continue to use the tidyverse packages that we’ve been using throughout the book so far.\nTidy evaluation can be tricky even for experienced R programmers to wrap their heads around at first. Therefore, it might not be productive for us to try to learn a lot about the theory behind, or internals of, tidy evaluation as a standalone concept. Instead, in the chapters that follow, we plan to sprinkle in just enough tidy evaluation to accomplish the task at hand. As a little preview, a telltale sign that we are using tidy evaluation will be when you start seeing the {{ (said, curly-curly) operator and the !! (said, bang bang) operator. Hopefully, this will all make more sense in the next chapter when we start to get into some examples.\nWe recommend the following resources for those of you who are interested in developing a deeper understanding of rlang and tidy evaluation:\n\nProgramming with dplyr. Accessed July 31, 2020. https://dplyr.tidyverse.org/articles/programming.html\nWickham H. Introduction. In: Advanced R. Accessed July 31, 2020. https://adv-r.hadley.nz/metaprogramming.html\n\nNow, let’s learn how to write our own functions!🤓",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Repeated Operations</span>"
    ]
  },
  {
    "objectID": "chapters/writing_functions/writing_functions.html",
    "href": "chapters/writing_functions/writing_functions.html",
    "title": "34  Writing Functions",
    "section": "",
    "text": "34.1 When to write functions\nHave you noticed how we will often calculate the same statistical measures for many different variables in our data? For example, let’s say that we have some pretty standard data about some study participants that looks like this:\nWhen we have data like this, it’s pretty common to calculate something like the number of missing values, mean, median, min, and max for all of the continuous variables. So, we might use the following code to calculate these measures:\nGreat! Next, we want to do the same calculations for ht_in. Of course, we don’t want to type everything in that code chunk again, so we copy and paste. And change all the instances of age to ht_in:\nNow, let’s do the same calculations for wt_lbs and bmi. Again, we will copy and paste, and change the variable name as needed:\nAnd, we’re done!\nHowever, there’s a problem. Did you spot it? We accidentally forgot to change ht_in to wt_lbs in the min calculation above. Therefore, our results incorrectly indicate that the minimum weight was 58 lbs. Part of the reason for making this mistake in the first place is that there is a fair amount of visual clutter in each code chunk. In other words, it’s hard to quickly scan each chunk and see only the elements that are changing.\nAdditionally, each code chunk was about 8 lines of code. Even with only 4 variables, that’s still 32 lines. We can improve on this code by writing our own function. That’s exactly what we will do in the code chunk below. For now, don’t worry if you don’t understand how the code works. We will dissect it later.\nNow, let’s use the function we just created above to once again calculate the descriptive measures we are interested in.\nPretty cool, right? We reduced 32 lines of code to 13 lines of code! Additionally, it’s very easy to quickly scan our code and see that the only thing changing from chunk-to-chunk is the name of the variable that we are passing to our function and ensure that it is actually changing. As an added bonus, because we’ve strategically given our function an informative name, the intent behind what we are trying to accomplish is clearer now – we are calculating summary statistics about our continuous variables.\nHopefully, this little demonstration has left you feeling like writing your own functions can be really useful, and maybe even kind of fun. We’re going to get into the nuts and bolts of how to write your own functions shortly, but first let’s briefly discuss when to write your own functions.\nHadley Wickham, prolific R developer and teacher says, “You should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code).”1 We completely agree with this general sentiment. We’ll only amend our advice to you slightly. Specifically, you should consider using an appropriate method for repeating operations whenever you’ve copied and pasted a block of code more than twice. In other words, writing a function is not the only option available to us when we notice ourselves copying and pasting code.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/writing_functions/writing_functions.html#how-to-write-functions",
    "href": "chapters/writing_functions/writing_functions.html#how-to-write-functions",
    "title": "34  Writing Functions",
    "section": "34.2 How to write functions",
    "text": "34.2 How to write functions\nNow, the fun part – writing our own functions. Writing functions can seem intimidating to many people at first. However, the basics are actually pretty simple.\n\n34.2.1 The function() function\nIt all starts with the function() function. This is how you tell R that you are about to write your own function.\n\n\n\n\n\nThe function() function.\n\n\n\n\nIf you think back to the chapter on Speaking R’s language, we talked about the analogy that is sometimes drawn between functions and factories.\n\n\n\n\n\nA factory making bicycles.\n\n\n\n\nTo build on that analogy, thefunction() function is sort of like the factory building. Without it, there is no factory, but an empty building alone doesn’t do anything interesting:\n\nfunction()\n\nError in parse(text = input): &lt;text&gt;:2:0: unexpected end of input\n1: function()\n   ^\n\n\nIn order to build our bicycles, we need to add some workers and equipment to our empty factory building. The R function equivalent to the workers and equipment is the function body.\n\n\n\n\n\nThe function body.\n\n\n\n\nAnd just like the factory needs doors to contain our workers and equipment and keep them safe (This is admittedly a bit of a reach, but just go with it), our function body needs to be wrapped with curly braces.\n\n\n\n\n\nCurly braces around the function body.\n\n\n\n\nWe already talked about how the values we pass to arguments are raw material inputs that go into the factory.\n\n\n\n\n\nThe function argument(s).\n\n\n\n\nIn the bicycle factory example, the raw materials were steel and rubber. In the function displayed above, the raw materials are variables.\nIf we want to be able to call our function (i.e., use it) later, then we have to have some way to refer to it. Therefore, we will assign our function a name.\n\n\n\n\n\nThe named function.\n\n\n\n\n\n\n34.2.2 The function writing process\nSo, we have some idea about why writing our own functions can be a good idea. We have some idea about when to write functions (i.e., don’t repeat yourself… more than twice). And, we now know what the basic components of functions are. They are the function() function, the function body (wrapped in curly braces), the function argument(s), and the function name. But, if this is your first time being exposed to functions, then you may still be feeling like you aren’t quite sure how to get started with writing your own. So, here’s a little example of how a function writing workflow could go.\nFirst, let’s simulate some new data for this example. Let’s say we have two data frames that contain first and last names:\n\npeople_1 &lt;- tribble(\n  ~id_1, ~name_first_1, ~name_last_1, ~street_1,\n  1,     \"Easton\",      NA,           \"Alameda\",\n  2,     \"Elias\",       \"Salazar\",    \"Crissy Field\",\n  3,     \"Colton\",      \"Fox\",        \"San Bruno\",\n  4,     \"Cameron\",     \"Warren\",     \"Nottingham\",\n  5,     \"Carson\",      \"Mills\",      \"Jersey\",\n  6,     \"Addison\",     \"Meyer\",      \"Tingley\",\n  7,     \"Aubrey\",      \"Rice\",       \"Buena Vista\",\n  8,     \"Ellie\",       \"Schmidt\",    \"Division\",\n  9,     \"Robert\",      \"Garza\",      \"Red Rock\",\n  10,    \"Stella\",      \"Daniels\",    \"Holland\"\n) %&gt;% \n  print()\n\n# A tibble: 10 × 4\n    id_1 name_first_1 name_last_1 street_1    \n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;       \n 1     1 Easton       &lt;NA&gt;        Alameda     \n 2     2 Elias        Salazar     Crissy Field\n 3     3 Colton       Fox         San Bruno   \n 4     4 Cameron      Warren      Nottingham  \n 5     5 Carson       Mills       Jersey      \n 6     6 Addison      Meyer       Tingley     \n 7     7 Aubrey       Rice        Buena Vista \n 8     8 Ellie        Schmidt     Division    \n 9     9 Robert       Garza       Red Rock    \n10    10 Stella       Daniels     Holland     \n\n\n\npeople_2 &lt;- tribble(\n  ~id_2, ~name_first_2, ~name_last_2, ~street_2,\n  1,     \"Easton\",      \"Stone\",      \"Alameda\",\n  2,     \"Elas\",        \"Salazar\",    \"Field\",\n  3,     NA,            \"Fox\",        NA,\n  4,     \"Cameron\",     \"Waren\",      \"Notingham\",\n  5,     \"Carsen\",      \"Mills\",      \"Jersey\",\n  6,     \"Adison\",      NA,           NA,\n  7,     \"Aubrey\",      \"Rice\",       \"Buena Vista\",\n  8,     NA,            \"Schmidt\",    \"Division\",\n  9,     \"Bob\",         \"Garza\",      \"Red Rock\",\n  10,    \"Stella\",      NA,           \"Holland\"\n) %&gt;% \n  print()\n\n# A tibble: 10 × 4\n    id_2 name_first_2 name_last_2 street_2   \n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Stone       Alameda    \n 2     2 Elas         Salazar     Field      \n 3     3 &lt;NA&gt;         Fox         &lt;NA&gt;       \n 4     4 Cameron      Waren       Notingham  \n 5     5 Carsen       Mills       Jersey     \n 6     6 Adison       &lt;NA&gt;        &lt;NA&gt;       \n 7     7 Aubrey       Rice        Buena Vista\n 8     8 &lt;NA&gt;         Schmidt     Division   \n 9     9 Bob          Garza       Red Rock   \n10    10 Stella       &lt;NA&gt;        Holland    \n\n\nIn this scenario, we want to see if first name, last name, and street name match at each ID between our data frames. More specifically, we want to combine the two data frames into a single data frame and create three new dummy variables that indicate whether first name, last name, and address match respectively. Let’s go ahead and combine the data frames now:\n\npeople &lt;- people_1 %&gt;% \n  bind_cols(people_2) %&gt;% \n  print()\n\n# A tibble: 10 × 8\n    id_1 name_first_1 name_last_1 street_1      id_2 name_first_2 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n 1     1 Easton       &lt;NA&gt;        Alameda          1 Easton       Stone      \n 2     2 Elias        Salazar     Crissy Field     2 Elas         Salazar    \n 3     3 Colton       Fox         San Bruno        3 &lt;NA&gt;         Fox        \n 4     4 Cameron      Warren      Nottingham       4 Cameron      Waren      \n 5     5 Carson       Mills       Jersey           5 Carsen       Mills      \n 6     6 Addison      Meyer       Tingley          6 Adison       &lt;NA&gt;       \n 7     7 Aubrey       Rice        Buena Vista      7 Aubrey       Rice       \n 8     8 Ellie        Schmidt     Division         8 &lt;NA&gt;         Schmidt    \n 9     9 Robert       Garza       Red Rock         9 Bob          Garza      \n10    10 Stella       Daniels     Holland         10 Stella       &lt;NA&gt;       \n# ℹ 1 more variable: street_2 &lt;chr&gt;\n\n\nNow, our first attempt at creating the dummy variables might look something like this:\n\npeople %&gt;% \n  mutate(\n    name_first_match = name_first_1 == name_first_2,\n    name_last_match  = name_last_1 == name_last_2,\n    street_match     = street_1 == street_2\n  ) %&gt;% \n  # Order like columns next to each other for easier comparison\n  select(id_1, starts_with(\"name_f\"), starts_with(\"name_l\"), starts_with(\"s\"))\n\n# A tibble: 10 × 10\n    id_1 name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2     2 Elias        Elas         FALSE            Salazar     Salazar    \n 3     3 Colton       &lt;NA&gt;         NA               Fox         Fox        \n 4     4 Cameron      Cameron      TRUE             Warren      Waren      \n 5     5 Carson       Carsen       FALSE            Mills       Mills      \n 6     6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7     7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8     8 Ellie        &lt;NA&gt;         NA               Schmidt     Schmidt    \n 9     9 Robert       Bob          FALSE            Garza       Garza      \n10    10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\nLet’s take a moment to review the results we got. In row 1 we see that “Easton” and “Easton” match, and the value for name_first_match is TRUE. So far, so good. In row 2, we see that “Elias” and “Ela” do not match, and the value for name_first_match is FALSE. That is also the result we wanted. In row 3, we see that “Colton” and “NA” do not match; however, the value in name_first_match is NA. In this case, this is not the result we want. We have a problem. That brings us to the first step in this workflow.\n\n34.2.2.1 Spotting a need for a function\nIn some cases, the need is purely repetitive code – like the example at the beginning of this chapter. In other cases, like this one, a built-in R function is not giving the the desired result.\nHere is the basic problem in this particular case:\n\n1 == 1\n\n[1] TRUE\n\n\n\n1 == 2\n\n[1] FALSE\n\n\n\n1 == NA\n\n[1] NA\n\n\n\nNA == 2\n\n[1] NA\n\n\n\nNA == NA\n\n[1] NA\n\n\nThe equality operator (==) always returns NA when one, or both, of the values being tested is NA. Often, that is exactly the result we want. In this case, however, it is not. Fortunately, we can get the result we want by writing our own function. That brings us to step 2 in the workflow.\n\n\n34.2.2.2 Making the code work for one specific case\nDon’t try to solve the entire problem for every case right out of the gate. Instead, solve one problem for a specific case, and then build on that win! Let’s start by trying to figure out how to get the result we want for name_first_match in row 3 of our example data.\n\n\"Colton\" == NA\n\n[1] NA\n\n\nThis is essentially what we already had above. But, we want to change our result from NA to FALSE. Let’s start by saving the result to an object that we can manipulate:\n\nresult &lt;- \"Colton\" == NA\nresult\n\n[1] NA\n\n\nSo, now the value returned by the equality comparison is saved to an object named result. Let’s go ahead and use a conditional operation to change the value of result to FALSE when it is initially NA, and leave it alone otherwise:\n\nresult &lt;- \"Colton\" == NA\nresult &lt;- if_else(is.na(result), FALSE, result)\nresult\n\n[1] FALSE\n\n\nAlright! This worked! At least, it worked for this case. That brings us to step 3 in the workflow.\n\n\n34.2.2.3 Making the solution into a “function”\nHow can this be done? Well, first we start with a skeleton of the function components we discussed above. They are the function() function, the function body (wrapped in curly braces), and the function name. At the moment, we don’t have any arguments. We’ll explain why soon.\n\nis_match &lt;- function() {\n  \n}\n\nThen, we literally copy the solution from above and paste it into the function body, making sure to indent the code. Next, we need to run the code chunk to create the function. After doing so, you should see the function appear in your global environment. Keep in mind, this creates the function so that we can use it later, but the function isn’t immediately run.\n\nis_match &lt;- function() {\n  result &lt;- \"Colton\" == NA\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\nNow, let’s test out our shiny new function. To run the function, we can simply type the function name, with the parentheses, and run the code chunk.\n\nis_match()\n\n[1] FALSE\n\n\nAnd, it works! When we ask R to run a function we are really asking R to run the code in the body of the function. In this case, we know that the code in the body of the function results in the value FALSE because this results in FALSE:\n\nresult &lt;- \"Colton\" == NA\nresult &lt;- if_else(is.na(result), FALSE, result)\nresult\n\n[1] FALSE\n\n\nAnd all we did was stick that code in the function body. Said another way, this:\n\nresult &lt;- \"Colton\" == NA\nresult &lt;- if_else(is.na(result), FALSE, result)\nresult\n\nand this:\n\nis_match()\n\nmean essentially the same thing to R now if that makes sense. Hang in there even if it still isn’t quite clear. We’ll get more practice soon.\nAt this point, you may be wondering about the function arguments, and why there aren’t any. Well, we can try passing a value to our is_match() function. How about we pass the name “Easton” from the first row of our example data above:\n\nis_match(name = \"Easton\")\n\nError in is_match(name = \"Easton\"): unused argument (name = \"Easton\")\n\n\nBut, we get an error. R doesn’t know what the name argument is or what to do with the values we are passing to it. That’s because we never said anything about any arguments when we created the is_match() function. We left the parentheses where the function arguments go empty.\n\nis_match &lt;- function() {\n  result &lt;- \"Colton\" == NA\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\nLet’s create is_match() again, but this time, let’s add an argument:\n\nis_match &lt;- function(name) {\n  result &lt;- \"Colton\" == NA\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\n\nis_match(name = \"Easton\")\n\n[1] FALSE\n\n\nHmmm, let’s add another argument and see what happens:\n\nis_match &lt;- function(name_1, name_2) {\n  result &lt;- \"Colton\" == NA\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\n\nis_match(name_1 = \"Easton\", name_2 = \"Easton\")\n\n[1] FALSE\n\n\nIt looks as though the arguments we are adding don’t have any effect on our returned value. That’s because they don’t. We oversimplified how function arguments work just a little bit in our factory analogy earlier. When we add arguments to function our definition (i.e., when we create the function) it’s really more like adding a loading dock to our factory. It’s a place where our factory can receive raw materials. However, there still needs to be equipment inside the factory that can use those raw materials. If we drop off a load of rubber at our bicycle factory, but there’s no machine inside our bicycle factory that uses rubber, then we wouldn’t expect dropping off the rubber to have any effect on the outputs coming out of the factory.\nWe have similar situation above. We dropped the name “Easton” off at our is_match() function, but nothing inside our is_match() function can use the name “Easton”. There’s no machinery to plug that name into. That brings us to step 4 in the workflow.\n\n\n34.2.2.4 Start generalizing the function\nAs it stands right now, our is_match() function can’t accept any new names. The only result we will ever get from the current version of our is_match() function is the result of testing the equality between the values “Colton” and NA, and then converting that value to FALSE. This isn’t a problem if the only values we care about comparing are “Colton” and NA, but of course, that isn’t the case. We need a way to make our function work for other values too. Said another way, we need to make our function more general.\nAs you may have guessed already, that will require us creating an argument to receive input values and a place to use those input values in the function body. Let’s start by adding a first_name argument:\n\nis_match &lt;- function(first_name) {\n  result &lt;- first_name == NA\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\n\nis_match(first_name = \"Easton\")\n\n[1] FALSE\n\n\n👆Here’s what we did above:\n\nWe once again created our is_match() function. However, this time we created it with a single argument – first_name. We didn’t have to name the argument first_name. We could have named it anything that we can name any other variable in R. But, first_name seemed like a reasonable choice since the value we want to pass to this argument is a person’s first name. The first_name argument will receive the first name values that we want to pass to this function.\nWe replaced the constant value “Colton” in the function body with the variable first_name. It isn’t a coincidence that the name of the variable first_name matches the name of the argument first_name. R will take whatever value we give to the first_name argument and pass it to the variable with a matching name inside the function body. Then, R will run the code inside the function body as though the variable is the value we passed to it.\n\nSo, when we type:\n\nis_match(first_name = \"Easton\")\n\n[1] FALSE\n\n\nR sees:\n\nresult &lt;- \"Easton\" == NA\nresult &lt;- if_else(is.na(result), FALSE, result)\nresult\n\n[1] FALSE\n\n\nIt looks like our is_match() function is still going to return a value of FALSE no matter what value we pass to the first_name function. That’s because no matter what value we pass to result &lt;- first_name == NA, result will equal NA. Then, result &lt;- if_else(is.na(result), FALSE, result) will change the value of result to FALSE. So, we still need to make our function more general. As you may have guessed, we can do that by adding a second argument:\n\nis_match &lt;- function(first_name, first_name) {\n  result &lt;- first_name == first_name\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\nError: repeated formal argument 'first_name' (&lt;input&gt;:1:34)\n\n\nUh, oh! We got an error. This error is telling us that each function argument must have a unique name. Let’s try again:\n\nis_match &lt;- function(first_name_1, first_name_2) {\n  result &lt;- first_name_1 == first_name_2\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\n\nis_match(first_name_1 = \"Easton\", first_name_2 = \"Colton\")\n\n[1] FALSE\n\n\nIs this working or is our function still just returning FALSE no matter what we pass to the arguments? Let’s try to pass “Easton” to first_name_1 and first_name_2 and see what happens:\n\nis_match(first_name_1 = \"Easton\", first_name_2 = \"Easton\")\n\n[1] TRUE\n\n\nWe got a TRUE! That’s exactly the result we wanted! Let’s do one final check. Let’s see what happens when we pass NA to our is_match() function:\n\nis_match(first_name_1 = \"Easton\", first_name_2 = NA)\n\n[1] FALSE\n\n\nPerfect! It looks like our function is finally ready to help us solve the problem we identified way back at step one. But, while we are talking about generalizing our function, shouldn’t we go ahead and use more general names for our function arguments? We were only using first names when we were developing our function, but we are going to use our function to compare last names and street names as well. In fact, our function will compare any two values and tell us whether or not they are a match. So, let’s go ahead and change the argument names to value_1 and value_2:\n\nis_match &lt;- function(value_1, value_2) {\n  result &lt;- value_1 == value_2  # Don't forget to change the variable names here!!\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\nNow, we are ready to put our function to work testing whether or not the first name, last name, and street name match at each ID between our data frames:\n\npeople %&gt;% \n  mutate(\n    name_first_match = is_match(name_first_1, name_first_2),\n    name_last_match  = is_match(name_last_1, name_last_2),\n    street_match     = is_match(street_1, street_2)\n  ) %&gt;% \n  # Order like columns next to each other for easier comparison\n  select(id_1, starts_with(\"name_f\"), starts_with(\"name_l\"), starts_with(\"s\"))\n\n# A tibble: 10 × 10\n    id_1 name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2     2 Elias        Elas         FALSE            Salazar     Salazar    \n 3     3 Colton       &lt;NA&gt;         FALSE            Fox         Fox        \n 4     4 Cameron      Cameron      TRUE             Warren      Waren      \n 5     5 Carson       Carsen       FALSE            Mills       Mills      \n 6     6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7     7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8     8 Ellie        &lt;NA&gt;         FALSE            Schmidt     Schmidt    \n 9     9 Robert       Bob          FALSE            Garza       Garza      \n10    10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\nWorks like a charm! Notice, however, that we still have a lot of repetition in the code above. Unfortunately, we still don’t have all the tools we need to remove it. But, we will soon.\nAt this point in the chapter, the hope is that you’re developing a feel for how to write your own functions and why that might be useful. With R, it’s possible to write functions that are very complicated. But, hopefully, the examples above show you that functions don’t have to be complicated to be useful. In that spirit, we will not dive too much deeper into the details and technicalities of function writing at this point. However, there are a few details that should be at least mentioned so that you aren’t caught off guard by them as you begin to write your own functions. We will touch on each below, and then wrap up this chapter with resources for those of you who wish to dive deeper.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/writing_functions/writing_functions.html#giving-your-function-arguments-default-values",
    "href": "chapters/writing_functions/writing_functions.html#giving-your-function-arguments-default-values",
    "title": "34  Writing Functions",
    "section": "34.3 Giving your function arguments default values",
    "text": "34.3 Giving your function arguments default values\nWe’ve been introducing new functions to you all throughout the book so far. Each time, we try to discuss some, or all, of the function’s arguments – including the default values that are passed to the arguments. Most of you have probably developed some sort of intuitive understanding of just what it meant for the argument to have a default value. However, this seems like an appropriate point in the book to talk about default arguments a little more explicitly and show you how to add them to the functions you write.\nLet’s say that we want to write a function that will increase the value of a number, or set of numbers, incrementally. We may start with something like this:\n\nincrement &lt;- function(x) {\n  x + 1\n}\n\n👆Here’s what we did above:\n\nWe created our own function that will increase the value of a number, or set of numbers, incrementally. Specifically, when we pass a number to the x argument the value of that number plus one will be returned.\n\nLet’s go ahead and use our function now:\n\nincrement(2)\n\n[1] 3\n\n\n👆Here’s what we did above:\n\nWe passed the value 2 to the x argument of our increment() function. The x argument then passed the value 2 to the x variable in the function body. Said another way, R replaced the x variable in the function body with the value 2. Then, R executed the code in the function body. In this case, the code in the function body added the values 2 and 1 together. Finally, the function returned the value 3.\n\nBelieve it or not, our simple little increment() function is a full-fledged R function. It is just as legitimate as any other R function we’ve used in this book. But, let’s go ahead and add a little more to its functionality. For example, maybe we want to be able to increment by values other than just one. How might we do that?\nHopefully, your first thought was to replace the constant value 1 in the function body with a variable that can have any number passed to it. That’s exactly what we will do next:\n\nincrement &lt;- function(x, by) {\n  x + by\n}\n\n👆Here’s what we did above:\n\nWe created our own function that will increase the value of a number, or set of numbers, incrementally. Specifically, when we pass a number to the x argument the value of that number will be incremented by the value passed to the by argument.\n\nWhat value should increment() return if we pass 2 to the x argument and 2 to the by argument?\n\nincrement(2, 2)\n\n[1] 4\n\n\nHopefully, that’s what you were expecting. But, now what happens if we don’t pass any value to the by argument?\n\nincrement(2)\n\nError in increment(2): argument \"by\" is missing, with no default\n\n\nWe get an error saying that there wasn’t any value passed to the by argument, and the by argument doesn’t have a default value. But, we are really lazy, and it takes a lot of work to pass a value to the by argument every time we use the increment() function. Plus, we almost always only want to increment our numbers by one. In this case, our best course of action is to set the default value of by to 1. Fortunately for us, doing so is really easy!\n\nincrement &lt;- function(x, by = 1) {\n  x + by\n}\n\n👆Here’s what we did above:\n\nWe created our own function that will increase the value of a number, or set of numbers, incrementally. Specifically, when we pass a number to the x argument the value of that number will be incremented by the value passed to the by argument. The default value passed to the by argument is 1. Said another way, R will pretend that we passed the value 1 to the by argument if we don’t explicitly pass a number other than 1 to the by argument.\nAll we had to do to give by a default value was type = followed by the value (i.e., 1) when we created the function.\n\nNow let’s try out our latest version of increment():\n\n# Default value\nincrement(2)\n\n[1] 3\n\n\n\n# Passing the value 1\nincrement(2, 1)\n\n[1] 3\n\n\n\n# Passing a value other than 1\nincrement(2, 2)\n\n[1] 4\n\n\n\n# Passing a vector of numbers to the x argument\nincrement(c(1, 2, 3), 2)\n\n[1] 3 4 5",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/writing_functions/writing_functions.html#the-values-your-functions-return",
    "href": "chapters/writing_functions/writing_functions.html#the-values-your-functions-return",
    "title": "34  Writing Functions",
    "section": "34.4 The values your functions return",
    "text": "34.4 The values your functions return\nWhen we run our functions, they typically execute each line of code in the function body, one after another, starting with the first line and ending at the last line. Therefore, the value that your function returns (i.e., the thing that comes out of the factory) is typically dictated by the last line of code in your function body.\nTo explain this further, let’s take another look at our is_match() function:\n\nis_match &lt;- function(value_1, value_2) {\n  result &lt;- value_1 == value_2                     # Do this first\n  result &lt;- if_else(is.na(result), FALSE, result)  # Then this\n  result                                           # Then this\n}\n\nWhy did we type that third line of code? Afterall, that line of code isn’t doing anything. Well, let’s see what happens if we take it out:\n\nis_match &lt;- function(value_1, value_2) {\n  result &lt;- value_1 == value_2 \n  result &lt;- if_else(is.na(result), FALSE, result)\n}\n\n\nis_match(\"Easton\", \"Easton\")\n\nIt appears as though nothing happened! Did our function break?\nLet’s think about what typically happens when we use R’s built-in functions. When we don’t assign the value returned by the function to an object, then the returned value is printed to the screen:\n\nsum(1, 1)\n\n[1] 2\n\n\nBut, when we do assign the value returned by the function to an object, nothing is printed to the screen:\n\nx &lt;- sum(1, 1)\n\nThe same thing is happening in our function above. The last line of our function body is assigning a value (i.e., TRUE or FALSE) to the variable result. Just like x &lt;- sum(1, 1) didn’t print to the screen, result &lt;- if_else(is.na(result), FALSE, result) doesn’t print to the screen when we run is_match(\"Easton\", \"Easton\") using this version of is_match().\nHowever, we can see in the example below that result of the operations being executed inside the function body can still be assigned to an object in our global environment, and we can print the contents of that object to screen:\n\nx &lt;- is_match(\"Easton\", \"Easton\")\nx\n\n[1] TRUE\n\n\nIf all of that seems confusing, here is the bottom line. In general, it’s a best practice for your function to print its return value to the screen. You can do this in one of three ways:\n1️⃣ The value that results from the code in the last line of the function body isn’t assigned to anything. We saw an example of this above with our increment() function:\n\nincrement &lt;- function(x, by = 1) {\n  x + by # Last line doesn't assign the value to an object\n}\n\n\nincrement(2)\n\n[1] 3\n\n\n2️⃣ If you assign values to objects inside your function, then type the name of the object that contains the value you want your function to return on the last line of the function body. We saw an example of this with our is_match() function. We can also amend our increment() function follow this pattern:\n\nincrement &lt;- function(x, by = 1) {\n  out &lt;- x + by # Now we assign the value to an object\n  out           # Type object name on last line of the function body \n}\n\n\nincrement(2)\n\n[1] 3\n\n\n3️⃣ Use the return() function.\n\nincrement &lt;- function(x, by = 1) {\n  out &lt;- x + by \n  return(out)   \n}\n\n\nincrement(2)\n\n[1] 3\n\n\nSo, which method should you use? Well, for all but the simplest functions (like the one above) method 1 is not considered good coding practice. Method 3 may seem like it’s the most explicit; however, it’s actually considered best practice to use the return() function only when you want your function to return its value before R reaches the last line of the function body. For example, let’s add another line of code to our function body that adds another 1 to the value of out:\n\nincrement &lt;- function(x, by = 1) {\n  out &lt;- x + by \n  out &lt;- out + 1 # Adding an extra 1\n  return(out)    # Return still in the last line\n}\n\n\nincrement(2)\n\n[1] 4\n\n\nNow, let’s move return(out) to the second line of the function body – above the line of code that adds an additional 1 to the value of out:\n\nincrement &lt;- function(x, by = 1) {\n  out &lt;- x + by \n  return(out)    # Return in the second line above adding an extra 1\n  out &lt;- out + 1 # Adding an extra 1\n}\n\n\nincrement(2)\n\n[1] 3\n\n\nIn the example above, the last 1 wasn’t added to the value of out because we used the return() function. Said another way, increment() returned the value of out “early”, and the last line of the function body was never executed.\nIn the example above, using the return() function in the way that we did obviously makes no sense. It was just meant to illustrate what the return() function can do. The return() function doesn’t actually become useful until we start writing more complex functions. But, because the return() function has the special ability to end the execution of the function body early, it’s considered a best practice to only use it for that purpose.\nTherefore, in most situations, you will want to use method 2 (i.e., object name on last line) when writing your own functions.\nOne final note before we move on to the next section. Notice that we never used the print() function on the last line of our code. This was intentional. Using print() will give you the result you expect when you don’t assign the value that your function returns to an object in your global environment:\n\nincrement &lt;- function(x, by = 1) {\n  out &lt;- x + by \n  print(out)   \n}\n\n\nincrement(2)\n\n[1] 3\n\n\nBut, it will not give you the result you want if you do assign the value that your function returns to an object in your global environment:\n\nincrement &lt;- function(x, by = 1) {\n  out &lt;- x + by \n  print(out)   \n}\n\n\nx &lt;- increment(2)\n\n[1] 3\n\nx\n\n[1] 3",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/writing_functions/writing_functions.html#lexical-scoping-and-functions",
    "href": "chapters/writing_functions/writing_functions.html#lexical-scoping-and-functions",
    "title": "34  Writing Functions",
    "section": "34.5 Lexical scoping and functions",
    "text": "34.5 Lexical scoping and functions\nIf you have been following along with the code above on your computer, you may have noticed that the objects we create inside our functions do not appear in our global environment. If you haven’t been following along, you may want to jump on your computer really quickly for this section (or just take our word for it).\nThe reason the objects we created inside our functions do not appear in our global environment is that R actually has multiple environments were objects can live. Additionally, R uses something called lexical scoping rules to look for the objects you refer to in your R code. The vast majority of the time, we won’t need to concern ourselves much with any of these other environments or the lexical scoping rules. However, function writing does require us to have some minimal understanding of these concepts. At the very least, you should be aware of the following when writing your own functions:\n1️⃣ Objects we create inside of functions don’t live in our global environment and we can’t do anything with them outside of the function we created them in.\nIn the example below, we create an object named out inside of the increment() function:\n\nincrement &lt;- function(x, by = 1) {\n  out &lt;- x + by # Assign the value to the out object inside the function\n  out           \n}\n\nWe then use the function:\n\nx &lt;- increment(2)\nx\n\n[1] 3\n\n\nHowever, the out object is not available to us:\n\nout\n\nError: object 'out' not found\n\n\n2️⃣ If the function we write can’t find the object it’s looking for inside the function body, then it will try to find it in the global environment.\nFor example, let’s create a new function named add that adds the values of x and y together in its function body. Notice, however, that there is no y argument to pass a value to, and that y is never assigned a value inside of the add() function:\n\nadd &lt;- function(x) {\n  x + y\n}\n\nWhen we call the function:\n\nadd(2)\n\nError in add(2): object 'y' not found\n\n\nWe get an error. R can’t find the object y. Now let’s create a y object in our global environment:\n\ny &lt;- 100\n\nAnd call the add() function again:\n\nadd(2)\n\n[1] 102\n\n\nAs you can see, R wasn’t able to find a value for y inside of the function body so it looked outside of the function in the global environment. This is definitely something to be aware of, but usually isn’t an actual problem.\nFor starters, there is no obviously good reason to add a variable to your function body without assigning it a value inside the function body or matching it to a function argument. In other words, there’s generally no good reason to have variables that serve no purpose floating around inside your functions.\nIf you do assign it a value inside the function, then R will not look outside of the function for a value:\n\nadd &lt;- function(x) {\n  y &lt;- 1\n  x + y\n}\n\n\ny &lt;- 100\nadd(2)\n\n[1] 3\n\n\nLikewise, if you create the function with a matching argument, then R will not look outside of the function for a value:\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\n\ny &lt;- 100\nadd(2)\n\nError in add(2): argument \"y\" is missing, with no default\n\n\nAgain, this aspect of the lexical scoping rules is something to be aware of, but generally isn’t a problem in practice.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/writing_functions/writing_functions.html#tidy-evaluation",
    "href": "chapters/writing_functions/writing_functions.html#tidy-evaluation",
    "title": "34  Writing Functions",
    "section": "34.6 Tidy evaluation",
    "text": "34.6 Tidy evaluation\nNow that you have all the basics of function writing under your belt, let’s take look at what happens when we try to write functions that use tidyverse package functions in the function body.\nFor this section, let’s return to our study data we used for the first example in this chapter. As a reminder, here’s what the data looks like:\n\nstudy\n\n# A tibble: 68 × 7\n     age age_group       gender ht_in wt_lbs   bmi bmi_3cat  \n   &lt;dbl&gt; &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     \n 1    32 30 and Older    Male      70    216  31.0 Obese     \n 2    30 30 and Older    Female    63    106  18.8 Normal    \n 3    32 30 and Older    Female    62    145  26.5 Overweight\n 4    29 Younger than 30 Male      67    195  30.5 Obese     \n 5    24 Younger than 30 Female    67    143  22.4 Normal    \n 6    38 30 and Older    Female    58    125  26.1 Overweight\n 7    25 Younger than 30 Female    64    138  23.7 Normal    \n 8    24 Younger than 30 Male      69    140  20.7 Normal    \n 9    48 30 and Older    Male      65    158  26.3 Overweight\n10    29 Younger than 30 Male      68    167  25.4 Overweight\n# ℹ 58 more rows\n\n\nWe already calculated the number of missing values, mean, median, min, and max for all of the continuous variables. So, let’s go ahead and calculate the number and percent of observations for each level of our categorical variables.\nWe know that we have 3 categorical variables (i.e., age_group, gender, and bmi_3cat), and we know that we want to perform the same calculation on all of them. So, we decide to write our own function. Following the workflow we discussed earlier, our next step is to make the code work for one specific case:\n\nstudy %&gt;% \n  count(age_group) %&gt;% \n  mutate(percent = n / sum(n) * 100)\n\n# A tibble: 3 × 3\n  age_group           n percent\n  &lt;fct&gt;           &lt;int&gt;   &lt;dbl&gt;\n1 Younger than 30    56   82.4 \n2 30 and Older       11   16.2 \n3 &lt;NA&gt;                1    1.47\n\n\nGreat! Thanks to dplyr, we have the result we were looking for! The next step in the workflow is to make our solution into a function. Let’s copy and paste our solution into a function skeleton like we did before:\n\ncat_stats &lt;- function(var) {\n  study %&gt;% \n    count(age_group) %&gt;% \n    mutate(percent = n / sum(n) * 100)\n}\n\n\ncat_stats()\n\n# A tibble: 3 × 3\n  age_group           n percent\n  &lt;fct&gt;           &lt;int&gt;   &lt;dbl&gt;\n1 Younger than 30    56   82.4 \n2 30 and Older       11   16.2 \n3 &lt;NA&gt;                1    1.47\n\n\nSo far, so good! Now, let’s replace age_group with var in the function body to generalize our function:\n\ncat_stats &lt;- function(var) {\n  study %&gt;% \n    count(var) %&gt;% \n    mutate(percent = n / sum(n) * 100)\n}\n\n\ncat_stats(age_group)\n\nError in `count()`:\n! Must group by variables found in `.data`.\n✖ Column `var` is not found.\n\n\nUnfortunately, this doesn’t work. As we stated in the introduction to this part of the book, non-standard evaluation prevents us from using dplyr and other tidyverse packages inside of our functions in the same way that we might use other functions. Fortunately, the fix for this is pretty easy. All we need to do is embrace (i.e., wrap) the var variable with double curly braces:\n\ncat_stats &lt;- function(var) {\n  study %&gt;% \n    count({{ var }}) %&gt;% \n    mutate(percent = n / sum(n) * 100)\n}\n\n\ncat_stats(age_group)\n\n# A tibble: 3 × 3\n  age_group           n percent\n  &lt;fct&gt;           &lt;int&gt;   &lt;dbl&gt;\n1 Younger than 30    56   82.4 \n2 30 and Older       11   16.2 \n3 &lt;NA&gt;                1    1.47\n\n\nNow, we can use our new function on the rest of our categorical variables:\n\ncat_stats(gender)\n\n# A tibble: 3 × 3\n  gender     n percent\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;\n1 Female    43   63.2 \n2 Male      24   35.3 \n3 &lt;NA&gt;       1    1.47\n\n\n\ncat_stats(bmi_3cat)\n\n# A tibble: 4 × 3\n  bmi_3cat       n percent\n  &lt;fct&gt;      &lt;int&gt;   &lt;dbl&gt;\n1 Normal        43   63.2 \n2 Overweight    16   23.5 \n3 Obese          5    7.35\n4 &lt;NA&gt;           4    5.88\n\n\nThis is working beautifully! However, we should probably make one final adjustment to our cat_stats() function. Let’s say that we had another data frame with categorical variable we wanted to analyze:\n\nother_study &lt;- tibble(\n  id = 1:10,\n  age_group = c(rep(\"Younger\", 9), \"Older\"),\n) %&gt;% \n  print()\n\n# A tibble: 10 × 2\n      id age_group\n   &lt;int&gt; &lt;chr&gt;    \n 1     1 Younger  \n 2     2 Younger  \n 3     3 Younger  \n 4     4 Younger  \n 5     5 Younger  \n 6     6 Younger  \n 7     7 Younger  \n 8     8 Younger  \n 9     9 Younger  \n10    10 Older    \n\n\nNow, let’s pass age_group to our cat_stats() function again:\n\ncat_stats(age_group)\n\n# A tibble: 3 × 3\n  age_group           n percent\n  &lt;fct&gt;           &lt;int&gt;   &lt;dbl&gt;\n1 Younger than 30    56   82.4 \n2 30 and Older       11   16.2 \n3 &lt;NA&gt;                1    1.47\n\n\nIs that the result you expected? Hopefully not! That’s the same result we got from the original study data. Have you figured out why this happened? Take another look at our function definition:\n\ncat_stats &lt;- function(var) {\n  study %&gt;% \n    count({{ var }}) %&gt;% \n    mutate(percent = n / sum(n) * 100)\n}\n\nWe have the study data frame hard coded into the first line of the function body. In the same way we need a matching argument-variable pair to pass multiple different columns into our function, we need a matching argument-variable pair to pass multiple different data frames into our function. We start by adding an argument to accept the data frame:\n\ncat_stats &lt;- function(data, var) {\n  study %&gt;% \n    count({{ var }}) %&gt;% \n    mutate(percent = n / sum(n) * 100)\n}\n\nAgain, we could name this argument almost anything, but data seems like a reasonable choice. Then, we replace study with data in the function body to generalize our function:\n\ncat_stats &lt;- function(data, var) {\n  data %&gt;% \n    count({{ var }}) %&gt;% \n    mutate(percent = n / sum(n) * 100)\n}\n\nAnd now we can use our cat_stats() function on any data frame – including the other_study data frame we created above:\n\ncat_stats(other_study, age_group)\n\n# A tibble: 2 × 3\n  age_group     n percent\n  &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Older         1      10\n2 Younger       9      90\n\n\nWe can even use it with a pipe:\n\nother_study %&gt;% \n  cat_stats(age_group)\n\n# A tibble: 2 × 3\n  age_group     n percent\n  &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Older         1      10\n2 Younger       9      90\n\n\nSome of you may be wondering why we didn’t have to wrap data with double curly braces in the code above. Remember, we only have to use the curly braces with column names because of non-standard evaluation. More specifically, because of one aspect of non-standard evaluation called data masking. Data masking is what lets us refer to a column in a data frame without using dollar sign or bracket notation. For example, age_group doesn’t exist in our global environment as a standalone object:\n\nage_group\n\nError: object 'age_group' not found\n\n\nIt only exists as a part of (i.e. a column in) the other_study object:\n\nother_study$age_group\n\n [1] \"Younger\" \"Younger\" \"Younger\" \"Younger\" \"Younger\" \"Younger\" \"Younger\"\n [8] \"Younger\" \"Younger\" \"Older\"  \n\n\nBut the data frames themselves are not data masked. They do exist as standalone objects in our global environment:\n\nother_study\n\n# A tibble: 10 × 2\n      id age_group\n   &lt;int&gt; &lt;chr&gt;    \n 1     1 Younger  \n 2     2 Younger  \n 3     3 Younger  \n 4     4 Younger  \n 5     5 Younger  \n 6     6 Younger  \n 7     7 Younger  \n 8     8 Younger  \n 9     9 Younger  \n10    10 Older    \n\n\nTherefore, there is no need to wrap them with double curly braces. Having said that, it doesn’t appear as though doing so will hurt anything:\n\ncat_stats &lt;- function(data, var) {\n  {{data}} %&gt;% \n    count({{ var }}) %&gt;% \n    mutate(percent = n / sum(n) * 100)\n}\n\n\ncat_stats(other_study, age_group)\n\n# A tibble: 2 × 3\n  age_group     n percent\n  &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Older         1      10\n2 Younger       9      90\n\n\nThat pretty much wraps up this chapter on the basics of writing function to reduce unnecessary repetition in your R code. If you’re feeling good about writing your own functions, great! If you want to dig even deeper, take a look at the functions chapter of the Advanced R book.\nIf you’re still feeling a little apprehensive or confused, don’t feel bad. It takes most people (myself included) a while to get comfortable with writing functions. Just remember, functions can be complicated, but they don’t have to be. Even very simple functions can sometimes be useful. So, start simple and get more complex as your skills and confidence grow.\nIf you find that you’ve written a function that is really useful, consider saving it for use again in the future. One way is saving functions as R scripts in a folder on your computer that can then be copied and pasted from the scripts into R programs as needed.\nA much better way is using the source() function, which allows you to use use your saved functions without having to manually copy and paste them.\nAn even better way is learning how to make your own packages that contain groups of related functions and save them to your Github account. From there, you can use your functions on any computer, and even share them with others. Finally, you can even publish your packages on CRAN if you want to them with the broadest possible audience.\n\n\n\n\n1. Grolemund G, Wickham H. R for Data Science.; 2017.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Writing Functions</span>"
    ]
  },
  {
    "objectID": "chapters/dplyr_column_wise/dplyr_column_wise.html",
    "href": "chapters/dplyr_column_wise/dplyr_column_wise.html",
    "title": "35  Column-wise Operations in dplyr",
    "section": "",
    "text": "35.1 The across() function\nThroughout the chapters in this book we have learned to do a really vast array of useful data transformations and statistical analyses with the help of the dplyr package.\nSo far, however, we’ve always done these transformations and statistical analyses on one column of our data frame at a time. There isn’t anything inherently “wrong” with this approach, but, for reasons we’ve already discussed, there are often advantages to telling R what you want to do one time, and then asking R to do that thing repeatedly across all, or a subset of, the columns in your data frame. That is exactly what dplyr’s across() function allows us to do.\nThere are so many ways we might want to use the across() function in our R programs. We can’t begin to cover, or even imagine, them all. Instead, the goal of this chapter is just to provide you with an overview of the across() function and show you some examples of using it with filter(), mutate(), and summarise() to get you thinking about how you might want to use it in your R programs.\nBefore we discuss further, let’s take a look at a quick example. The first thing we will need to do is load dplyr.\nThen, we will simulate some data. In this case, we are creating a data frame that contains three columns of 10 random numbers:\nUp to this point, if we wanted to find the mean of each column, we would probably have written code like this:\nWith the help of the across() function, we can now get the mean of each column like this:\nNow, you might ask why this is a better approach. Fair question.\nIn this case, using across() doesn’t actually reduce the number of lines of code we wrote. In fact, we wrote two additional lines when we used the across() function. However, imagine if we added 20 additional columns to our data frame. Using the first approach, we would have to write 20 additional lines of code inside the summarise() function. Using the across() approach, we wouldn’t have to add any additional code at all. We would simply update the value we pass to the .cols argument.\nPerhaps more importantly, did you notice that we “accidentally” forgot to replace y with z when we copied and pasted z_mean = mean(y) in the code chunk for the first approach? If not, go back and take a look. That mistake is fairly easy to catch and fix in this very simple example. But, in real-world projects, mistakes like this are easy to make, and not always so easy to catch. We are much less likely to make similar mistakes when we use across().\nThe across() function is part of the dplyr package. We will always use across() inside of one of the dplyr verbs we’ve been learning about. Specifically, mutate(), and summarise(). We will not use across() outside of the dplyr verbs. Additionally, we will always use across() within the context of a data frame (as opposed to a vector, matrix, or some other data structure).\nTo view the help documentation for across(), you can copy and paste ?dplyr::across into your R console. If you do, you will see that across() has four arguments. They are:\n1️⃣.cols. The value we pass to this argument should be columns of the data frame we want to operate on. We can once again use tidy-select argument modifiers here. In the example above, we used c(x:z) to tell R that we wanted to operate on columns x through z (inclusive). If we had also wanted the mean of the row column for some reason, we could have used the everything() tidy-select modifier to tell R that we wanted to operate on all of the columns in the data frame.\n2️⃣.fns. This is where you tell across() what function, or functions, you want to apply to the columns you selected in .cols. In the example above, we passed the mean function to the .fns argument. Notice that we typed mean without the parentheses (i.e., mean()).\n3️⃣.... In this case, the ... argument is where we pass any additional arguments to the function we passed to the .fns argument. For example, we passed the mean function to the .fns argument above. In the data frame above, none of the columns had any missing values. Let’s go ahead and add some missing values so that we can take a look at how ... works in across().\ndf_xyz$x[2] &lt;- NA_real_\ndf_xyz$y[4] &lt;- NA_real_\ndf_xyz$z[6] &lt;- NA_real_\ndf_xyz\n\n# A tibble: 10 × 4\n     row       x      y      z\n   &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 -0.560   1.22  -1.07 \n 2     2 NA       0.360 -0.218\n 3     3  1.56    0.401 -1.03 \n 4     4  0.0705 NA     -0.729\n 5     5  0.129  -0.556 -0.625\n 6     6  1.72    1.79  NA    \n 7     7  0.461   0.498  0.838\n 8     8 -1.27   -1.97   0.153\n 9     9 -0.687   0.701 -1.14 \n10    10 -0.446  -0.473  1.25\nAs we’ve already seen many times, R won’t drop the missing values and carry out a complete case analysis by default:\ndf_xyz %&gt;% \n  summarise(\n    x_mean = mean(x),\n    y_mean = mean(y),\n    z_mean = mean(y)\n  )\n\n# A tibble: 1 × 3\n  x_mean y_mean z_mean\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     NA     NA     NA\nInstead, we have to explicitly tell R to carry out a complete case analysis. We can do so by filtering our rows with missing data (more on this later) or by changing the value of the mean() function’s na.rm argument from FALSE (the default) to TRUE:\ndf_xyz %&gt;% \n  summarise(\n    x_mean = mean(x, na.rm = TRUE),\n    y_mean = mean(y, na.rm = TRUE),\n    z_mean = mean(z, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 3\n  x_mean y_mean z_mean\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1  0.108  0.220 -0.284\nWhen we use across(), we will need to pass the na.rm = TRUE to the mean() function in across()’s ... argument like this:\ndf_xyz %&gt;%\n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = mean,\n      na.rm  = TRUE, # Passing na.rm = TRUE to the ... argument\n      .names = \"{col}_mean\"\n    )\n  )\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(.cols = everything(), .fns = mean, na.rm = TRUE, .names\n  = \"{col}_mean\")`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 1 × 4\n  row_mean x_mean y_mean z_mean\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1      5.5  0.108  0.220 -0.284\nNotice that we do not actually type out ... = or anything like that.\n4️⃣.names. You can use this argument to adjust the column names that will result from the operation you pass to .fns. In the example above, we used the special {cols} keyword to use each of the column names that were passed to the .cols argument as the first part of each of the new columns’ names. Then, we asked R to add a literal underscore and the word “mean” because these are all mean values. That resulted in the new column names you see above. The default value for .names is just {cols}. So, if we hadn’t modified the value passed to the .names argument, our results would have looked like this:\ndf_xyz %&gt;%\n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = mean,\n      na.rm  = TRUE\n    )\n  )\n\n# A tibble: 1 × 4\n    row     x     y      z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1   5.5 0.108 0.220 -0.284\nThere is also a special {fn} keyword that we can use to pass the name of each of the functions we used in .fns as part of the new column names. However, in order to get {fn} to work the way we want it to, we have to pass a list of name-function pairs to the .fns argument. We’ll explain further.\nFirst, we will keep the code exactly as it was, but replace “mean” with “{fn}” in the .names argument:\ndf_xyz %&gt;%\n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = mean, \n      na.rm  = TRUE,\n      .names = \"{col}_{fn}\"\n    )\n  )\n\n# A tibble: 1 × 4\n  row_1   x_1   y_1    z_1\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1   5.5 0.108 0.220 -0.284\nThis is not the result we wanted. Because, we didn’t name the function that we passed to .fns, across() essentially used “function number 1” as its name. In order to get the result we want, we need to pass a list of name-function pairs to the .fns argument like this:\ndf_xyz %&gt;% \n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = list(mean = mean),\n      na.rm  = TRUE,\n      .names = \"{col}_{fn}\"\n    )\n  )\n\n# A tibble: 1 × 4\n  row_mean x_mean y_mean z_mean\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1      5.5  0.108  0.220 -0.284\nAlthough it may not be self-evident from just looking at the code above, the first mean in the list(mean = mean) name-function pair is a name that we are choosing to be passed to the new column names. Theoretically, we could have picked any name. For example:\ndf_xyz %&gt;% \n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = list(r4epi = mean),\n      na.rm  = TRUE,\n      .names = \"{col}_{fn}\"\n    )\n  )\n\n# A tibble: 1 × 4\n  row_r4epi x_r4epi y_r4epi z_r4epi\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       5.5   0.108   0.220  -0.284\nThe second mean in the list(mean = mean) name-function pair is the name of the actual function we want to apply to the columns in .cols. This part of the name-function pair must be the name of the function that we actually want to apply to the columns in .cols. Otherwise, we will get an error:\ndf_xyz %&gt;% \n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = list(mean = r4epi),\n      na.rm  = TRUE,\n      .names = \"{col}_{fn}\"\n    )\n  )\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nCaused by error:\n! object 'r4epi' not found\nAn additional advantage of passing a list of name-function pairs to the .fns argument is that we can pass multiple functions at once. For example, let’s say that we want the minimum and maximum value of each column in our data frame. Without across() we might do that analysis like this:\ndf_xyz %&gt;% \n  summarise(\n    x_min = min(x, na.rm = TRUE),\n    x_max = max(x, na.rm = TRUE),\n    y_min = min(y, na.rm = TRUE),\n    y_max = max(y, na.rm = TRUE),\n    z_min = min(z, na.rm = TRUE),\n    z_max = max(z, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 6\n  x_min x_max y_min y_max z_min z_max\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -1.27  1.72 -1.97  1.79 -1.14  1.25\nBut, we can simply pass min and max as a list of name-function pairs if we use across():\ndf_xyz %&gt;% \n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = list(min = min, max = max),\n      na.rm  = TRUE,\n      .names = \"{col}_{fn}\"\n    )\n  )\n\n# A tibble: 1 × 8\n  row_min row_max x_min x_max y_min y_max z_min z_max\n    &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1      10 -1.27  1.72 -1.97  1.79 -1.14  1.25\nHow great is that?!?\nSo, we’ve seen how to pass an individual function to the .fns argument and we’ve seen how to pass a list containing multiple functions to the .fns argument. There is actually a third syntax for passing functions to the .fns argument. The across() documentation calls it “a purrr-style lambda”. This can be a little bit confusing, so I’m going to show you an example, and then walk through it step by step.\ndf_xyz %&gt;% \n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = ~ mean(.x, na.rm = TRUE),\n      .names = \"{col}_mean\"\n    )\n  )\n\n# A tibble: 1 × 4\n  row_mean x_mean y_mean z_mean\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1      5.5  0.108  0.220 -0.284\nThe purrr-style lambda always begins with the tilde symbol (~). Then we type out a function call behind the tilde symbol. We place the special .x symbol inside the function call where we would normally want to type the name of the column we want the function to operate on. The across() function will then substitute each column name we passed to the .cols argument for .x sequentially. In the example above, there isn’t really any good reason to use this syntax. However, this syntax can be useful at times. We will see some examples below.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Column-wise Operations in dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/dplyr_column_wise/dplyr_column_wise.html#across-with-mutate",
    "href": "chapters/dplyr_column_wise/dplyr_column_wise.html#across-with-mutate",
    "title": "35  Column-wise Operations in dplyr",
    "section": "35.2 Across with mutate",
    "text": "35.2 Across with mutate\nWe’ve already seen a number of examples of manipulating columns of our data frames using the mutate() function. In this section, we are going to take a look at two examples where using the across() function inside mutate() will allow us to apply the same manipulation to multiple columns in our data frame at once.\nLet’s go ahead and simulate the same demographics data frame we simulated for the recoding missing section of the conditional operations chapter. Let’s also add two new columns: a four-category education column and a six-category income column. For all columns except id and age, a value of 7 represents “Don’t know” and a value of 9 represents “refused.”\n\nset.seed(123)\ndemographics &lt;- tibble(\n  id       = 1:10,\n  age      = c(sample(1:30, 9, TRUE), NA),\n  race     = c(1, 2, 1, 4, 7, 1, 2, 9, 1, 3),\n  hispanic = c(7, 0, 1, 0, 1, 0, 1, 9, 0, 1),\n  edu_4cat = c(4, 2, 9, 1, 2, 3, 4, 9, 3, 3),\n  inc_6cat = c(1, 4, 1, 1, 5, 3, 2, 2, 7, 9)\n) %&gt;% \n  print()\n\n# A tibble: 10 × 6\n      id   age  race hispanic edu_4cat inc_6cat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1    15     1        7        4        1\n 2     2    19     2        0        2        4\n 3     3    14     1        1        9        1\n 4     4     3     4        0        1        1\n 5     5    10     7        1        2        5\n 6     6    18     1        0        3        3\n 7     7    22     2        1        4        2\n 8     8    11     9        9        9        2\n 9     9     5     1        0        3        7\n10    10    NA     3        1        3        9\n\n\nWhen working with data like this, it’s common to want to recode all the 7’s and 9’s to NA’s. We saw how to do that one column at a time already:\n\ndemographics %&gt;% \n  mutate(\n    race     = if_else(race == 7 | race == 9, NA_real_, race),\n    hispanic = if_else(race == 7 | hispanic == 9, NA_real_, hispanic),\n    edu_4cat = if_else(edu_4cat == 7 | edu_4cat == 9, NA_real_, edu_4cat)\n  )\n\n# A tibble: 10 × 6\n      id   age  race hispanic edu_4cat inc_6cat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1    15     1        7        4        1\n 2     2    19     2        0        2        4\n 3     3    14     1        1       NA        1\n 4     4     3     4        0        1        1\n 5     5    10    NA       NA        2        5\n 6     6    18     1        0        3        3\n 7     7    22     2        1        4        2\n 8     8    11    NA       NA       NA        2\n 9     9     5     1        0        3        7\n10    10    NA     3        1        3        9\n\n\n🚩In the code chunk above, we have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition from our code.\nAlso, did you notice that we forgot to replace race with hispanic in hispanic = if_else(race == 7 | hispanic == 9, NA_real_, hispanic)? This time, we didn’t write “forgot” in quotes because we really did forget and only noticed it later. In this case, the error caused a value of 1 to be recoded to NA in the hispanic column. These typos we’ve been talking about really do happen – even to me!\nHere’s how we can use across() in this situation:\n\ndemographics %&gt;% \n  mutate(\n    across(\n      .cols = c(-id, -age),\n      .fns  = ~ if_else(.x == 7 | .x == 9, NA_real_, .x)\n    )\n  )\n\n# A tibble: 10 × 6\n      id   age  race hispanic edu_4cat inc_6cat\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1    15     1       NA        4        1\n 2     2    19     2        0        2        4\n 3     3    14     1        1       NA        1\n 4     4     3     4        0        1        1\n 5     5    10    NA        1        2        5\n 6     6    18     1        0        3        3\n 7     7    22     2        1        4        2\n 8     8    11    NA       NA       NA        2\n 9     9     5     1        0        3       NA\n10    10    NA     3        1        3       NA\n\n\n👆Here’s what we did above:\n\nWe used a purrr-style lambda to replace 7’s and 9’s in all columns in our data frame, except id and age, with NA.\nRemember, the special .x symbol is just shorthand for each column passed to the .cols argument.\n\nAs another example, let’s say that we are once again working with data from a drug trial that includes a list of side effects for each person:\n\nset.seed(123)\ndrug_trial &lt;- tibble(\n  id           = 1:10,\n  se_headache  = sample(0:1, 10, TRUE),\n  se_diarrhea  = sample(0:1, 10, TRUE),\n  se_dry_mouth = sample(0:1, 10, TRUE),\n  se_nausea    = sample(0:1, 10, TRUE)\n) %&gt;% \n print()\n\n# A tibble: 10 × 5\n      id se_headache se_diarrhea se_dry_mouth se_nausea\n   &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;     &lt;int&gt;\n 1     1           0           1            0         0\n 2     2           0           1            1         1\n 3     3           0           1            0         0\n 4     4           1           0            0         1\n 5     5           0           1            0         1\n 6     6           1           0            0         0\n 7     7           1           1            1         0\n 8     8           1           0            1         0\n 9     9           0           0            0         0\n10    10           0           0            1         1\n\n\nNow, we want to create a factor version of each of the side effect columns. We’ve already learned how to do so one column at a time:\n\ndrug_trial %&gt;% \n  mutate(\n    se_headache_f  = factor(se_headache, 0:1, c(\"No\", \"Yes\")),\n    se_diarrhea_f  = factor(se_diarrhea, 0:1, c(\"No\", \"Yes\")),\n    se_dry_mouth_f = factor(se_dry_mouth, 0:1, c(\"No\", \"Yes\"))\n  )\n\n# A tibble: 10 × 8\n      id se_headache se_diarrhea se_dry_mouth se_nausea se_headache_f\n   &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;     &lt;int&gt; &lt;fct&gt;        \n 1     1           0           1            0         0 No           \n 2     2           0           1            1         1 No           \n 3     3           0           1            0         0 No           \n 4     4           1           0            0         1 Yes          \n 5     5           0           1            0         1 No           \n 6     6           1           0            0         0 Yes          \n 7     7           1           1            1         0 Yes          \n 8     8           1           0            1         0 Yes          \n 9     9           0           0            0         0 No           \n10    10           0           0            1         1 No           \n# ℹ 2 more variables: se_diarrhea_f &lt;fct&gt;, se_dry_mouth_f &lt;fct&gt;\n\n\n🚩Once again, we have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition from our code. Here’s how we can use across() to do so:\n\ndrug_trial %&gt;% \n  mutate(\n    across(\n      .cols  = starts_with(\"se\"),\n      .fns   = ~ factor(.x, 0:1, c(\"No\", \"Yes\")),\n      .names = \"{col}_f\"\n    )\n  )\n\n# A tibble: 10 × 9\n      id se_headache se_diarrhea se_dry_mouth se_nausea se_headache_f\n   &lt;int&gt;       &lt;int&gt;       &lt;int&gt;        &lt;int&gt;     &lt;int&gt; &lt;fct&gt;        \n 1     1           0           1            0         0 No           \n 2     2           0           1            1         1 No           \n 3     3           0           1            0         0 No           \n 4     4           1           0            0         1 Yes          \n 5     5           0           1            0         1 No           \n 6     6           1           0            0         0 Yes          \n 7     7           1           1            1         0 Yes          \n 8     8           1           0            1         0 Yes          \n 9     9           0           0            0         0 No           \n10    10           0           0            1         1 No           \n# ℹ 3 more variables: se_diarrhea_f &lt;fct&gt;, se_dry_mouth_f &lt;fct&gt;,\n#   se_nausea_f &lt;fct&gt;\n\n\n👆Here’s what we did above:\n\nWe used a purrr-style lambda to create a factor version of all the side effect columns in our data frame.\nWe used the .names argument to add an “_f” to the end of the new column names.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Column-wise Operations in dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/dplyr_column_wise/dplyr_column_wise.html#across-with-summarise",
    "href": "chapters/dplyr_column_wise/dplyr_column_wise.html#across-with-summarise",
    "title": "35  Column-wise Operations in dplyr",
    "section": "35.3 Across with summarise",
    "text": "35.3 Across with summarise\nLet’s return to the ehr data frame we used in the chapter on working with character strings for our first example of using across() inside of summarise.\nYou may click here to download this file to your computer.\n\n# We will need here, readr and stringr in the examples below\nlibrary(readr)\nlibrary(stringr)\nlibrary(here)\n\n\n# Read in the data\nehr &lt;- read_rds(\"ehr.Rds\")\n\nFor this example, the only column we will concern ourselves with is the symptoms column:\n\nsymptoms &lt;- ehr %&gt;% \n  select(symptoms) %&gt;% \n  print()\n\n# A tibble: 15 × 1\n   symptoms                            \n   &lt;chr&gt;                               \n 1 \"\\\"Pain\\\", \\\"Headache\\\", \\\"Nausea\\\"\"\n 2 \"Pain\"                              \n 3 \"Pain\"                              \n 4 \"\\\"Nausea\\\", \\\"Headache\\\"\"          \n 5 \"\\\"Pain\\\", \\\"Headache\\\"\"            \n 6 \"\\\"Nausea\\\", \\\"Headache\\\"\"          \n 7 \"Pain\"                              \n 8  &lt;NA&gt;                               \n 9 \"Pain\"                              \n10  &lt;NA&gt;                               \n11 \"\\\"Nausea\\\", \\\"Headache\\\"\"          \n12 \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\"\n13 \"Headache\"                          \n14 \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\"\n15  &lt;NA&gt;                               \n\n\nYou may recall that we created dummy variables for each symptom like this:\n\nsymptoms &lt;- symptoms %&gt;% \n  mutate(\n    pain     = str_detect(symptoms, \"Pain\"),\n    headache = str_detect(symptoms, \"Headache\"),\n    nausea   = str_detect(symptoms, \"Nausea\")\n  ) %&gt;% \n  print()\n\n# A tibble: 15 × 4\n   symptoms                             pain  headache nausea\n   &lt;chr&gt;                                &lt;lgl&gt; &lt;lgl&gt;    &lt;lgl&gt; \n 1 \"\\\"Pain\\\", \\\"Headache\\\", \\\"Nausea\\\"\" TRUE  TRUE     TRUE  \n 2 \"Pain\"                               TRUE  FALSE    FALSE \n 3 \"Pain\"                               TRUE  FALSE    FALSE \n 4 \"\\\"Nausea\\\", \\\"Headache\\\"\"           FALSE TRUE     TRUE  \n 5 \"\\\"Pain\\\", \\\"Headache\\\"\"             TRUE  TRUE     FALSE \n 6 \"\\\"Nausea\\\", \\\"Headache\\\"\"           FALSE TRUE     TRUE  \n 7 \"Pain\"                               TRUE  FALSE    FALSE \n 8  &lt;NA&gt;                                NA    NA       NA    \n 9 \"Pain\"                               TRUE  FALSE    FALSE \n10  &lt;NA&gt;                                NA    NA       NA    \n11 \"\\\"Nausea\\\", \\\"Headache\\\"\"           FALSE TRUE     TRUE  \n12 \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\" TRUE  TRUE     TRUE  \n13 \"Headache\"                           FALSE TRUE     FALSE \n14 \"\\\"Headache\\\", \\\"Pain\\\", \\\"Nausea\\\"\" TRUE  TRUE     TRUE  \n15  &lt;NA&gt;                                NA    NA       NA    \n\n\n\n\n\n\n\n\nNote\n\n\n\nSome of you may have noticed that we repeated ourselves more than twice in the code chunk above and thought about using across() to remove it. Unfortunately, across() won’t solve our problem in this situation. We will need some of the tools that we learn about in later chapters if we want to remove this repetition.\n\n\nAnd finally, we used the table() function to get a count of how many people reported having a headache:\n\ntable(symptoms$headache)\n\n\nFALSE  TRUE \n    4     8 \n\n\nThis is where the example stopped in the chapter on working with character strings. However, what if we wanted to know how many people reported the other symptoms as well? Well, we could repeatedly call the table() function:\n\ntable(symptoms$pain)\n\n\nFALSE  TRUE \n    4     8 \n\n\n\ntable(symptoms$nausea)\n\n\nFALSE  TRUE \n    6     6 \n\n\nBut, that would cause us to copy and paste repeatedly. Additionally, wouldn’t it be nice to view these counts in a way that makes them easier to compare? One solution would be to use summarise() like this:\n\nsymptoms %&gt;% \n  summarise(\n    had_headache = sum(headache, na.rm = TRUE),\n    had_pain     = sum(pain, na.rm = TRUE),\n    had_nausea   = sum(nausea, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 3\n  had_headache had_pain had_nausea\n         &lt;int&gt;    &lt;int&gt;      &lt;int&gt;\n1            8        8          6\n\n\nThis works, but we can do better with across():\n\nsymptoms %&gt;% \n  summarise(\n    across(\n      .cols  = c(headache, pain, nausea),\n      .fns   = ~ sum(.x, na.rm = TRUE)\n    )\n  )\n\n# A tibble: 1 × 3\n  headache  pain nausea\n     &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1        8     8      6\n\n\nGreat! But, wouldn’t it be nice to know the proportion of people with each symptom as well? You may recall that R treats TRUE and FALSE as 1 and 0 when used in a mathematical operation. Additionally, you may already be aware that the mean of a set of 1’s and 0’s is equal to the proportion of 1’s in the set. For example, there are three ones and three zeros in the set (1, 1, 1, 0, 0, 0). The proportion of 1’s in the set is 3 out of 6, which is 0.5. Equivalently, the mean value of the set is (1 + 1 + 1 + 0 + 0 + 0) / 6, which equals 3 / 6, which is 0.5. So, when we have dummy variables like headache, pain, and nausea above, passing them to the mean() function returns the proportion of TRUE values. In this case, the proportion of people who had each symptom. We know we can do that calculation like this:\n\nsymptoms %&gt;% \n  summarise(\n    had_headache = mean(headache, na.rm = TRUE),\n    had_pain     = mean(pain, na.rm = TRUE),\n    had_nausea   = mean(nausea, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 3\n  had_headache had_pain had_nausea\n         &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1        0.667    0.667        0.5\n\n\nAs before, we can do better with the across() function like this:\n\nsymptoms %&gt;% \n  summarise(\n    across(\n      .cols = c(pain, headache, nausea),\n      .fns  = ~ mean(.x, na.rm = TRUE)\n    )\n  )\n\n# A tibble: 1 × 3\n   pain headache nausea\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 0.667    0.667    0.5\n\n\nNow, at this point, we might think, “wouldn’t it be nice to see the count and the proportion in the same result?” Well, we can do that by supplying our purrr-style lambdas as functions in a list of name-function pairs like this:\n\nsymptom_summary &lt;- symptoms %&gt;% \n  summarise(\n    across(\n      .cols = c(pain, headache, nausea),\n      .fns  = list(\n        count = ~ sum(.x, na.rm = TRUE),\n        prop  = ~ mean(.x, na.rm = TRUE)\n      )\n    )\n  ) %&gt;% \n  print()\n\n# A tibble: 1 × 6\n  pain_count pain_prop headache_count headache_prop nausea_count nausea_prop\n       &lt;int&gt;     &lt;dbl&gt;          &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;       &lt;dbl&gt;\n1          8     0.667              8         0.667            6         0.5\n\n\nIn this case, it’s probably fine to stop here. But, what if we had 20 or 30 symptoms that we were analyzing? It would be really difficult to read and compare them arranged horizontally like this, wouldn’t it?\nDo you recall us discussing restructuring our results in the chapter on restructuring data frames? This is a circumstance where we might want to use pivot_longer() to make our results easier to read and interpret:\n\nsymptom_summary %&gt;% \n  tidyr::pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"symptom\", \".value\"),\n    names_sep = \"_\"\n  )\n\n# A tibble: 3 × 3\n  symptom  count  prop\n  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n1 pain         8 0.667\n2 headache     8 0.667\n3 nausea       6 0.5  \n\n\nThere! Isn’t that result much easier to read?\nFor our final example of this section, let’s return the first example from the writing functions chapter. We started with some simulated study data:\n\nstudy &lt;- tibble(\n  age       = c(32, 30, 32, 29, 24, 38, 25, 24, 48, 29, 22, 29, 24, 28, 24, 25, \n                25, 22, 25, 24, 25, 24, 23, 24, 31, 24, 29, 24, 22, 23, 26, 23, \n                24, 25, 24, 33, 27, 25, 26, 26, 26, 26, 26, 27, 24, 43, 25, 24, \n                27, 28, 29, 24, 26, 28, 25, 24, 26, 24, 26, 31, 24, 26, 31, 34, \n                26, 25, 27, NA),\n  age_group = c(2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n                1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, \n                1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, \n                2, 1, 1, 1, NA),\n  gender    = c(2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, \n                1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, \n                1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, \n                1, 1, 2, 1, NA),\n  ht_in     = c(70, 63, 62, 67, 67, 58, 64, 69, 65, 68, 63, 68, 69, 66, 67, 65, \n                64, 75, 67, 63, 60, 67, 64, 73, 62, 69, 67, 62, 68, 66, 66, 62, \n                64, 68, NA, 68, 70, 68, 68, 66, 71, 61, 62, 64, 64, 63, 67, 66, \n                69, 76, NA, 63, 64, 65, 65, 71, 66, 65, 65, 71, 64, 71, 60, 62, \n                61, 69, 66, NA),\n  wt_lbs    = c(216, 106, 145, 195, 143, 125, 138, 140, 158, 167, 145, 297, 146, \n                125, 111, 125, 130, 182, 170, 121, 98, 150, 132, 250, 137, 124, \n                186, 148, 134, 155, 122, 142, 110, 132, 188, 176, 188, 166, 136, \n                147, 178, 125, 102, 140, 139, 60, 147, 147, 141, 232, 186, 212, \n                110, 110, 115, 154, 140, 150, 130, NA, 171, 156, 92, 122, 102, \n                163, 141, NA),\n  bmi       = c(30.99, 18.78, 26.52, 30.54, 22.39, 26.12, 23.69, 20.67, 26.29, \n                25.39, 25.68, 45.15, 21.56, 20.17, 17.38, 20.8, 22.31, 22.75, \n                26.62, 21.43, 19.14, 23.49, 22.66, 32.98, 25.05, 18.31, 29.13, \n                27.07, 20.37, 25.01, 19.69, 25.97, 18.88, 20.07, NA, 26.76, \n                26.97, 25.24, 20.68, 23.72, 24.82, 23.62, 18.65, 24.03, 23.86, \n                10.63, 23.02, 23.72, 20.82, 28.24, NA, 37.55, 18.88, 18.3, \n                19.13, 21.48, 22.59, 24.96, 21.63, NA, 29.35, 21.76, 17.97, \n                22.31, 19.27, 24.07, 22.76, NA),\n  bmi_3cat  = c(3, 1, 2, 3, 1, 2, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, \n                1, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, NA, 2, 2, 2, 1, 1, 1, 1, \n                1, 1, 1, 1, 1, 1, 1, 2, NA, 3, 1, 1, 1, 1, 1, 1, 1, NA, 2, 1, \n                1, 1, 1, 1, 1, NA)\n) %&gt;% \n  mutate(\n    age_group = factor(age_group, labels = c(\"Younger than 30\", \"30 and Older\")),\n    gender    = factor(gender, labels = c(\"Female\", \"Male\")),\n    bmi_3cat  = factor(bmi_3cat, labels = c(\"Normal\", \"Overweight\", \"Obese\"))\n  ) %&gt;% \n  print()\n\n# A tibble: 68 × 7\n     age age_group       gender ht_in wt_lbs   bmi bmi_3cat  \n   &lt;dbl&gt; &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     \n 1    32 30 and Older    Male      70    216  31.0 Obese     \n 2    30 30 and Older    Female    63    106  18.8 Normal    \n 3    32 30 and Older    Female    62    145  26.5 Overweight\n 4    29 Younger than 30 Male      67    195  30.5 Obese     \n 5    24 Younger than 30 Female    67    143  22.4 Normal    \n 6    38 30 and Older    Female    58    125  26.1 Overweight\n 7    25 Younger than 30 Female    64    138  23.7 Normal    \n 8    24 Younger than 30 Male      69    140  20.7 Normal    \n 9    48 30 and Older    Male      65    158  26.3 Overweight\n10    29 Younger than 30 Male      68    167  25.4 Overweight\n# ℹ 58 more rows\n\n\nAnd wrote our own function to calculate the number of missing values, mean, median, min, and max for all of the continuous variables:\n\ncontinuous_stats &lt;- function(var) {\n  study %&gt;% \n    summarise(\n      n_miss = sum(is.na({{ var }})),\n      mean   = mean({{ var }}, na.rm = TRUE),\n      median = median({{ var }}, na.rm = TRUE),\n      min    = min({{ var }}, na.rm = TRUE),\n      max    = max({{ var }}, na.rm = TRUE)\n    )\n}\n\nWe then used that function to calculate our statistics of interest for each continuous variable:\n\ncontinuous_stats(age)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1  26.9     26    22    48\n\n\n\ncontinuous_stats(ht_in)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      3  66.0     66    58    76\n\n\n\ncontinuous_stats(wt_lbs)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      2  148.   142.    60   297\n\n\n\ncontinuous_stats(bmi)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      4  23.6   22.9  10.6  45.2\n\n\nThis is definitely an improvement over all the copying and pasting we were doing before we wrote our own function. However, there is still some unnecessary repetition above. One way we can remove this repetition is to use across() like this:\n\nsummary_stats &lt;- study %&gt;% \n  summarise(\n    across(\n      .cols = c(age, ht_in, wt_lbs, bmi),\n      .fns  = list(\n        n_miss = ~ sum(is.na(.x)),\n        mean   = ~ mean(.x, na.rm = TRUE),\n        median = ~ median(.x, na.rm = TRUE),\n        min    = ~ min(.x, na.rm = TRUE),\n        max    = ~ max(.x, na.rm = TRUE)\n      )\n    ) \n  ) %&gt;% \n  print()\n\n# A tibble: 1 × 20\n  age_n_miss age_mean age_median age_min age_max ht_in_n_miss ht_in_mean\n       &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;int&gt;      &lt;dbl&gt;\n1          1     26.9         26      22      48            3       66.0\n# ℹ 13 more variables: ht_in_median &lt;dbl&gt;, ht_in_min &lt;dbl&gt;, ht_in_max &lt;dbl&gt;,\n#   wt_lbs_n_miss &lt;int&gt;, wt_lbs_mean &lt;dbl&gt;, wt_lbs_median &lt;dbl&gt;,\n#   wt_lbs_min &lt;dbl&gt;, wt_lbs_max &lt;dbl&gt;, bmi_n_miss &lt;int&gt;, bmi_mean &lt;dbl&gt;,\n#   bmi_median &lt;dbl&gt;, bmi_min &lt;dbl&gt;, bmi_max &lt;dbl&gt;\n\n\nThis method works, but it has the same problem that our symptom summaries had above. Our results are hard to read and interpret because they are arranged horizontally. We can once again pivot this data longer, but it won’t be quite as easy as it was before. Our first attempt might look like this:\n\nsummary_stats %&gt;% \n  tidyr::pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"characteristic\", \".value\"),\n    names_sep = \"_\"\n  )\n\nWarning: Expected 2 pieces. Additional pieces discarded in 12 rows [1, 6, 7, 8, 9, 10,\n11, 12, 13, 14, 15, 16].\n\n\n# A tibble: 12 × 8\n   characteristic     n  mean median   min   max  `in`   lbs\n   &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 age                1  26.9   26    22    48    NA     NA \n 2 ht                NA  NA     NA    NA    NA     3     NA \n 3 ht                NA  NA     NA    NA    NA    66.0   NA \n 4 ht                NA  NA     NA    NA    NA    66     NA \n 5 ht                NA  NA     NA    NA    NA    58     NA \n 6 ht                NA  NA     NA    NA    NA    76     NA \n 7 wt                NA  NA     NA    NA    NA    NA      2 \n 8 wt                NA  NA     NA    NA    NA    NA    148.\n 9 wt                NA  NA     NA    NA    NA    NA    142.\n10 wt                NA  NA     NA    NA    NA    NA     60 \n11 wt                NA  NA     NA    NA    NA    NA    297 \n12 bmi                4  23.6   22.9  10.6  45.2  NA     NA \n\n\nWhat do you think the problem is here?\nWell, we passed an underscore to the names_sep argument. This tells pivot_longer() that that character string on the left side of the underscore should make up the values of the new characteristic column and each unique character string on the right side of the underscore should be used to create a new column name. In the symptoms data, this worked fine because all of the column names followed this pattern (e.g., pain_count and pain_prop). But, do the column names in summary_stats always follow this pattern? What about age_n_miss and ht_in_n_miss? All the extra underscores in the column names makes this pattern ineffective.\nThere are probably many ways we could address this problem. We think the most straightforward way is probably to go back to the code we used to create summary_stats and use the .names argument to separate the column name and statistic name with a character other than an underscore. Maybe a hyphen instead:\n\nsummary_stats &lt;- study %&gt;% \n  summarise(\n    across(\n      .cols  = c(age, ht_in, wt_lbs, bmi),\n      .fns   = list(\n        n_miss = ~ sum(is.na(.x)),\n        mean   = ~ mean(.x, na.rm = TRUE),\n        median = ~ median(.x, na.rm = TRUE),\n        min    = ~ min(.x, na.rm = TRUE),\n        max    = ~ max(.x, na.rm = TRUE)\n      ),\n      .names = \"{col}-{fn}\" # This is the new part of the code\n    ) \n  ) %&gt;% \n  print()\n\n# A tibble: 1 × 20\n  `age-n_miss` `age-mean` `age-median` `age-min` `age-max` `ht_in-n_miss`\n         &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;          &lt;int&gt;\n1            1       26.9           26        22        48              3\n# ℹ 14 more variables: `ht_in-mean` &lt;dbl&gt;, `ht_in-median` &lt;dbl&gt;,\n#   `ht_in-min` &lt;dbl&gt;, `ht_in-max` &lt;dbl&gt;, `wt_lbs-n_miss` &lt;int&gt;,\n#   `wt_lbs-mean` &lt;dbl&gt;, `wt_lbs-median` &lt;dbl&gt;, `wt_lbs-min` &lt;dbl&gt;,\n#   `wt_lbs-max` &lt;dbl&gt;, `bmi-n_miss` &lt;int&gt;, `bmi-mean` &lt;dbl&gt;,\n#   `bmi-median` &lt;dbl&gt;, `bmi-min` &lt;dbl&gt;, `bmi-max` &lt;dbl&gt;\n\n\nNow, we can simply pass a hyphen to the names_sep argument to pivot_longer():\n\nsummary_stats %&gt;% \n  tidyr::pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"characteristic\", \".value\"),\n    names_sep = \"-\"\n  )\n\n# A tibble: 4 × 6\n  characteristic n_miss  mean median   min   max\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age                 1  26.9   26    22    48  \n2 ht_in               3  66.0   66    58    76  \n3 wt_lbs              2 148.   142.   60   297  \n4 bmi                 4  23.6   22.9  10.6  45.2\n\n\nLook at how much easier those results are to read!\n\nrm(study, summary_stats, continuous_stats)",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Column-wise Operations in dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/dplyr_column_wise/dplyr_column_wise.html#across-with-filter",
    "href": "chapters/dplyr_column_wise/dplyr_column_wise.html#across-with-filter",
    "title": "35  Column-wise Operations in dplyr",
    "section": "35.4 Across with filter",
    "text": "35.4 Across with filter\nWe’ve already discussed complete case analysis multiple times in this book. That is, including only the rows from our data frame that don’t have any missing values in our analysis. Additionally, we’ve already seen how we can use the filter() function to remove the rows of a single column where the data are missing. For example:\n\ndf_xyz %&gt;% \n  filter(!is.na(x))\n\n# A tibble: 9 × 4\n    row       x      y      z\n  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 -0.560   1.22  -1.07 \n2     3  1.56    0.401 -1.03 \n3     4  0.0705 NA     -0.729\n4     5  0.129  -0.556 -0.625\n5     6  1.72    1.79  NA    \n6     7  0.461   0.498  0.838\n7     8 -1.27   -1.97   0.153\n8     9 -0.687   0.701 -1.14 \n9    10 -0.446  -0.473  1.25 \n\n\nNotice that row 2 – the row that had a missing value for x – is no longer in the data frame, and we can now easily calculate the mean value of x.\n\ndf_xyz %&gt;% \n  filter(!is.na(x)) %&gt;% \n  summarise(mean = mean(x))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 0.108\n\n\nHowever, we want to remove the rows that have a missing value in any column – not just x. We could get this result using multiple sequential filter() functions like this:\n\ndf_xyz %&gt;% \n  filter(!is.na(x)) %&gt;% \n  filter(!is.na(y)) %&gt;% \n  filter(!is.na(z))\n\n# A tibble: 7 × 4\n    row      x      y      z\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 -0.560  1.22  -1.07 \n2     3  1.56   0.401 -1.03 \n3     5  0.129 -0.556 -0.625\n4     7  0.461  0.498  0.838\n5     8 -1.27  -1.97   0.153\n6     9 -0.687  0.701 -1.14 \n7    10 -0.446 -0.473  1.25 \n\n\nAs you can see, rows 2, 4, and 6 – the rows with a missing value for x, y, and z – were dropped.\n🚩Of course, in the code chunk above, we have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition from our code.\nAt this point in the book, our first thought might be to use the across() function, inside the filter() function, to remove all of the rows rows with missing values from our data frame. However, as of dplyr version 1.0.4, using the across() function inside of filter() is deprecated. That means we shouldn’t use it anymore. Instead, we should use the if_any() or if_all() functions, which take the exact same arguments as across(). In the code chunk below, we will show you how to solve this problem, then we will dissect the solution below.\n\ndf_xyz %&gt;% \n  filter(\n    if_all(\n      .cols = c(x:z),\n      .fns  = ~ !is.na(.x)\n    )\n  )\n\n# A tibble: 7 × 4\n    row      x      y      z\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 -0.560  1.22  -1.07 \n2     3  1.56   0.401 -1.03 \n3     5  0.129 -0.556 -0.625\n4     7  0.461  0.498  0.838\n5     8 -1.27  -1.97   0.153\n6     9 -0.687  0.701 -1.14 \n7    10 -0.446 -0.473  1.25 \n\n\n👆Here’s what we did above:\n\nYou can type ?dplyr::if_any or ?dplyr::if_all into your R console to view the help documentation for this function and follow along with the explanation below.\nWe used the if_all() function inside of the filter() function to keep only the rows in our data frame that had nonmissing values for all of the columns x, y, and z.\nWe passed the value c(x:z) to the .cols argument. This told R to apply the function passed to the .fns argument to the columns x through z inclusive.\nWe used a purrr-style lambda to test whether or not each value of each of the columns passed to .cols is NOT missing.\nRemember, the special .x symbol is just shorthand for each column passed to the .cols argument.\n\nSo, how does this work? Well, first let’s remember that the is.na() function returns TRUE when the value of the vector passed to it is missing and FALSE when it is not missing. For example:\n\nis.na(df_xyz$x)\n\n [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nWe can then use the ! operator to “flip” those results. In other words, to return TRUE when the value of the vector passed to it is not missing and FALSE when it is missing. For example:\n\n!is.na(df_xyz$x)\n\n [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nThe filter() function] then returns the rows from the data frame where the values returned by !is.na() are TRUE and drops the rows where they are FALSE. For example, we can copy and paste the TRUE/FALSE values above to keep only the rows with nonmissing values for x:\n\ndf_xyz %&gt;% \n  filter(c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE))\n\n# A tibble: 9 × 4\n    row       x      y      z\n  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 -0.560   1.22  -1.07 \n2     3  1.56    0.401 -1.03 \n3     4  0.0705 NA     -0.729\n4     5  0.129  -0.556 -0.625\n5     6  1.72    1.79  NA    \n6     7  0.461   0.498  0.838\n7     8 -1.27   -1.97   0.153\n8     9 -0.687   0.701 -1.14 \n9    10 -0.446  -0.473  1.25 \n\n\nNow, let’s repeat this process for the columns y and z as well.\n\n!is.na(df_xyz$y)\n\n [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\n!is.na(df_xyz$z)\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nNext, let’s stack these results next to each other to make them even easier to view.\n\nnot_missing &lt;- tibble(\n  row = 1:10,\n  x   = !is.na(df_xyz$x),\n  y   = !is.na(df_xyz$y),\n  z   = !is.na(df_xyz$z)\n) %&gt;% \n  print()\n\n# A tibble: 10 × 4\n     row x     y     z    \n   &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n 1     1 TRUE  TRUE  TRUE \n 2     2 FALSE TRUE  TRUE \n 3     3 TRUE  TRUE  TRUE \n 4     4 TRUE  FALSE TRUE \n 5     5 TRUE  TRUE  TRUE \n 6     6 TRUE  TRUE  FALSE\n 7     7 TRUE  TRUE  TRUE \n 8     8 TRUE  TRUE  TRUE \n 9     9 TRUE  TRUE  TRUE \n10    10 TRUE  TRUE  TRUE \n\n\n👆Here’s what we did above:\n\nWe created a data frame that contains the value TRUE in each position where df_xyz has a nonmissing value and FALSE in each position where df_xyz has a missing value. We wouldn’t typically create this for our data analysis. We just created it here for teaching purposes.\n\nYou can think of the data frame of TRUE and FALSE values above as an intermediate product that if_any() and if_all() uses “under the hood” to decide which rows to keep. We think using this data frame as a conceptual model makes it a little easier to understand how if_any() and if_all() differ.\nif_any() will keep the rows where any value of x, y, or z are TRUE. In this case, there is at least one TRUE value in every row. Therefore, we would expect if_any() to return all rows in our data frame. And, that’s exactly what happens.\n\ndf_xyz %&gt;% \n  filter(\n    if_any(\n      .cols = c(x:z),\n      .fns  = ~ !is.na(.x)\n    )\n  )\n\n# A tibble: 10 × 4\n     row       x      y      z\n   &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 -0.560   1.22  -1.07 \n 2     2 NA       0.360 -0.218\n 3     3  1.56    0.401 -1.03 \n 4     4  0.0705 NA     -0.729\n 5     5  0.129  -0.556 -0.625\n 6     6  1.72    1.79  NA    \n 7     7  0.461   0.498  0.838\n 8     8 -1.27   -1.97   0.153\n 9     9 -0.687   0.701 -1.14 \n10    10 -0.446  -0.473  1.25 \n\n\nOn the other hand, if_all() will the keep the rows where all value of x, y, and z are TRUE. In this case, there is at least one FALSE value in rows 2, 4, and 6. Therefore, we would expect if_all() to return all rows in our data frame except rows 2, 4, and 6. That’s exactly what happens, and it’s exaclty the result we want.\n\ndf_xyz %&gt;% \n  filter(\n    if_all(\n      .cols = c(x:z),\n      .fns  = ~ !is.na(.x)\n    )\n  )\n\n# A tibble: 7 × 4\n    row      x      y      z\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1 -0.560  1.22  -1.07 \n2     3  1.56   0.401 -1.03 \n3     5  0.129 -0.556 -0.625\n4     7  0.461  0.498  0.838\n5     8 -1.27  -1.97   0.153\n6     9 -0.687  0.701 -1.14 \n7    10 -0.446 -0.473  1.25 \n\n\nBecause this is a small, simple example, using if_all() doesn’t actually reduce the number of lines of code we wrote. But again, try to imagine if we added 20 additional columns to our data frame. We would only need to update the value we pass to the .cols argument. This makes our code more concise, easier to maintain, and less error-prone.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Column-wise Operations in dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/dplyr_column_wise/dplyr_column_wise.html#summary",
    "href": "chapters/dplyr_column_wise/dplyr_column_wise.html#summary",
    "title": "35  Column-wise Operations in dplyr",
    "section": "35.5 Summary",
    "text": "35.5 Summary\nWe are big fans of using across(), if_any(), and if_all() in conjunction with the dplyr verbs. They allows us to remove a lot of the unnecessary repetition from our code in a way that integrates pretty seamlessly with the tools we are already using. Perhaps you will see value in using these functions as well. In the next chapter, we will learn about using for loops to remove unnecessary repetition from our code.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Column-wise Operations in dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/writing_for_loops/writing_for_loops.html",
    "href": "chapters/writing_for_loops/writing_for_loops.html",
    "title": "36  Writing For Loops",
    "section": "",
    "text": "36.1 How to write for loops\nIn this third chapter on repeated operations, we are going to discuss writing for loops.\nIn other documents you read, you may see for loops referred to as iterative processing, iterative operations, iteration, or just loops. Regardless of what you call them, for loops are not unique to R. Many if not all statistical software applications allow users to write for loops; although, the exact words and symbols used to construct them may differ slightly from one program to another.\nLet’s take a look at an example. After seeing a working example, we will take the code apart iteratively (do you see what we did there? 😆) to figure out how it works.\nWe’ll start by simulating some data. This is the same data we simulated at the beginning of the chapter on column-wise operations in dplyr. It’s a data frame that contains three columns of 10 random numbers:\nAs we previously discussed, if we wanted to find the mean of each column before learning about repeated operations, we would probably have written code like this:\nIn the previous chapter, we learned how to use the across() function to remove unnecessary repetition from our code like this:\nAn alternative approach that would also work is to use a for loop like this:\nMost people would agree that the for loop code is a little more complicated looking, and it’s a little bit harder to quickly glance at it and figure out what’s going on. It may even be a little bit intimidating for some of you.\nAlso, note that the result from the code that uses the across() function is a data frame with three columns and one row. The result from the code that uses a for loop is a character vector with three elements.\nFor the particular case above, we prefer to use the across() function instead of a for loop. However, as we will see below, there are some challenges that can be overcome with for loops that cannot currently be overcome with the across() function. But, before we jump into more examples, let’s take a look at the basic structure of the for loop.\nFor starters, using for loops in practice will generally require us to write code for two separate structures: An object to contain the results of our for loop and the for loop itself.\nTwo structures required for a for loop\nIn practice, we will generally write the code for structure 1 before writing the code for structure 2. However, it will be easier to understand why we need structure 1 if we first learn about the components of the for loop, and how they work together. Further, it will likely be easiest to understand the components of the for loop if we start on the inside and work our way out. Therefore, the first component of for loops that we are going to discuss is the body.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Writing For Loops</span>"
    ]
  },
  {
    "objectID": "chapters/writing_for_loops/writing_for_loops.html#how-to-write-for-loops",
    "href": "chapters/writing_for_loops/writing_for_loops.html#how-to-write-for-loops",
    "title": "36  Writing For Loops",
    "section": "",
    "text": "36.1.1 The for loop body\n\n\n\n\n\nBody of the for loop\n\n\n\n\nSimilar to when we learned to write our own functions, the body of the for loop is where all the “stuff” happens. This is where we write the code that we want to be executed over and over. In our example, we want the mean value of the x column, the mean value of the y column, and the mean value of the z column of our data frame called df_xyz. We can do that manually like this using dollar sign notation:\n\nmean(df_xyz$x)\n\n[1] 0.07462564\n\nmean(df_xyz$y)\n\n[1] 0.208622\n\nmean(df_xyz$z)\n\n[1] -0.4245589\n\n\nOr, we’ve also learned how to get the same result using bracket notation:\n\nmean(df_xyz[[\"x\"]])\n\n[1] 0.07462564\n\nmean(df_xyz[[\"y\"]])\n\n[1] 0.208622\n\nmean(df_xyz[[\"z\"]])\n\n[1] -0.4245589\n\n\nIn the code above, we used the quoted column names inside the double brackets. However, we could have also used each column’s position inside the double brackets. In other words, we can use 1 to refer to the x column because it is the first column in the data frame, we can use 2 to refer to the y column because it is the second column in the data frame, and we can use 3 to refer to the z column because it is the third column in the data frame:\n\nmean(df_xyz[[1]])\n\n[1] 0.07462564\n\nmean(df_xyz[[2]])\n\n[1] 0.208622\n\nmean(df_xyz[[3]])\n\n[1] -0.4245589\n\n\nFor reasons that will become clearer later, this will actually be the syntax we want to use inside of our for loop.\nNotice, however, the we copied the same code more than twice above. For all of the reasons we’ve already discussed, we would like to just type mean(df_xyz[[ # ]] once and have R fill in the number inside the double brackets for us, one after the other. As you’ve probably guessed, that’s exactly what the for loop does.\n\n\n36.1.2 The for() function\nAll for loops start with the for() function. This is how you tell R that you are about to write a for loop.\n\n\n\n\n\nThe for() function\n\n\n\n\nIn the examples in this book, the arguments to the for() function are generally going to follow this pattern:\n\n\n\n\n\nPattern of the for() function arguments\n\n\n\n\n1️⃣An index variable, which is also sometimes called a “counter,” to the left of the keyword in.\n2️⃣The keyword in.\n3️⃣The name of the object we want to loop (or iterate) over — often passed to theseq_along() function.\nIt can be a little intimidating to look at, but that’s the basic structure. We will talk about all three arguments simultaneously because they all work together, and we will get an error if we are missing any one of them:\n\n# No index variable\nfor(in 1) {\n  print(i)\n}\n\nError in parse(text = input): &lt;text&gt;:2:5: unexpected 'in'\n1: # No index variable\n2: for(in\n       ^\n\n\n\n# No keyword \"in\"\nfor(i 1) {\n  print(i)\n}\n\nError in parse(text = input): &lt;text&gt;:2:7: unexpected numeric constant\n1: # No keyword \"in\"\n2: for(i 1\n         ^\n\n\n\n# No object to loop over\nfor(i in ) {\n  print(i)\n}\n\nError in parse(text = input): &lt;text&gt;:2:10: unexpected ')'\n1: # No object to loop over\n2: for(i in )\n            ^\n\n\nSo, what happens when we do have all three of these components? Well, the index variable will take on each value of the object to loop over iteratively (i.e., one at a time). If there is only one object to loop over, this is how R sees the index variable inside of the loop:\n\nfor(i in 1) {\n  print(i)\n}\n\n[1] 1\n\n\nIf there are multiple objects to loop over, this is how R sees the index variable inside of the loop:\n\nfor(i in c(1, 2, 3)) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nNotice that the values being printed out are not a single numeric vector with three elements (e.g. [1] 1, 2, 3) like the object we started with to the right of the keyword in. Instead, three vectors with one element each are being printed out. One for 1 (i.e., [1] 1), one for 2 (i.e., [1] 2), and one for 3 (i.e., [1] 3). This is pointed out because it illustrates the iterative nature of a for loop. The index variable doesn’t take on the values of the object to the right of the keyword in simultaneously. It takes them on iteratively, or separately, one after the other.\nFurther, it may not be immediately obvious at this point, but that’s the basic “magic” of the for loop. The index variable changes once for each element of whatever object is on the right side of the keyword in. Even the most complicated for loops generally start from this basic idea.\nNote that the index variable does not have to be the letter i. It can be any letter:\n\nfor(x in c(1, 2, 3)) {\n  print(x)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nOr even a word:\n\nfor(number in c(1, 2, 3)) {\n  print(number)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nHowever, i is definitely the most common letter to use as the index variable and we suggest that you also use it in most cases. It’s just what people will expect to see and easily understand.\nNow, let’s discuss the object to the right of the keyword in. In all of the examples above, we passed a vector to the right of the keyword in. As you saw, when there is a vector to the right of the keyword in, the index variable takes on the value of each element of the vector. However, the object to the right of the keyword in does not have to be a vector. In fact, it will often be a data frame.\nWhen we ask the for loop to iterate over a data frame, what value do you think the index variable will take? The value of each cell of the data frame? The name or number of each column? The name or number of each row? Let’s see:\n\nfor(i in df_xyz) {\n  print(i)\n}\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n [1]  1.2240818  0.3598138  0.4007715  0.1106827 -0.5558411  1.7869131\n [7]  0.4978505 -1.9666172  0.7013559 -0.4727914\n [1] -1.0678237 -0.2179749 -1.0260044 -0.7288912 -0.6250393 -1.6866933\n [7]  0.8377870  0.1533731 -1.1381369  1.2538149\n\n\nIt may not be totally obvious to you, but inside the for loop above, the index variable took on three separate vectors of values – one for each column in the data frame. Of course, getting the mean value of each of these vectors is equivalent to getting the mean value of each column in our data frame. Remember, data frame columns are vectors. So, let’s replace the print() function with the mean() function in the for loop body and see what happens:\n\nfor(i in df_xyz) {\n  mean(i)\n}\n\nHmmm, it doesn’t seem as though anything happened. This is probably a good time to mention a little peculiarity about using for loops. As you can see in the example above, the return value of functions, and the contents of objects, referenced inside of the for loop body will not be printed to the screen unless we explicitly pass them to the print() function:\n\nfor(i in df_xyz) {\n  print(mean(i))\n}\n\n[1] 0.07462564\n[1] 0.208622\n[1] -0.4245589\n\n\nIt worked! This is the exact same answer we got above. And, if all we want to do is print the mean values of x, y, and z to the screen, then we could stop here and call it a day. However, we often want to save our analysis results to an object. In the chapter on using column-wise operations with dplyr, we saved our summary statistics to an object in the usual way (i.e., with the assignment arrow):\n\nxyz_means &lt;- df_xyz %&gt;%\n  summarise(\n    across(\n      .cols  = everything(),\n      .fns   = mean,\n      .names = \"{col}_mean\"\n    )\n  )\n\nFrom there, we can manipulate the results, save the results to a file, or print them to screen:\n\nxyz_means\n\n# A tibble: 1 × 3\n  x_mean y_mean z_mean\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0746  0.209 -0.425\n\n\nAt first, it may seem as though we can assign the results of our for loop to an object in a similar way:\n\nxyz_means &lt;- for(i in df_xyz) {\n  mean(i)\n}\n\n\nxyz_means\n\nNULL\n\n\nUnfortunately, this doesn’t work. Instead, we need to create an object that can store the results of our for loop. Then, we update (i.e., add to) that object at each iteration of the for loop. That brings us back to structure number 1.\n\n\n\n\n\nTwo structures required for a for loop\n\n\n\n\nBecause the result of our for loop will be three numbers – the mean of x, the mean of y, and the mean of z – the most straightforward object to store them in is a numeric vector with a length of three (i.e., three “slots”). We can use the vector() function to create an empty vector:\n\nmy_vec &lt;- vector()\nmy_vec\n\nlogical(0)\n\n\nAs you can see, by default, the vector() function creates a logical vector with length zero. We can change the vector type to numeric by passing \"numeric\" to the mode argument of the vector() function. We can also change the length to 3 by passing 3 to the length argument of the vector() function, and because we know we want this vector to hold the mean values of x, y, and z, let’s name it xyz_means:\n\nxyz_means &lt;- vector(\"numeric\", 3)\nxyz_means\n\n[1] 0 0 0\n\n\nFinally, let’s update xyz_means inside our for loop body:\n\nfor(i in df_xyz) {\n  xyz_means &lt;- mean(i)\n}\n\n\nxyz_means\n\n[1] -0.4245589\n\n\nHmmm, we’re getting closer, but that obviously still isn’t the result we want. Below, we attempt to illustrate what’s going on inside our loop.\nR starts executing at the top of the for loop. In the first iteration, the value of i is set to a numeric vector with the same values as the x column in df_xyz. Then, the i in mean(i) inside the for loop body is replaced with those numeric values. Then, the mean of those numeric values is calculated and assigned to the object named xyz_means.\n\n\n\n\n\nIllustration of a for loop process - I\n\n\n\n\nAt this point, there is no more code left to execute inside of the for loop, so R returns to the top of the loop.\n\n\n\n\n\nIllustration of a for loop process - II\n\n\n\n\ni has not yet taken every value of the object to the right of the keyword in, so R starts another iteration of the for loop. In the second iteration, the value of i is set to a numeric vector with the same values as the y column in df_xyz. Then, the i in mean(i) inside the for loop body is replaced with those numeric values. Then, the mean of those numeric values is calculated and assigned to the object named xyz_means.\n\n\n\n\n\nIllustration of a for loop process - III\n\n\n\n\nAt this point, there is no more code left to execute inside of the for loop, so R returns to the top of the loop.\n\n\n\n\n\nIllustration of a for loop process - IV\n\n\n\n\ni still has not yet taken every value of the object to the right of the keyword in, so R starts another iteration of the for loop. In the third iteration, the value of i is set to a numeric vector with the same values as the z column in df_xyz. Then, the i in mean(i) inside the for loop body is replaced with those numeric values. Then, the mean of those numeric values is calculated and assigned to the object named xyz_means.\n\n\n\n\n\nIllustration of a for loop process - V\n\n\n\n\nAt this point, there is no more code left to execute inside of the for loop, so R returns to the top of the loop. However, this time, i has taken every value of the object to the right of the keyword in, so R does not start another iteration. It leaves the looping process, and the value of xyz_means remains -0.4245589 – The result we got above.\nYou might be thinking, “wait, we made three slots in the xyz_means vector. Why does it only contain one number?” Well, remember that all we have to do to overwrite one object with another object is to assign the second object to the same name. For example, let’s create a vector with three values called my_vec:\n\nmy_vec &lt;- c(1, 2, 3)\nmy_vec\n\n[1] 1 2 3\n\n\nNow, let’s assign another value to my_vec:\n\nmy_vec &lt;- -0.4245589\nmy_vec\n\n[1] -0.4245589\n\n\nAs you can see, assignment (&lt;-) doesn’t add to the vector, it overwrites (i.e., replaces) the vector. That’s exactly what was happening inside of our for loop. To R, it basically looked like this:\n\nxyz_means &lt;- 0.07462564\nxyz_means &lt;- 0.208622\nxyz_means &lt;- -0.4245589\nxyz_means\n\n[1] -0.4245589\n\n\nWhat we really want is to create the empty vector:\n\nxyz_means &lt;- vector(\"numeric\", 3)\nxyz_means\n\n[1] 0 0 0\n\n\nAnd then add a value to each slot in the vector. Do you remember how to do this?\nWe can do this using bracket notation:\n\nxyz_means[[1]] &lt;- 0.07462564\nxyz_means[[2]] &lt;- 0.208622\nxyz_means[[3]] &lt;- -0.4245589\nxyz_means\n\n[1]  0.07462564  0.20862200 -0.42455890\n\n\nThat’s exactly the result we want.\nDoes that code above remind you of any other code we’ve already seen? How about this code:\n\nmean(df_xyz[[1]])\n\n[1] 0.07462564\n\nmean(df_xyz[[2]])\n\n[1] 0.208622\n\nmean(df_xyz[[3]])\n\n[1] -0.4245589\n\n\nHmmm, what if we combine the two? First, let’s once again create our empty vector, and then try combining the two code chunks above to fill it:\n\nxyz_means &lt;- vector(\"numeric\", 3)\nxyz_means\n\n[1] 0 0 0\n\n\n\nxyz_means[[1]] &lt;- mean(df_xyz[[1]])\nxyz_means[[2]] &lt;- mean(df_xyz[[2]])\nxyz_means[[3]] &lt;- mean(df_xyz[[3]])\nxyz_means\n\n[1]  0.07462564  0.20862196 -0.42455887\n\n\nAgain, that’s exactly the result we want. Of course, there is still unnecessary repetition. If you look at the code carefully, you may notice that the only thing that changes from line to line is the number inside the double brackets. So, if we could just type xyz_means[[ # ]] &lt;- mean(df_xyz[[ # ]]) once, and update the number inside the double brackets, we should be able to get the result we want. We’ve actually already seen how to do that with a for loop too. Remember this for loop for the very beginning of the chapter:\n\nfor(i in c(1, 2, 3)) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nThat looks promising, right? Let’s once again create our empty vector, and then try combining the two code chunks above to fill it:\n\nxyz_means &lt;- vector(\"numeric\", 3)\nxyz_means\n\n[1] 0 0 0\n\n\n\nfor(i in c(1, 2, 3)) {\n  xyz_means[[i]] &lt;- mean(df_xyz[[i]])\n}\n\n\nxyz_means\n\n[1]  0.07462564  0.20862196 -0.42455887\n\n\nIt works! We have used a for loop to successfully remove the unnecessary repetition from our code. However, there’s still something we could do to make the code more robust. In the for loop above, we knew that we needed three iterations. Therefore, we passed c(1, 2, 3) as the object to the right of the keyword in. But, what if we didn’t know exactly how columns there were? What if we just knew that we wanted to iterate over all the columns in the data frame passed to the right of the keyword in. How could we do that?\nWe can do that with the seq_along() function. When we pass a vector to the seq_along() function, it returns a sequence of integers with the same length as the vector being passed, starting at one. For example:\n\nseq_along(c(4, 5, 6))\n\n[1] 1 2 3\n\n\nOr:\n\nseq_along(c(\"a\", \"b\", \"c\", \"d\"))\n\n[1] 1 2 3 4\n\n\nSimilarly, when we pass a data frame to the seq_along() function, it returns a sequence of integers with a length equal to the number of columns in the data frame being passed, starting at one. For example:\n\nseq_along(df_xyz)\n\n[1] 1 2 3\n\n\nTherefore, we can replace for(i in c(1, 2, 3)) with for(i in seq_along(df_xyz)) to make our code more robust (i.e., it will work in more situations):\n\nxyz_means &lt;- vector(\"numeric\", 3)\n\nfor(i in seq_along(df_xyz)) {\n  xyz_means[[i]] &lt;- mean(df_xyz[[i]])\n}\n\nxyz_means\n\n[1]  0.07462564  0.20862196 -0.42455887\n\n\nJust to make sure that we really understand what’s going on in the code above, let’s walk through the entire process one more time.\n\n\n\n\n\nIllustration of a for loop process again - I\n\n\n\n\nR starts executing at the top of the for loop. In the first iteration, the value of i is set to the first value in seq_along(df_xyz), which is 1. Then, the i in df_xyz[[i]] inside the for loop body is replaced with 1. Then, R calculates the mean of df_xyz[[1]], which is x column of the df_xyz data frame. Finally, the mean value is assigned to xyz_means[[i]], which is xyz_means[[1]] in this iteration. So, the value of the first element in the xyz_means vector is 0.07462564.\nAt this point, there is no more code left to execute inside of the for loop, so R returns to the top of the loop. i has not yet taken every value of the object to the right of the keyword in, so R starts another iteration of the for loop.\n\n\n\n\n\nIllustration of a for loop process again - II\n\n\n\n\nIn the second iteration, the value of i is set to the second value in seq_along(df_xyz), which is 2. Then, the i in df_xyz[[i]] inside the for loop body is replaced with 2. Then, R calculates the mean of df_xyz[[2]], which is y column of the df_xyz data frame. Finally, the mean value is assigned to xyz_means[[i]], which is xyz_means[[2]] in this iteration. So, the value of the second element in the xyz_means vector is 0.20862196.\nAt this point, there is no more code left to execute inside of the for loop, so R returns to the top of the loop. i still has not yet taken every value of the object to the right of the keyword in, so R starts another iteration of the for loop.\n\n\n\n\n\nIllustration of a for loop process again - III\n\n\n\n\nIn the third iteration, the value of i is set to the third value in seq_along(df_xyz), which is 3. Then, the i in df_xyz[[i]] inside the for loop body is replaced with 3. Then, R calculates the mean of df_xyz[[3]], which is z column of the df_xyz data frame. Finally, the mean value is assigned to xyz_means[[i]], which is xyz_means[[3]] in this iteration. So, the value of the third element in the xyz_means vector is -0.42455887.\n\n\n\n\n\nIllustration of a for loop process - V\n\n\n\n\nAt this point, there is no more code left to execute inside of the for loop, so R returns to the top of the loop. However, this time, i has taken every value of the object to the right of the keyword in, so R does not start another iteration. It leaves the looping process, and the value of xyz_means remains 0.07462564, 0.20862196, -0.4245589.\nThere’s one final adjustment we should probably make to the code above. Did you notice that when we create the empty vector to contain our results, we’re still hard coding its length to 3? For the same reason we replaced for(i in c(1, 2, 3)) with for(i in seq_along(df_xyz)), we want to replace vector(\"numeric\", 3) with vector(\"numeric\", length(df_xyz)).\nNow, let’s add a fourth column to our data frame:\n\ndf_xyz &lt;- df_xyz %&gt;% \n  mutate(a = rnorm(10)) %&gt;% \n  print()\n\n# A tibble: 10 × 4\n         x      y      z       a\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.560   1.22  -1.07   0.426 \n 2 -0.230   0.360 -0.218 -0.295 \n 3  1.56    0.401 -1.03   0.895 \n 4  0.0705  0.111 -0.729  0.878 \n 5  0.129  -0.556 -0.625  0.822 \n 6  1.72    1.79  -1.69   0.689 \n 7  0.461   0.498  0.838  0.554 \n 8 -1.27   -1.97   0.153 -0.0619\n 9 -0.687   0.701 -1.14  -0.306 \n10 -0.446  -0.473  1.25  -0.380 \n\n\nAnd see what happens when we pass it to our new, robust for loop code:\n\nxyz_means &lt;- vector(\"numeric\", length(df_xyz)) # Using length() instead of 3\n\nfor(i in seq_along(df_xyz)) { # Using seq_along() instead of c(1, 2, 3)\n  xyz_means[[i]] &lt;- mean(df_xyz[[i]])\n}\n\nxyz_means\n\n[1]  0.07462564  0.20862196 -0.42455887  0.32204455\n\n\nOur for loop now gives us the result we want no matter how many columns are in the data frame. Having the flexibility to loop over an arbitrary number of columns wasn’t that important in this case – we knew exactly how many columns we wanted to loop over. However, what if we wanted to add more columns in the future? Using the second method, we wouldn’t have to make any changes to our code. This is often an important consideration when we embed for loops inside of functions that we write ourselves.\nFor example, maybe we think, “that for loop above was really useful. I want to write it into a function so that I can use it again in my other projects.” Well, we’ve already seen how to take our working code, embed it inside of a function, make it more general, and assign it a name. If you forgot how to do this, please review the function writing process. In this case, that process would result in something like this:\n\nmulti_means &lt;- function(data) {\n  # Create a structure to contain results\n  result &lt;- vector(\"numeric\", length(data))\n  \n  # Iterate over each column of data\n  for(i in seq_along(data)) {\n    result[[i]] &lt;- mean(data[[i]])\n  }\n  \n  # Return the result\n  result\n}\n\nWhich we can easily apply to our data frame like this:\n\nmulti_means(df_xyz)\n\n[1]  0.07462564  0.20862196 -0.42455887  0.32204455\n\n\nFurther, because we’ve made the for loop code inside of the function body flexible with length() and seq_along() we can easily pass any other data frame (with all numeric columns) to our function like this:\n\nset.seed(123)\nnew_df &lt;- tibble(\n  age    = rnorm(10, 50, 10),\n  height = rnorm(10, 65, 5),\n  weight = rnorm(10, 165, 10)\n) %&gt;% \n  print()\n\n# A tibble: 10 × 3\n     age height weight\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  44.4   71.1   154.\n 2  47.7   66.8   163.\n 3  65.6   67.0   155.\n 4  50.7   65.6   158.\n 5  51.3   62.2   159.\n 6  67.2   73.9   148.\n 7  54.6   67.5   173.\n 8  37.3   55.2   167.\n 9  43.1   68.5   154.\n10  45.5   62.6   178.\n\n\n\nmulti_means(new_df)\n\n[1]  50.74626  66.04311 160.75441\n\n\nIf we want our for loop to return the results with informative names, similar those that are returned when we use the across() method, we can simply add one line of code to our for loop body that names each result:\n\nxyz_means &lt;- vector(\"numeric\", length(df_xyz))\n\nfor(i in seq_along(df_xyz)) {\n  xyz_means[[i]] &lt;- mean(df_xyz[[i]])\n  names(xyz_means)[[i]] &lt;- paste0(names(df_xyz)[[i]], \"_mean\") # Name results here\n}\n\nxyz_means\n\n     x_mean      y_mean      z_mean      a_mean \n 0.07462564  0.20862196 -0.42455887  0.32204455 \n\n\nIf it isn’t quite clear to you why that code works, try picking it apart, replacing i with a number, and figuring out how it works.\nWe can make our results resemble those returned by the across() method even more by converting our named vector to a data frame like this:\n\nxyz_means %&gt;% \n  as.list() %&gt;% \n  as_tibble()\n\n# A tibble: 1 × 4\n  x_mean y_mean z_mean a_mean\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0746  0.209 -0.425  0.322\n\n\nFinally, we can update our multi_means() function with changes we made above so that our results are returned as a data frame with informative column names:\n\nmulti_means &lt;- function(data) {\n  # Create a structure to contain results\n  result &lt;- vector(\"numeric\", length(data))\n  \n  # Iterate over each column of data\n  for(i in seq_along(data)) {\n    result[[i]] &lt;- mean(data[[i]])\n    names(result)[[i]] &lt;- paste0(names(data)[[i]], \"_mean\")\n  }\n  \n  # Return the result as a tibble\n  as_tibble(as.list(result))\n}\n\n\nmulti_means(new_df)\n\n# A tibble: 1 × 3\n  age_mean height_mean weight_mean\n     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1     50.7        66.0        161.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Writing For Loops</span>"
    ]
  },
  {
    "objectID": "chapters/writing_for_loops/writing_for_loops.html#using-for-loops-for-data-transfer",
    "href": "chapters/writing_for_loops/writing_for_loops.html#using-for-loops-for-data-transfer",
    "title": "36  Writing For Loops",
    "section": "36.2 Using for loops for data transfer",
    "text": "36.2 Using for loops for data transfer\nIn the previous section, we used an example that wasn’t really all that realistic, but it was useful (hopefully) for learning the mechanics of for loops. As mentioned at the beginning of the chapter, rather than using a for loop for the analysis above, using across() with summarise() might be preferable.\nHowever, keep in mind that across() is designed specifically for repeatedly applying functions column-wise (i.e., across columns) of a single data frame in conjunction with dplyr verbs. By definition, if we are repeating code outside of dplyr, or if we are applying code across multiple data frames, then we probably aren’t going to be able to use across() to complete our coding task.\nFor example, let’s say that we have data stored across multiple sheets of an Excel workbook. This simulated data contains some demographic information about three different cities: Houston, Atlanta, and Charlotte. We need to import each sheet, clean the data, and combine them into a single data frame in order to complete our analysis. First, we will load the readxlpackage:\n\nlibrary(readxl)\n\nYou may click here to download this file to your computer.\nThen, we may import each sheet like this:\n\nhouston &lt;- read_excel(\n  \"city_ses.xlsx\",\n  sheet = \"Houston\"\n) %&gt;% \n  print()\n\n# A tibble: 5 × 4\n  pid     age sex   ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 001      13 F            88\n2 003      13 F            78\n3 007      14 M            83\n4 014      12 F            76\n5 036      13 M            84\n\n\n\natlanta &lt;- read_excel(\n  \"city_ses.xlsx\",\n  sheet = \"Atlanta\"\n) %&gt;% \n  print()\n\n# A tibble: 5 × 4\n  id      age gender ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 002      14 M             64\n2 009      15 M             35\n3 012      13 F             70\n4 013      13 F             66\n5 022      12 F             59\n\n\n\ncharlotte &lt;- read_excel(\n  \"city_ses.xlsx\",\n  sheet = \"Charlotte\"\n) %&gt;% \n  print()\n\n# A tibble: 5 × 4\n  pid     age sex     ses\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 004      13 F        84\n2 011      14 M        66\n3 018      12 M        92\n4 023      12 M        89\n5 030      13 F        83\n\n\n🚩In the code chunks above, we have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition from our code. Of course, we could write our own function to reduce some of the repetition:\n\nimport_cities &lt;- function(sheet) {\n  df &lt;- read_excel(\n    \"city_ses.xlsx\",\n    sheet = sheet\n  )\n}\n\n\nhouston &lt;- import_cities(\"Houston\") %&gt;% print()\n\n# A tibble: 5 × 4\n  pid     age sex   ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 001      13 F            88\n2 003      13 F            78\n3 007      14 M            83\n4 014      12 F            76\n5 036      13 M            84\n\n\n\natlanta &lt;- import_cities(\"Atlanta\") %&gt;% print()\n\n# A tibble: 5 × 4\n  id      age gender ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 002      14 M             64\n2 009      15 M             35\n3 012      13 F             70\n4 013      13 F             66\n5 022      12 F             59\n\n\n\ncharlotte &lt;- import_cities(\"Charlotte\") %&gt;% print()\n\n# A tibble: 5 × 4\n  pid     age sex     ses\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 004      13 F        84\n2 011      14 M        66\n3 018      12 M        92\n4 023      12 M        89\n5 030      13 F        83\n\n\nThat method is better. And depending on the circumstances of your project, it may be the best approach. However, an alternative approach would be to use a for loop. Using the for loop approach might look something like this:\n\n# Save the file path to an object so we don't have to type it repeatedly \n# or hard-code it in.\npath &lt;-  \"city_ses.xlsx\"\n\n# Use readxl::excel_sheets to get the name of each sheet in the workbook.\n# this makes our code more robust.\nsheets &lt;- excel_sheets(path)\n\nfor(i in seq_along(sheets)) {\n  # Convert sheet name to lowercase before using it to name the df\n  new_nm &lt;- tolower(sheets[[i]])\n  assign(new_nm, read_excel(path, sheet = sheets[[i]]))\n}\n\n\nhouston\n\n# A tibble: 5 × 4\n  pid     age sex   ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 001      13 F            88\n2 003      13 F            78\n3 007      14 M            83\n4 014      12 F            76\n5 036      13 M            84\n\n\n\natlanta\n\n# A tibble: 5 × 4\n  id      age gender ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 002      14 M             64\n2 009      15 M             35\n3 012      13 F             70\n4 013      13 F             66\n5 022      12 F             59\n\n\n\ncharlotte\n\n# A tibble: 5 × 4\n  pid     age sex     ses\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 004      13 F        84\n2 011      14 M        66\n3 018      12 M        92\n4 023      12 M        89\n5 030      13 F        83\n\n\n👆Here’s what we did above:\n\nWe used a for loop to import every sheet from an Excel workbook.\nFirst, we saved the path to the Excel workbook to a separate object. We didn’t have to do this. However, doing so prevented us from having to type out the full file path repeatedly in the rest of our code. Additionally, if the file path ever changed, we would only have to update it in one place.\nSecond, we used the excel_sheets() function to create a character vector containing each sheet name. We didn’t have to do this. We could have typed each sheet name manually. However, there shouldn’t be any accidental typos if we use the excel_sheets() function, and we don’t have to make any changes to our code if more sheets are added to the Workbook in the future.\nInside the for loop, we assigned each data frame created by the read_excel() function to our global environment using the assign() function. We haven’t used the assign() function before, but you can read the help documentation by typing ?assign in your R console.\n\nThe first argument to the assign() function is x. The value you pass to x should be the name of the object you want to create. Above, we passed new_nm (for new name) to the x argument. At each iteration of the for loop, new_nm contained the name of each sheet in sheets. So, Houston at the first iteration, Atlanta at the second iteration, and Charlotte at the third iteration. Of course, we like using lowercase names for our data frames, so we used tolower() to convert Houston, Atlanta, and Charlotte to houston, atlanta, and charlotte. These will be the names used for each data frame assigned to our global environment inside of the for loop.\nThe second argument to the assign() function is value. The value you pass to value should be the contents you want to assign the object with the name you passed to the x argument. Above, we passed the code that imports each sheet of the city_ses.xlsx data frame to the value argument.\n\n\nFor loops can often be helpful for data transfer tasks. In the code above, we looped over sheets of a single Excel workbook. However, we could have similarly looped over file paths to import multiple different Excel workbooks instead. We could have even used nested for loops to import multiple sheets from multiple Excel workbooks. The code would not have looked drastically different.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Writing For Loops</span>"
    ]
  },
  {
    "objectID": "chapters/writing_for_loops/writing_for_loops.html#using-for-loops-for-data-management",
    "href": "chapters/writing_for_loops/writing_for_loops.html#using-for-loops-for-data-management",
    "title": "36  Writing For Loops",
    "section": "36.3 Using for loops for data management",
    "text": "36.3 Using for loops for data management\nIn the chapter on writing functions, we created an is_match() function. In that scenario, we wanted to see if first name, last name, and street name matched at each ID between our data frames. More specifically, we wanted to combine the two data frames into a single data frame and create three new dummy variables that indicated whether first name, last name, and address matched respectively.\nHere are the data frames we simulated and combined:\n\npeople_1 &lt;- tribble(\n  ~id_1, ~name_first_1, ~name_last_1, ~street_1,\n  1,     \"Easton\",      NA,           \"Alameda\",\n  2,     \"Elias\",       \"Salazar\",    \"Crissy Field\",\n  3,     \"Colton\",      \"Fox\",        \"San Bruno\",\n  4,     \"Cameron\",     \"Warren\",     \"Nottingham\",\n  5,     \"Carson\",      \"Mills\",      \"Jersey\",\n  6,     \"Addison\",     \"Meyer\",      \"Tingley\",\n  7,     \"Aubrey\",      \"Rice\",       \"Buena Vista\",\n  8,     \"Ellie\",       \"Schmidt\",    \"Division\",\n  9,     \"Robert\",      \"Garza\",      \"Red Rock\",\n  10,    \"Stella\",      \"Daniels\",    \"Holland\"\n)\n\n\npeople_2 &lt;- tribble(\n  ~id_2, ~name_first_2, ~name_last_2, ~street_2,\n  1,     \"Easton\",      \"Stone\",      \"Alameda\",\n  2,     \"Elas\",        \"Salazar\",    \"Field\",\n  3,     NA,            \"Fox\",        NA,\n  4,     \"Cameron\",     \"Waren\",      \"Notingham\",\n  5,     \"Carsen\",      \"Mills\",      \"Jersey\",\n  6,     \"Adison\",      NA,           NA,\n  7,     \"Aubrey\",      \"Rice\",       \"Buena Vista\",\n  8,     NA,            \"Schmidt\",    \"Division\",\n  9,     \"Bob\",         \"Garza\",      \"Red Rock\",\n  10,    \"Stella\",      NA,           \"Holland\"\n)\n\n\npeople &lt;- people_1 %&gt;% \n  bind_cols(people_2) %&gt;% \n  print()\n\n# A tibble: 10 × 8\n    id_1 name_first_1 name_last_1 street_1      id_2 name_first_2 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n 1     1 Easton       &lt;NA&gt;        Alameda          1 Easton       Stone      \n 2     2 Elias        Salazar     Crissy Field     2 Elas         Salazar    \n 3     3 Colton       Fox         San Bruno        3 &lt;NA&gt;         Fox        \n 4     4 Cameron      Warren      Nottingham       4 Cameron      Waren      \n 5     5 Carson       Mills       Jersey           5 Carsen       Mills      \n 6     6 Addison      Meyer       Tingley          6 Adison       &lt;NA&gt;       \n 7     7 Aubrey       Rice        Buena Vista      7 Aubrey       Rice       \n 8     8 Ellie        Schmidt     Division         8 &lt;NA&gt;         Schmidt    \n 9     9 Robert       Garza       Red Rock         9 Bob          Garza      \n10    10 Stella       Daniels     Holland         10 Stella       &lt;NA&gt;       \n# ℹ 1 more variable: street_2 &lt;chr&gt;\n\n\nHere is the function we wrote to help us create the dummy variables:\n\nis_match &lt;- function(value_1, value_2) {\n  result &lt;- value_1 == value_2\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\nAnd here is how we applied the function we wrote to get our results:\n\npeople %&gt;% \n  mutate(\n    name_first_match = is_match(name_first_1, name_first_2),\n    name_last_match  = is_match(name_last_1, name_last_2),\n    street_match     = is_match(street_1, street_2)\n  ) %&gt;% \n  # Order like columns next to each other for easier comparison\n  select(id_1, starts_with(\"name_f\"), starts_with(\"name_l\"), starts_with(\"s\"))\n\n# A tibble: 10 × 10\n    id_1 name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2     2 Elias        Elas         FALSE            Salazar     Salazar    \n 3     3 Colton       &lt;NA&gt;         FALSE            Fox         Fox        \n 4     4 Cameron      Cameron      TRUE             Warren      Waren      \n 5     5 Carson       Carsen       FALSE            Mills       Mills      \n 6     6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7     7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8     8 Ellie        &lt;NA&gt;         FALSE            Schmidt     Schmidt    \n 9     9 Robert       Bob          FALSE            Garza       Garza      \n10    10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\n🚩However, in the code chunk above, we still have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition from our code. Because we are using dplyr, and all of our data resides inside of a single data frame, your first instinct might be to use across() inside of mutate() to perform column-wise operations. Unfortunately, that method won’t work in this scenario.\nThe across() function will apply the function we pass to the .fns argument to each column passed to the .cols argument, one at a time. But, we need to pass two columns at a time to the is_match() function. For example, name_first_1 and name_first_2. There’s really no good way to accomplish this task using is_match() inside of across(). However, it is fairly simple to accomplish this task with a for loop:\n\ncols &lt;- c(\"name_first\", \"name_last\", \"street\")\n\nfor(i in seq_along(cols)) {\n  col_1   &lt;- paste0(cols[[i]], \"_1\")\n  col_2   &lt;- paste0(cols[[i]], \"_2\")\n  new_col &lt;- paste0(cols[[i]], \"_match\")\n  people[[new_col]] &lt;- is_match(people[[col_1]], people[[col_2]])\n}\n\n\npeople %&gt;% \n  select(id_1, starts_with(\"name_f\"), starts_with(\"name_l\"), starts_with(\"s\"))\n\n# A tibble: 10 × 10\n    id_1 name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2     2 Elias        Elas         FALSE            Salazar     Salazar    \n 3     3 Colton       &lt;NA&gt;         FALSE            Fox         Fox        \n 4     4 Cameron      Cameron      TRUE             Warren      Waren      \n 5     5 Carson       Carsen       FALSE            Mills       Mills      \n 6     6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7     7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8     8 Ellie        &lt;NA&gt;         FALSE            Schmidt     Schmidt    \n 9     9 Robert       Bob          FALSE            Garza       Garza      \n10    10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\n👆Here’s what we did above:\n\nWe used our is_match() function inside of a for loop to create three new dummy variables that indicated whether first name, last name, and address matched respectively.\n\nLet’s pull the code apart piece-by-piece to see how it works.\n\ncols &lt;- c(\"name_first\", \"name_last\", \"street\")\n\nfor(i in seq_along(cols)) {\n  col_1   &lt;- paste0(cols[[i]], \"_1\")\n  col_2   &lt;- paste0(cols[[i]], \"_2\")\n  new_col &lt;- paste0(cols[[i]], \"_match\")\n  print(col_1)\n  print(col_2)\n  print(new_col)\n}\n\n[1] \"name_first_1\"\n[1] \"name_first_2\"\n[1] \"name_first_match\"\n[1] \"name_last_1\"\n[1] \"name_last_2\"\n[1] \"name_last_match\"\n[1] \"street_1\"\n[1] \"street_2\"\n[1] \"street_match\"\n\n\nFirst, we created a character vector that contained the base name (i.e., no _1 or _2) of each of the columns we wanted to compare. Then, we iterated over that character vector by passing it as the object to the right of the keyword in.\nAt each iteration, we used paste0() to create three column names from the character string in cols. For example, in the first iteration of the loop, the value of cols was name_first. The first line of code in the for loop body combined name_first with _1 to make the character string name_first_1 and save it as an object named col_1. The second line of code in the for loop body combined name_first with _2 to make the character string name_first_2 and save it as an object named col_2. And, the third line of code in the for loop body combined name_first with _match to make the character string name_first_match and save it as an object named new_col.\nThis will allow us to use col_1, col_2, and new_col in the code that compares the columns and creates each dummy variable. For example, here is what people[[col_1]] looks like at each iteration:\n\ncols &lt;- c(\"name_first\", \"name_last\", \"street\")\n\nfor(i in seq_along(cols)) {\n  col_1 &lt;- paste0(cols[[i]], \"_1\")\n  col_2 &lt;- paste0(cols[[i]], \"_2\")\n  print(people[[col_1]])\n}\n\n [1] \"Easton\"  \"Elias\"   \"Colton\"  \"Cameron\" \"Carson\"  \"Addison\" \"Aubrey\" \n [8] \"Ellie\"   \"Robert\"  \"Stella\" \n [1] NA        \"Salazar\" \"Fox\"     \"Warren\"  \"Mills\"   \"Meyer\"   \"Rice\"   \n [8] \"Schmidt\" \"Garza\"   \"Daniels\"\n [1] \"Alameda\"      \"Crissy Field\" \"San Bruno\"    \"Nottingham\"   \"Jersey\"      \n [6] \"Tingley\"      \"Buena Vista\"  \"Division\"     \"Red Rock\"     \"Holland\"     \n\n\nIt is a vector that matches people[[\"name_first_1\"]], people[[\"name_last_1\"]], and people[[\"street_1\"]] respectively.\nAnd here is what col_2 looks like at each iteration:\n\ncols &lt;- c(\"name_first\", \"name_last\", \"street\")\n\nfor(i in seq_along(cols)) {\n  col_1 &lt;- paste0(cols[[i]], \"_1\")\n  col_2 &lt;- paste0(cols[[i]], \"_2\")\n  print(people[[col_2]])\n}\n\n [1] \"Easton\"  \"Elas\"    NA        \"Cameron\" \"Carsen\"  \"Adison\"  \"Aubrey\" \n [8] NA        \"Bob\"     \"Stella\" \n [1] \"Stone\"   \"Salazar\" \"Fox\"     \"Waren\"   \"Mills\"   NA        \"Rice\"   \n [8] \"Schmidt\" \"Garza\"   NA       \n [1] \"Alameda\"     \"Field\"       NA            \"Notingham\"   \"Jersey\"     \n [6] NA            \"Buena Vista\" \"Division\"    \"Red Rock\"    \"Holland\"    \n\n\nNow, we can pass each vector to our is_match() function at each iteration like this:\n\ncols &lt;- c(\"name_first\", \"name_last\", \"street\")\n\nfor(i in seq_along(cols)) {\n  col_1 &lt;- paste0(cols[[i]], \"_1\")\n  col_2 &lt;- paste0(cols[[i]], \"_2\")\n  print(is_match(people[[col_1]], people[[col_2]]))\n}\n\n [1]  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE\n [1] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nThese logical vectors are the results we want to go into our new dummy variables. Therefore, the last step is to assign each logical vector above to a new variable in our data frame called people[[\"name_first_match\"]], people[[\"name_last_match\"]], and people[[\"street_match\"]] respectively. We do so by allowing people[[new_col]] to represent those values at each iteration of the loop:\n\ncols &lt;- c(\"name_first\", \"name_last\", \"street\")\n\nfor(i in seq_along(cols)) {\n  col_1   &lt;- paste0(cols[[i]], \"_1\")\n  col_2   &lt;- paste0(cols[[i]], \"_2\")\n  new_col &lt;- paste0(cols[[i]], \"_match\")\n  people[[new_col]] &lt;- is_match(people[[col_1]], people[[col_2]])\n}\n\nAnd here is our result:\n\npeople %&gt;% \n  select(id_1, starts_with(\"name_f\"), starts_with(\"name_l\"), starts_with(\"s\"))\n\n# A tibble: 10 × 10\n    id_1 name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2     2 Elias        Elas         FALSE            Salazar     Salazar    \n 3     3 Colton       &lt;NA&gt;         FALSE            Fox         Fox        \n 4     4 Cameron      Cameron      TRUE             Warren      Waren      \n 5     5 Carson       Carsen       FALSE            Mills       Mills      \n 6     6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7     7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8     8 Ellie        &lt;NA&gt;         FALSE            Schmidt     Schmidt    \n 9     9 Robert       Bob          FALSE            Garza       Garza      \n10    10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\nIn the code above, we used roughly the same amount of code to complete the task with a loop that we used to complete it without a loop. However, this code still has some advantages. We only typed “name_first”, “name_last”, and “street” once at the beginning of the code chunk. Therefore, we didn’t have to worry about forgetting to change a column name after copying and pasting code. Additionally, if we later decide that we also want to compare other columns (e.g., middle name, birth date, city, state, zip code), we only have to update the code in one place – where we create the cols vector.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Writing For Loops</span>"
    ]
  },
  {
    "objectID": "chapters/writing_for_loops/writing_for_loops.html#using-for-loops-for-analysis",
    "href": "chapters/writing_for_loops/writing_for_loops.html#using-for-loops-for-analysis",
    "title": "36  Writing For Loops",
    "section": "36.4 Using for loops for analysis",
    "text": "36.4 Using for loops for analysis\nFor our final example of this chapter, let’s return to the final example from the column-wise operations chapter. We started with some simulated study data:\n\nstudy &lt;- tibble(\n  age       = c(32, 30, 32, 29, 24, 38, 25, 24, 48, 29, 22, 29, 24, 28, 24, 25, \n                25, 22, 25, 24, 25, 24, 23, 24, 31, 24, 29, 24, 22, 23, 26, 23, \n                24, 25, 24, 33, 27, 25, 26, 26, 26, 26, 26, 27, 24, 43, 25, 24, \n                27, 28, 29, 24, 26, 28, 25, 24, 26, 24, 26, 31, 24, 26, 31, 34, \n                26, 25, 27, NA),\n  age_group = c(2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n                1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, \n                1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, \n                2, 1, 1, 1, NA),\n  gender    = c(2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, \n                1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, \n                1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, \n                1, 1, 2, 1, NA),\n  ht_in     = c(70, 63, 62, 67, 67, 58, 64, 69, 65, 68, 63, 68, 69, 66, 67, 65, \n                64, 75, 67, 63, 60, 67, 64, 73, 62, 69, 67, 62, 68, 66, 66, 62, \n                64, 68, NA, 68, 70, 68, 68, 66, 71, 61, 62, 64, 64, 63, 67, 66, \n                69, 76, NA, 63, 64, 65, 65, 71, 66, 65, 65, 71, 64, 71, 60, 62, \n                61, 69, 66, NA),\n  wt_lbs    = c(216, 106, 145, 195, 143, 125, 138, 140, 158, 167, 145, 297, 146, \n                125, 111, 125, 130, 182, 170, 121, 98, 150, 132, 250, 137, 124, \n                186, 148, 134, 155, 122, 142, 110, 132, 188, 176, 188, 166, 136, \n                147, 178, 125, 102, 140, 139, 60, 147, 147, 141, 232, 186, 212, \n                110, 110, 115, 154, 140, 150, 130, NA, 171, 156, 92, 122, 102, \n                163, 141, NA),\n  bmi       = c(30.99, 18.78, 26.52, 30.54, 22.39, 26.12, 23.69, 20.67, 26.29, \n                25.39, 25.68, 45.15, 21.56, 20.17, 17.38, 20.8, 22.31, 22.75, \n                26.62, 21.43, 19.14, 23.49, 22.66, 32.98, 25.05, 18.31, 29.13, \n                27.07, 20.37, 25.01, 19.69, 25.97, 18.88, 20.07, NA, 26.76, \n                26.97, 25.24, 20.68, 23.72, 24.82, 23.62, 18.65, 24.03, 23.86, \n                10.63, 23.02, 23.72, 20.82, 28.24, NA, 37.55, 18.88, 18.3, \n                19.13, 21.48, 22.59, 24.96, 21.63, NA, 29.35, 21.76, 17.97, \n                22.31, 19.27, 24.07, 22.76, NA),\n  bmi_3cat  = c(3, 1, 2, 3, 1, 2, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, \n                1, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, NA, 2, 2, 2, 1, 1, 1, 1, \n                1, 1, 1, 1, 1, 1, 1, 2, NA, 3, 1, 1, 1, 1, 1, 1, 1, NA, 2, 1, \n                1, 1, 1, 1, 1, NA)\n) %&gt;% \n  mutate(\n    age_group = factor(age_group, labels = c(\"Younger than 30\", \"30 and Older\")),\n    gender    = factor(gender, labels = c(\"Female\", \"Male\")),\n    bmi_3cat  = factor(bmi_3cat, labels = c(\"Normal\", \"Overweight\", \"Obese\"))\n  ) %&gt;% \n  print()\n\n# A tibble: 68 × 7\n     age age_group       gender ht_in wt_lbs   bmi bmi_3cat  \n   &lt;dbl&gt; &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     \n 1    32 30 and Older    Male      70    216  31.0 Obese     \n 2    30 30 and Older    Female    63    106  18.8 Normal    \n 3    32 30 and Older    Female    62    145  26.5 Overweight\n 4    29 Younger than 30 Male      67    195  30.5 Obese     \n 5    24 Younger than 30 Female    67    143  22.4 Normal    \n 6    38 30 and Older    Female    58    125  26.1 Overweight\n 7    25 Younger than 30 Female    64    138  23.7 Normal    \n 8    24 Younger than 30 Male      69    140  20.7 Normal    \n 9    48 30 and Older    Male      65    158  26.3 Overweight\n10    29 Younger than 30 Male      68    167  25.4 Overweight\n# ℹ 58 more rows\n\n\nThen we saw how to use across() with pivot_longer() to remove repetition and get our results into a format that were easier to read an interpret:\n\nsummary_stats &lt;- study %&gt;% \n  summarise(\n    across(\n      .cols  = c(age, ht_in, wt_lbs, bmi),\n      .fns   = list(\n        n_miss = ~ sum(is.na(.x)),\n        mean   = ~ mean(.x, na.rm = TRUE),\n        median = ~ median(.x, na.rm = TRUE),\n        min    = ~ min(.x, na.rm = TRUE),\n        max    = ~ max(.x, na.rm = TRUE)\n      ),\n      .names = \"{col}-{fn}\" # This is the new part of the code\n    ) \n  ) %&gt;% \n  print()\n\n# A tibble: 1 × 20\n  `age-n_miss` `age-mean` `age-median` `age-min` `age-max` `ht_in-n_miss`\n         &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;          &lt;int&gt;\n1            1       26.9           26        22        48              3\n# ℹ 14 more variables: `ht_in-mean` &lt;dbl&gt;, `ht_in-median` &lt;dbl&gt;,\n#   `ht_in-min` &lt;dbl&gt;, `ht_in-max` &lt;dbl&gt;, `wt_lbs-n_miss` &lt;int&gt;,\n#   `wt_lbs-mean` &lt;dbl&gt;, `wt_lbs-median` &lt;dbl&gt;, `wt_lbs-min` &lt;dbl&gt;,\n#   `wt_lbs-max` &lt;dbl&gt;, `bmi-n_miss` &lt;int&gt;, `bmi-mean` &lt;dbl&gt;,\n#   `bmi-median` &lt;dbl&gt;, `bmi-min` &lt;dbl&gt;, `bmi-max` &lt;dbl&gt;\n\n\n\nsummary_stats %&gt;% \n  tidyr::pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"characteristic\", \".value\"),\n    names_sep = \"-\"\n  )\n\n# A tibble: 4 × 6\n  characteristic n_miss  mean median   min   max\n  &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age                 1  26.9   26    22    48  \n2 ht_in               3  66.0   66    58    76  \n3 wt_lbs              2 148.   142.   60   297  \n4 bmi                 4  23.6   22.9  10.6  45.2\n\n\nI think that method works really nicely for our continuous variables; however, the situation is slightly more complicated for categorical variables. To illustrate the problem as simply as possible, let’s start by just getting counts for each of our categorical variables:\n\nstudy %&gt;% \n  count(age_group)\n\n# A tibble: 3 × 2\n  age_group           n\n  &lt;fct&gt;           &lt;int&gt;\n1 Younger than 30    56\n2 30 and Older       11\n3 &lt;NA&gt;                1\n\n\n\nstudy %&gt;% \n  count(gender)\n\n# A tibble: 3 × 2\n  gender     n\n  &lt;fct&gt;  &lt;int&gt;\n1 Female    43\n2 Male      24\n3 &lt;NA&gt;       1\n\n\n\nstudy %&gt;% \n  count(bmi_3cat)\n\n# A tibble: 4 × 2\n  bmi_3cat       n\n  &lt;fct&gt;      &lt;int&gt;\n1 Normal        43\n2 Overweight    16\n3 Obese          5\n4 &lt;NA&gt;           4\n\n\nYou are, of course, and old pro at this by now, and you quickly spot all the unnecessary repetition. So, you decide to pass count to the .fns argument like this:\n\nstudy %&gt;% \n  summarise(\n    across(\n      .cols = c(age_group, gender, bmi_3cat),\n      .fns  = count\n    )\n  )\n\nError in `summarise()`:\nℹ In argument: `across(.cols = c(age_group, gender, bmi_3cat), .fns =\n  count)`.\nCaused by error in `across()`:\n! Can't compute column `age_group`.\nCaused by error in `UseMethod()`:\n! no applicable method for 'count' applied to an object of class \"factor\"\n\n\nUnfortunately, this won’t work. At least not currently. There are a couple reasons why this won’t work, but the one that is probably easiest to wrap your head around is related to the number of results produced by count(). What does this mean? Well, when we pass each continuous variable to mean() (or median, min, or max) we get one result back for each column:\n\nstudy %&gt;% \n  summarise(\n    across(\n      .cols = c(age, ht_in),\n      .fns  = ~ mean(.x, na.rm = TRUE)\n    )\n  )\n\n# A tibble: 1 × 2\n    age ht_in\n  &lt;dbl&gt; &lt;dbl&gt;\n1  26.9  66.0\n\n\nIt’s easy for dplyr to arrange those results into a data frame. However, the results from count() are much less predictable. For example, study %&gt;% count(age_group) had three results, study %&gt;% count(gender) had three results, and study %&gt;% count(bmi_3cat) had four results. Also, remember that every column of a data frame has to have the same number of rows. So, if the code we used to try to pass count to the .fns argument above would actually run, it might look something like this:\n\n\n\n\n\nRepresentation of result from the above code if it could be run successfully\n\n\n\n\nBecause summarise() lays the results out side-by-side, it’s not clear what would go into the 4 cells in the bottom-left corner of the results data frame. Therefore, it isn’t necessarily straightforward for dplyr to figure out how it should return such results to us.\nHowever, when we use a for loop, we can create our own structure to hold the results. And, that structure can be pretty much any structure that meets our needs. In this case, one option would be to create a data frame to hold our categorical counts that looks like this:\n\n\n\n\n\nEmpty data frame structure to hold for loop results\n\n\n\n\nThen, we can use a for loop to fill in the empty data frame so that we end up with results that look like this:\n\n\n\n\n\nData frame structure with for loop results\n\n\n\n\nThe process for getting to our finished product is a little bit involved (and probably a little intimidating for some of you) and will require us to cover a couple new topics. So, we’ll start by giving you the complete code for accomplishing this task. Then, we’ll pick the code apart, piece-by-piece, to make sure we understand how it works.\nHere is the complete solution:\n\n# Structure 1. An object to contain the results.\n  # Create the data frame structure that will contain our results\ncat_table &lt;- tibble(\n  variable = vector(\"character\"), \n  category = vector(\"character\"), \n  n        = vector(\"numeric\")\n) \n\n# Structure 2. The actual for loop.\n  # For each column, get the column name, category names, and count.\n  # Then, add them to the bottom of the results data frame we created above.\nfor(i in c(\"age_group\", \"gender\", \"bmi_3cat\")) {\n  cat_stats &lt;- study %&gt;% \n    count(.data[[i]]) %&gt;% # Use .data to refer to the current data frame.\n    mutate(variable = names(.)[1]) %&gt;% # Use . to refer to the result to this point.\n    rename(category = 1)\n  \n  # Here is where we update cat_table with the results for each column\n  cat_table &lt;- bind_rows(cat_table, cat_stats)\n}\n\n\ncat_table\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nWe’ll use the rest of this chapter section to walk through the code above and make sure we understand how it works. For starters, we will create our results data frame structure like this:\n\ncat_table &lt;- tibble(\n  variable = vector(\"character\"), \n  category = vector(\"character\"), \n  n        = vector(\"numeric\")\n) \n\n\nstr(cat_table)\n\ntibble [0 × 3] (S3: tbl_df/tbl/data.frame)\n $ variable: chr(0) \n $ category: chr(0) \n $ n       : num(0) \n\n\nAs you can see, we created an empty data frame with three columns. One to hold the variable names, one to hold the variable categories, and one to hold the count of occurrences of each category. Now, we can use a for loop to iteratively add results to our empty data frame structure. This works similarly to the way we added mean values to the xyz_means vector in the first example above. As a reminder, here is what the for loop code looks like:\n\nfor(i in c(\"age_group\", \"gender\", \"bmi_3cat\")) {\n  cat_stats &lt;- study %&gt;% \n    count(.data[[i]]) %&gt;%\n    mutate(variable = names(.)[1]) %&gt;%\n    rename(category = 1)\n  \n  cat_table &lt;- bind_rows(cat_table, cat_stats)\n}\n\nFor our next step, let’s walk through the first little chunk of code inside the for loop body. Specifically:\n\ncat_stats &lt;- study %&gt;% \n  count(.data[[i]]) %&gt;%\n  mutate(variable = names(.)[1]) %&gt;%\n  rename(category = 1)\n\nIf we were using this code to analyze a single variable, as opposed to using it in a for loop, this is what the result would look like:\n\ncat_stats &lt;- study %&gt;% \n  count(age_group) %&gt;%\n  mutate(variable = names(.)[1]) %&gt;%\n  rename(category = 1) %&gt;% \n  print()\n\n# A tibble: 3 × 3\n  category            n variable \n  &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;    \n1 Younger than 30    56 age_group\n2 30 and Older       11 age_group\n3 &lt;NA&gt;                1 age_group\n\n\nWe’ve already seen what the study %&gt;% count(age_group) part of the code does, and we already know that we can use mutate() to create a new column in our data frame. In this case, the name of the new column is variable. But, you may be wondering what the names(.)[1] after the equal sign does. Let’s take a look. Here, we can see the data frame that is getting passed to mutate():\n\ncat_stats &lt;- study %&gt;% \n  count(age_group) %&gt;% \n  print()\n\n# A tibble: 3 × 2\n  age_group           n\n  &lt;fct&gt;           &lt;int&gt;\n1 Younger than 30    56\n2 30 and Older       11\n3 &lt;NA&gt;                1\n\n\nIt’s a data frame with two columns. The first column actually has two different kinds of information that we need. It contains the name of the variable being analyzed as the column name, and it contains all the categories of that variable as the column values. We want to separate those two pieces of information into two columns. This task is similar to some of the “tidy data” tasks we worked through in the chapter on restructuring data frames. In fact, we can also use pivot_longer() to get the result we want:\n\nstudy %&gt;% \n  count(age_group) %&gt;% \n  tidyr::pivot_longer(\n    cols      = \"age_group\",\n    names_to  = \"variable\",\n    values_to = \"category\"\n  )\n\n# A tibble: 3 × 3\n      n variable  category       \n  &lt;int&gt; &lt;chr&gt;     &lt;fct&gt;          \n1    56 age_group Younger than 30\n2    11 age_group 30 and Older   \n3     1 age_group &lt;NA&gt;           \n\n\nIn this solution for this task, however, we’re not going to use pivot_longer() for a couple of reasons. First, it’s an opportunity for us to learn about the special use of dot (.) inside of dplyr verbs. Second, this solution will use dplyr only. It will not require us to use the tidyr package.\nBefore we talk about the dot, however, let’s make sure we know what the names()[1] is doing. There aren’t any new concepts here, but we may not have used them this way before. The name() function just returns a character vector containing the column names of the data frame we pass to it. So, when we pass the cat_stats data frame to it, this is what it returns:\n\nnames(cat_stats)\n\n[1] \"age_group\" \"n\"        \n\n\nWe want to use the first value, \"age_group\" to fill-in our the new variable column we want to create. We can use bracket notation to subset the first element of the character vector of column names above like this:\n\nnames(cat_stats)[1]\n\n[1] \"age_group\"\n\n\nWhat does the dot do? Well, outside of our dplyr pipeline, it doesn’t do anything useful:\n\nnames(.)[1]\n\nError: object '.' not found\n\n\nInside of our dplyr pipeline, you can think of it as a placeholder for whatever is getting passed to the dplyr verb – mutate() in this case. So, what is getting passed to mutate? The result of everything that comes before mutate() in the pipeline. And what does that result look like in this case? It looks like this:\n\nstudy %&gt;% \n  count(age_group)\n\n# A tibble: 3 × 2\n  age_group           n\n  &lt;fct&gt;           &lt;int&gt;\n1 Younger than 30    56\n2 30 and Older       11\n3 &lt;NA&gt;                1\n\n\nSo, we can use the dot inside of mutate as a substitute for the results data frame getting passed to mutate(). Said another way. To dplyr, this:\n\nnames(study %&gt;% count(age_group))\n\nand this:\n\nstudy %&gt;% count(age_group) %&gt;% names(.)\n\nare the exact same thing in this context:\n\ncat_stats &lt;- study %&gt;% \n  count(age_group) %&gt;%\n  mutate(variable = names(.)[1]) %&gt;%\n  print()\n\n# A tibble: 3 × 3\n  age_group           n variable \n  &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;    \n1 Younger than 30    56 age_group\n2 30 and Older       11 age_group\n3 &lt;NA&gt;                1 age_group\n\n\nNow, we have all the variables we wanted for our final results table. Keep in mind, however, that we will eventually be stacking similar results from our other variables (i.e., gender and bmi_3cat) below these results using bind_rows(). You may remember from the chapter on working with multiple data frames that the bind_rows() function matches columns together by name, not by position. So, we need to change the age_group column name to category. If we don’t, we will end up with something that looks like this:\n\nstudy %&gt;% \n  count(age_group) %&gt;% \n  bind_rows(study %&gt;% count(gender))\n\n# A tibble: 6 × 3\n  age_group           n gender\n  &lt;fct&gt;           &lt;int&gt; &lt;fct&gt; \n1 Younger than 30    56 &lt;NA&gt;  \n2 30 and Older       11 &lt;NA&gt;  \n3 &lt;NA&gt;                1 &lt;NA&gt;  \n4 &lt;NA&gt;               43 Female\n5 &lt;NA&gt;               24 Male  \n6 &lt;NA&gt;                1 &lt;NA&gt;  \n\n\nNot what we want, right? Again, if we were doing this analysis one variable at a time, our code might look like this:\n\ncat_stats &lt;- study %&gt;% \n  count(age_group) %&gt;% \n  mutate(variable = names(.)[1]) %&gt;% \n  rename(category = age_group) %&gt;% \n  print()\n\n# A tibble: 3 × 3\n  category            n variable \n  &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;    \n1 Younger than 30    56 age_group\n2 30 and Older       11 age_group\n3 &lt;NA&gt;                1 age_group\n\n\nWe used the rename() function above to change the name of the first column from age_group to category. Remember, the syntax for renaming columns with the rename() function is new_name = old_name. But, inside our for loop we will actually have 3 old names, right? In the first iteration old_name will be age_group, in the second iteration old_name will be gender, and in the third iteration old_name will be bmi_cat. We could loop over the names, but there’s an even easier solution. Instead of asking rename() to rename our column by name using this syntax, new_name = old_name, we can also ask rename() to rename our column by position using this syntax, new_name = column_number. So, in our example above, we could get the same result by replacing age_group with 1 because age_group is the first column in the data frame:\n\ncat_stats &lt;- study %&gt;% \n  count(age_group) %&gt;% \n  mutate(variable = names(.)[1]) %&gt;% \n  rename(category = 1) %&gt;% # Replace age_group with 1\n  print()\n\n# A tibble: 3 × 3\n  category            n variable \n  &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;    \n1 Younger than 30    56 age_group\n2 30 and Older       11 age_group\n3 &lt;NA&gt;                1 age_group\n\n\nAnd, using this method, we don’t have to make any changes to the value being passed to rename() when we are analyzing our other variables. For example:\n\ncat_stats &lt;- study %&gt;% \n  count(gender) %&gt;% # Changed the column from age_group to gender\n  mutate(variable = names(.)[1]) %&gt;% \n  rename(category = 1) %&gt;% # Still have 1 here\n  print()\n\n# A tibble: 3 × 3\n  category     n variable\n  &lt;fct&gt;    &lt;int&gt; &lt;chr&gt;   \n1 Female      43 gender  \n2 Male        24 gender  \n3 &lt;NA&gt;         1 gender  \n\n\nAt this point, we have all the elements we need manually create the data frame of final results we want. First, we create the empty results table:\n\ncat_table &lt;- tibble(\n  variable = vector(\"character\"), \n  category = vector(\"character\"), \n  n        = vector(\"numeric\")\n) %&gt;%\n  print()\n\n# A tibble: 0 × 3\n# ℹ 3 variables: variable &lt;chr&gt;, category &lt;chr&gt;, n &lt;dbl&gt;\n\n\nThen, we get the data frame of results for age_group:\n\ncat_stats &lt;- study %&gt;% \n  count(age_group) %&gt;%\n  mutate(variable = names(.)[1]) %&gt;% \n  rename(category = 1) %&gt;% \n  print()\n\n# A tibble: 3 × 3\n  category            n variable \n  &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;    \n1 Younger than 30    56 age_group\n2 30 and Older       11 age_group\n3 &lt;NA&gt;                1 age_group\n\n\nThen, we use bind_rows() to add those results to our cat_table data frame:\n\ncat_table &lt;- cat_table %&gt;% \n  bind_rows(cat_stats) %&gt;% \n  print()\n\n# A tibble: 3 × 3\n  variable  category            n\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n1 age_group Younger than 30    56\n2 age_group 30 and Older       11\n3 age_group &lt;NA&gt;                1\n\n\nThen, we copy and paste the last two steps above, replacing age_group with gender:\n\ncat_stats &lt;- study %&gt;% \n  count(gender) %&gt;% # Change to gender\n  mutate(variable = names(.)[1]) %&gt;% \n  rename(category = 1)\n\ncat_table &lt;- cat_table %&gt;% \n  bind_rows(cat_stats) %&gt;% \n  print()\n\n# A tibble: 6 × 3\n  variable  category            n\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n1 age_group Younger than 30    56\n2 age_group 30 and Older       11\n3 age_group &lt;NA&gt;                1\n4 gender    Female             43\n5 gender    Male               24\n6 gender    &lt;NA&gt;                1\n\n\nThen, we copy and paste the two steps above, replacing gender with bmi_3cat:\n\ncat_stats &lt;- study %&gt;% \n  count(bmi_3cat) %&gt;%  # Change to bmi_3cat\n  mutate(variable = names(.)[1]) %&gt;% \n  rename(category = 1)\n\ncat_table &lt;- cat_table %&gt;% \n  bind_rows(cat_stats) %&gt;% \n  print()\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nThat is exactly the final result we wanted, and you might have noticed that the only elements of the code chunks above that changed were the column names being passed to count(). If we can just figure out how to loop over the column names, then we can remove a ton of unnecessary repetition from our code. Our first attempt might look like this:\n\nfor(i in c(age_group, gender, bmi_3cat)) {\n  study %&gt;% \n    count(i) %&gt;% \n    mutate(variable = names(.)[1]) %&gt;% \n    rename(category = 1)\n}\n\nError: object 'age_group' not found\n\n\nHowever, it doesn’t work. In the code above, R is looking for and object in the global environment called age_group. Of course, there is no object in the global environment named age_group. Rather, there is an object in the global environment named study that has a column named age_group.\nWe can get rid of that error by wrapping each column name in quotes:\n\nfor(i in c(\"age_group\", \"gender\", \"bmi_3cat\")) {\n  study %&gt;% \n    count(i) %&gt;% \n    mutate(variable = names(.)[1]) %&gt;% \n    rename(category = 1)\n}\n\nError in `count()`:\n! Must group by variables found in `.data`.\n✖ Column `i` is not found.\n\n\nUnfortunately, that just gives us a different error. In the code above, count() is looking for a column named i in the study data frame. You may be wondering why i is not being converted to \"age_group\", \"gender\", and \"bmi_3cat\" in the code above. The short answer is that it’s because of tidy evaluation and data masking.\nSo, we need a way to iteratively pass each quoted column name to the count() function inside our for loop body, but also let dplyr know that they are column names, not just random character strings. Fortunately, the rlang package (which is partially imported with dplyr), provides us with a special construct that can help us solve this problem. It’s called the .data pronoun. Here’s how we can use it:\n\nfor(i in c(\"age_group\", \"gender\", \"bmi_3cat\")) {\n  study %&gt;% \n    count(.data[[i]]) %&gt;% \n    mutate(variable = names(.)[1]) %&gt;% \n    rename(category = 1) %&gt;% \n    print()\n}\n\n# A tibble: 3 × 3\n  category            n variable \n  &lt;fct&gt;           &lt;int&gt; &lt;chr&gt;    \n1 Younger than 30    56 age_group\n2 30 and Older       11 age_group\n3 &lt;NA&gt;                1 age_group\n# A tibble: 3 × 3\n  category     n variable\n  &lt;fct&gt;    &lt;int&gt; &lt;chr&gt;   \n1 Female      43 gender  \n2 Male        24 gender  \n3 &lt;NA&gt;         1 gender  \n# A tibble: 4 × 3\n  category       n variable\n  &lt;fct&gt;      &lt;int&gt; &lt;chr&gt;   \n1 Normal        43 bmi_3cat\n2 Overweight    16 bmi_3cat\n3 Obese          5 bmi_3cat\n4 &lt;NA&gt;           4 bmi_3cat\n\n\nHere’s how it works. Remember that data masking allows us to write column names directly in dplyr code without having to use dollar sign or bracket notation to tell R which data frame that column lives in. For example, in the following code, dplyr just “knows” that age_group is a column in the study data frame:\n\nstudy %&gt;% \n  count(age_group)\n\n# A tibble: 3 × 2\n  age_group           n\n  &lt;fct&gt;           &lt;int&gt;\n1 Younger than 30    56\n2 30 and Older       11\n3 &lt;NA&gt;                1\n\n\nThe same is not true for base R functions. For example, we can’t pass age_group directly to the table() function:\n\ntable(age_group)\n\nError: object 'age_group' not found\n\n\nWe have to use dollar sign or bracket notation to tell R that age_group is a column in study:\n\ntable(study[[\"age_group\"]])\n\n\nYounger than 30    30 and Older \n             56              11 \n\n\nThis is a really nice feature of dplyr when we’re using dplyr interactively. But, as we’ve already discussed, it does present us with some challenges when we use dplyr functions inside of the functions we write ourselves and inside of for loops.\nAs you can see in the code below, the tidy evaluation essentially blocks the i inside of count() from being replaced with each of the character strings we are looping over. Instead, dplyr looks for a literal i as a column name in the study data frame.\n\nfor(i in c(\"age_group\", \"gender\", \"bmi_3cat\")) {\n  study %&gt;% \n    count(i)\n}\n\nError in `count()`:\n! Must group by variables found in `.data`.\n✖ Column `i` is not found.\n\n\nSo, we need a way to tell dplyr that \"age_group\" is a column in the study data frame. Well, we know how to use quoted column names inside bracket notation. So, we could write code like this:\n\nstudy %&gt;% \n  count(study[[\"age_group\"]])\n\n# A tibble: 3 × 2\n  `study[[\"age_group\"]]`     n\n  &lt;fct&gt;                  &lt;int&gt;\n1 Younger than 30           56\n2 30 and Older              11\n3 &lt;NA&gt;                       1\n\n\nHowever, the column name (i.e., study[[\"age_group\"]]) in the results data frame above isn’t ideal to work with. Additionally, the code above isn’t very flexible because we have the study data frame hard-coded into it (i.e., study[[\"age_group\"]]). That’s where the .data pronoun comes to the rescue:\n\nstudy %&gt;% \n  count(.data[[\"age_group\"]])\n\n# A tibble: 3 × 2\n  age_group           n\n  &lt;fct&gt;           &lt;int&gt;\n1 Younger than 30    56\n2 30 and Older       11\n3 &lt;NA&gt;                1\n\n\nThe .data pronoun “is not a data frame; it’s a special construct, a pronoun, that allows you to access the current variables either directly, with .data$x or indirectly with .data[[var]].”1\nWhen we put it all together, our code looks like this:\n\n# Create the data frame structure that will contain our results\ncat_table &lt;- tibble(\n  variable = vector(\"character\"), \n  category = vector(\"character\"), \n  n        = vector(\"numeric\")\n) \n\n# For each column, get the column name, category names, and count.\n# Then, add them to the bottom of the results data frame we created above.\nfor(i in c(\"age_group\", \"gender\", \"bmi_3cat\")) {\n  cat_stats &lt;- study %&gt;% \n    count(.data[[i]]) %&gt;% # Use .data to refer to the current data frame.\n    mutate(variable = names(.)[1]) %&gt;% # Use . to refer to the current data frame.\n    rename(category = 1)\n  \n  # Here is where we update cat_table with the results for each column\n  cat_table &lt;- bind_rows(cat_table, cat_stats)\n}\n\n\ncat_table\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nAnd, we can do other interesting things with our results now that we have it in this format. For example, we can easily add percentages along with our counts like this:\n\ncat_table %&gt;% \n  group_by(variable) %&gt;% \n  mutate(\n    percent = n / sum(n) * 100\n  )\n\n# A tibble: 10 × 4\n# Groups:   variable [3]\n   variable  category            n percent\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 age_group Younger than 30    56   82.4 \n 2 age_group 30 and Older       11   16.2 \n 3 age_group &lt;NA&gt;                1    1.47\n 4 gender    Female             43   63.2 \n 5 gender    Male               24   35.3 \n 6 gender    &lt;NA&gt;                1    1.47\n 7 bmi_3cat  Normal             43   63.2 \n 8 bmi_3cat  Overweight         16   23.5 \n 9 bmi_3cat  Obese               5    7.35\n10 bmi_3cat  &lt;NA&gt;                4    5.88\n\n\nFinally, we could also write our own function that uses the code above. That way, we can easily reuse this code in the future:\n\ncat_stats &lt;- function(data, ...) {\n  # Create the data frame structure that will contain our results\n  cat_table &lt;- tibble(\n    variable = vector(\"character\"), \n    category = vector(\"character\"), \n    n        = vector(\"numeric\")\n  ) \n  \n  # For each column in ..., get the column name, category names, and count.\n  # Then, add them to the bottom of the results data frame we created above.\n  for(i in c(...)) {\n    stats &lt;- data %&gt;% \n      count(.data[[i]]) %&gt;% # Use .data to refer to the current data frame.\n      mutate(variable = names(.)[1]) %&gt;% # Use . to refer to the current data frame.\n      rename(category = 1)\n    \n    # Here is where we update cat_table with the results for each column\n    cat_table &lt;- bind_rows(cat_table, stats)\n  }\n  # Return results\n  cat_table\n}\n\n\ncat_stats(study, \"age_group\", \"gender\", \"bmi_3cat\")\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nWe covered a lot of material in this chapter. For loops tend to be confusing for people who are just learning to program. When you throw in the tidy evaluation stuff, it can be really confusing – even for experienced R programmers. So, if you are still feeling a little confused, don’t beat yourself up. Also, trying to memorize everything we covered in this chapter is not recommended. Instead, we recommend that you read it until you have understood what for loops are and when they might be useful at a high level. Then, refer back to this chapter (or other online references that discuss for loops) if you find yourself in a situation where you believe that for loops might be the right tool to help you complete a given programming task. Having said that, also keep in mind that for loops are rarely the only tool you will have at your disposal to complete the task. In the next chapter, we will learn how to use functionals, specifically the purrr package, in place of for loops. You may find this approach to iteration more intuitive.\n\n\n\n\n1. Wickham H, François R, Henry L, Müller K, RStudio. Programming with Dplyr.; 2020.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Writing For Loops</span>"
    ]
  },
  {
    "objectID": "chapters/using_purrr/using_purrr.html",
    "href": "chapters/using_purrr/using_purrr.html",
    "title": "37  Using the purrr Package",
    "section": "",
    "text": "37.1 Comparing for loops and the map functions\nIn this final chapter of the repeated operations part of the book, we are going to discuss the purrr package.\nThe purrr package provides a really robust set of functions that can help us more efficiently complete a bunch of different tasks in R. For the purposes of this chapter, however, we are going to focus on using the purrr::map functions as an alternative approach to removing unnecessary repetition from the various different code chunks we’ve already seen in other chapters.\nFor our purposes, you can think of the purrr::map functions as a replacement for for loops. In other words, you can think of them as doing the same thing as a for loop, but writing the code in a different way.\nAs usual, let’s start by taking a look at a simple example – the same one we used to start the chapter on column-wise operations and the chapter on writing for loops. Afterwards, we will compare the basic structure of purrr::map functions to the basic structure of for loops. Finally, we will work through a number of the examples we’ve already worked through in this part of the book using the purrr approach.\nAt this point, we will go ahead and load dplyr and purrr and simulate our data:\nIn the chapter on column-wise operations we used dplyr’s across() function to efficiently find the mean of each column in the df_xyz data frame:\nIn the chapter on writing for loops, we learned an alternative approach that would also work:\nAn alternative way to complete the analysis above is with the map_dbl() function from the purrr package like this:\n👆Here’s what we did above:\nAs you can see, using the map_dbl() package requires far less code than the for loop did, which has at least two potential advantages. First, it’s less typing, which means less opportunity for typos. Second, many people in the R community feel as though this functional (i.e., use of a function) approach to iteration is much easier to read and understand than the traditional for loop approach.\nAdditionally, you may have also noticed that we were able to assign the returned results of map_dbl(df_xyz, mean) to an object in our global environment in the usual way (i.e., with the assignment arrow). This eliminates the need for creating a structure to hold our results ahead of time as we had to do with the for loop.\nFinally, when we use map_dbl() there isn’t a leftover index variable (i.e., i) floating around our global environment the way there was when we were writing for loops.\nFor those reasons, and possibly others, it’s been an observation that the majority of R users prefer the functional approach to iteration – either purrr or the apply functions – over using for loops in most situations.\nHowever, some of you reading this text book might have had their first experiences with programming in a language other than R that relied more heavily on for loops. For this reason, you might tend to think first in terms of a for loop and then mentally convert the for loop to a map function before writing your code. Therefore, the next section is going to compare and contrast the basic for loop with the map functions. You may find this section instructive or interesting even if you aren’t someone who first learned iteration using for loops.\nComparing for loops and map functions\nIn this section, we will compare for loops and the purrr::map functions using the example from the beginning of the chapter.\nComparing for loops and map functions using example\nIt’s probably obvious to you at this point, but when using purrr::map instead of a for loop, we will be using one of the map functions instead of the for() function.\nAssigning the results of map function to object\nNext, as previously discussed above, we are able to assign the returned results of map_dbl(df_xyz, mean) to an object in our global environment in the usual way (i.e., with the assignment arrow). This eliminates the need for creating a structure to hold our results ahead of time as we had to do with the for loop. It also eliminates the need to write code that explicitly updates the returned results structure at each iteration (i.e., xyz_means[[i]]) as we had to do with the for loop.\nHowever, one nice byproduct of creating the structure to hold our returned results ahead of time was that doing so made it obvious what form and type we expected our results to take.\nSetting the type of the returned results structure I\nIn the xyz_means example above, it’s obvious that we expected our returned results to be a vector of numbers because the structure we created to contain our results was a vector of type double.\nSetting the type of the returned results structure II\nWhen using the purrr::map functions, which map function we choose will serve the same purpose. In the example above, we used map_dbl(), which implied that we expected our results to be a vector of type double. In fact, it not only implied that our results should be a vector of type double, but it guaranteed that our results would be a vector of type double (or we would get an error). In this sense, the map functions are much safer to use than for loops – we don’t get unexpected results.\nAs a silly example, let’s say that we want to extract the number of letters in each name contained in a vector of names. We’ll start by creating a vector that contains three random names:\nnames &lt;- c(\"Avril\", \"Joe\", \"Whitney\")\nNext, let’s create a structure to contain our results:\nn_letters &lt;- vector(\"double\", length(names)) # Expecting double\nThe code above (i.e., vector(\"double\", length(names))) implies that we expect our results to be type double, which make sense if we expect our results to be the number of letters in some names.\nFinally, let’s write our for loop:\nfor(i in seq_along(names)) {\n  n_letters[[i]] &lt;- stringr::str_extract(names[[i]], \"\\\\w\") # Returns character\n}\n\nn_letters\n\n[1] \"A\" \"J\" \"W\"\nUh, oh! Those “counts” are letters! What happened? Well, apparently we thought that stringr::str_extract(names[[i]], \"\\\\w\") would return the count of letters in each name. In actuality, it returns the first letter in each name.\nAgain, this is a silly example. In this case, it’s easy to see and fix our mistake. However, it could be very difficult to debug this problem if the code were buried in a long script or inside of other functions.\nNow, let’s see what happens when we use purrr. We still start with the names:\nnames &lt;- c(\"Avril\", \"Joe\", \"Whitney\")\nWe also still imply our expectations that the returned result should be a numeric vector. However, this time we do so by using the map_dbl function:\nn_letters &lt;- map_dbl(\n  .x = names, \n  .f = stringr::str_extract, \"\\\\w{1}\"\n)\n\nError in `map_dbl()`:\nℹ In index: 1.\nCaused by error:\n! Can't coerce from a string to a double.\nBut, this time, we don’t get an unexpected result. This time, we get an error. This may seem like a pain if you are newish to programming. But it’s much better to get an error that you can go fix than an incorrect result that you are totally unaware of!\nWhile we are discussing return types, let’s go ahead and introduce some of the other map functions. They are:\nReplacing the arguments in the for function with the object we pass to the .x argument of the map function\nNext, the object we pass to the .x function of the map function replaces the entire i in seq_along(object) pattern that is passed to the for loop. Again, if the object passed to the .x argument is a vector, then map will apply the function passed to the .f argument to each element of the vector. If the object passed to the .x argument is a data frame, then map will apply the function passed to the .f argument to each column of the data frame.\nReplacing the body of the for function with the function we pass to the .f argument of the map function\nFinally, the function passed to the .f argument can replace the rest of the “stuff” going on in the for loop body. We can pass a single function (e.g., mean) to the .f argument as we did above. However, we can also pass anonymous functions to the .f argument. We pass anonymous functions to the .f function in basically the exact same way passed anonymous functions to the .fns argument of the across() function in the chapter on column-wise operations. And, yes, we can also write our anonymous functions using purrr-style lambdas. In fact, the purrr-style lambda syntax is called the purrr-style lambda syntax because it was first created for the purrr package and later adopted by dplyr::across(). That name probably makes a lot more sense than it did a couple of chapters ago!\nThat pretty much covers the basics of using the purrr::map functions. If you’ve been reading this book in sequence, there won’t really be any conceptually new material in this chapter. We’re basically going to do the same things we’ve been doing for the last couple of chapters. We’ll just be using a slightly different (and perhaps preferable) syntax. If you haven’t been reading the book in sequence, you might want to read the chapters on writing functions, column-wise operations, and writing for loops to get the most of the examples below.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Using the purrr Package</span>"
    ]
  },
  {
    "objectID": "chapters/using_purrr/using_purrr.html#comparing-for-loops-and-the-map-functions",
    "href": "chapters/using_purrr/using_purrr.html#comparing-for-loops-and-the-map-functions",
    "title": "37  Using the purrr Package",
    "section": "",
    "text": "map_dbl(), which we’ve already seen. The map_dbl() function always returns a numeric vector or an error.\nmap_int(), which always returns an integer vector or an error.\nmap_lgl(), which always returns a logical vector or an error.\nmap_chr(), which always returns a character vector or an error.\nmap_dfr(), which always returns a data frame created by row-binding results or an error.\nmap_dfc(), which always returns a data frame created by column-binding results or an error.\nmap(), which is the most generic, and always returns a list (or an error). We’ve haven’t discussed lists much in this book, but whenever something won’t fit into any other kind of object, it will fit into a list.\nwalk(), which is the only map function without a map name. We will use walk() when we are more interested in the “side-effects” of the function passed to .f than its return value. What in the world does that mean? It means that the only thing walk() “returns” is exactly what was passed to its .x argument. No matter what you pass to the .f argument, the object passed to .x will be returned by walk() unmodified. Your next question might be, “then what’s the point? How could that ever be useful?” Typically, walk() will only be useful to us for plotting (e.g., where you are interested in viewing the plots, but not saving them as an object) and/or data transfer (we will see an example of this below).",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Using the purrr Package</span>"
    ]
  },
  {
    "objectID": "chapters/using_purrr/using_purrr.html#using-purrr-for-data-transfer",
    "href": "chapters/using_purrr/using_purrr.html#using-purrr-for-data-transfer",
    "title": "37  Using the purrr Package",
    "section": "37.2 Using purrr for data transfer",
    "text": "37.2 Using purrr for data transfer\n\n37.2.1 Example 1: Importing multiple sheets from an Excel workbook\nIn the chapter on writing functions we used a for loop to help us import data from an Excel workbook that was stored across multiple sheets. We will once again go through this example using the purrr approach.\nThe simulated data contains some demographic information about three different cities: Houston, Atlanta, and Charlotte. In this scenario, we need to import each sheet, clean the data, and combine them into a single data frame in order to complete our analysis. First, we will load the readxl package:\n\nlibrary(readxl)\n\nYou may click here to download this file to your computer.\nThen, we may import each sheet like this:\n\n\nhouston &lt;- read_excel(\n  \"city_ses.xlsx\",\n  sheet = \"Houston\"\n)\n\natlanta &lt;- read_excel(\n  \"city_ses.xlsx\",\n  sheet = \"Atlanta\"\n)\n\ncharlotte &lt;- read_excel(\n  \"city_ses.xlsx\",\n  sheet = \"Charlotte\"\n)\n\n🚩In the code chunks above, we have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition from our code. So, our next step was to write a function to remove some of the unnecessary repetition:\n\n\nimport_cities &lt;- function(sheet) {\n  df &lt;- read_excel(\n    \"city_ses.xlsx\",\n    sheet = sheet\n  )\n}\n\nhouston   &lt;- import_cities(\"Houston\")\natlanta   &lt;- import_cities(\"Atlanta\")\ncharlotte &lt;- import_cities(\"Charlotte\")\n\n🚩However, that approach still has some repetition. So, we next learned how to use a for loop as an alternative approach:\n\n\npath &lt;- \"city_ses.xlsx\"\nsheets &lt;- excel_sheets(path)\n\nfor(i in seq_along(sheets)) {\n  new_nm &lt;- tolower(sheets[[i]])\n  assign(new_nm, read_excel(path, sheet = sheets[[i]]))\n}\n\nThat works just fine! However, we could alternatively use purrr::walk() instead like this:\n\n\n# Save the file path to an object so we don't have to type it repeatedly \n# or hard-code it in.\npath &lt;- \"city_ses.xlsx\"\n  \nwalk(\n  .x = excel_sheets(path),\n  .f = function(x) {\n    new_nm &lt;- tolower(x)\n    assign(new_nm, read_excel(path, sheet = x), envir = .GlobalEnv)\n  }\n)\n\n\nhouston\n\n# A tibble: 5 × 4\n  pid     age sex   ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 001      13 F            88\n2 003      13 F            78\n3 007      14 M            83\n4 014      12 F            76\n5 036      13 M            84\n\n\n\natlanta\n\n# A tibble: 5 × 4\n  id      age gender ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 002      14 M             64\n2 009      15 M             35\n3 012      13 F             70\n4 013      13 F             66\n5 022      12 F             59\n\n\n\ncharlotte\n\n# A tibble: 5 × 4\n  pid     age sex     ses\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 004      13 F        84\n2 011      14 M        66\n3 018      12 M        92\n4 023      12 M        89\n5 030      13 F        83\n\n\n👆Here’s what we did above:\n\nWe used the walk() function from the purrr package to import every sheet from an Excel workbook.\nFirst, we saved the path to the Excel workbook to a separate object. We didn’t have to do this. However, doing so prevented us from having to type out the full file path repeatedly in the rest of our code. Additionally, if the file path ever changed, we would only have to update it in one place.\nSecond, we passed the return value of the excel_sheets() function, which is a character vector containing each sheet name, to the .x argument of the walk() function. We didn’t have to do this. We could have typed each sheet name manually. However, there shouldn’t be any accidental typos if we use the excel_sheets() function, and we don’t have to make any changes to our code if more sheets are added to the Workbook in the future.\nThird, we passed an anonymous function to the walk()’s .f argument. Inside the anonymous function, we assigned each data frame created by the read_excel() function to our global environment using the assign() function. Notice that because we are using the assign() inside of another function, we have to explicitly tell the assign() function to assign the data frames being imported to the global environment using envir = .GlobalEnv. Without getting too technical, keep in mind that functions create their own little enclosed environments (see a discussion here), which makes the envir = .GlobalEnv part necessary.\n\nAdditionally, you may have some questions swirling around your head right now about the walk() function itself. In particular, you might be wondering why we used walk() instead of map() and why we didn’t assign the return value of walk() to an object. We’ll answer both questions next.\n\n\n37.2.2 Why walk instead of map?\nThe short answer is that map functions return one thing (i.e., a vector, list, or data frame). In this situation, we wanted to “return” three things (i.e., the houston data frame, the atlanta data frame, and the charlotte data frame).\nTechnically, we could have used the map() function to return a list of data frames like this:\n\nlist_of_df &lt;- map(\n  .x = excel_sheets(path),\n  .f = ~ read_excel(path, sheet = .x)\n)\n\n\nstr(list_of_df)\n\nList of 3\n $ : tibble [5 × 4] (S3: tbl_df/tbl/data.frame)\n  ..$ pid      : chr [1:5] \"001\" \"003\" \"007\" \"014\" ...\n  ..$ age      : num [1:5] 13 13 14 12 13\n  ..$ sex      : chr [1:5] \"F\" \"F\" \"M\" \"F\" ...\n  ..$ ses_score: num [1:5] 88 78 83 76 84\n $ : tibble [5 × 4] (S3: tbl_df/tbl/data.frame)\n  ..$ id       : chr [1:5] \"002\" \"009\" \"012\" \"013\" ...\n  ..$ age      : num [1:5] 14 15 13 13 12\n  ..$ gender   : chr [1:5] \"M\" \"M\" \"F\" \"F\" ...\n  ..$ ses_score: num [1:5] 64 35 70 66 59\n $ : tibble [5 × 4] (S3: tbl_df/tbl/data.frame)\n  ..$ pid: chr [1:5] \"004\" \"011\" \"018\" \"023\" ...\n  ..$ age: num [1:5] 13 14 12 12 13\n  ..$ sex: chr [1:5] \"F\" \"M\" \"M\" \"M\" ...\n  ..$ ses: num [1:5] 84 66 92 89 83\n\n\nFrom there, we could extract and modify each data frame from the list like this:\n\nhouston &lt;- list_of_df[[1]]\nhouston\n\n# A tibble: 5 × 4\n  pid     age sex   ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 001      13 F            88\n2 003      13 F            78\n3 007      14 M            83\n4 014      12 F            76\n5 036      13 M            84\n\n\n\natlanta &lt;- list_of_df[[2]]\natlanta\n\n# A tibble: 5 × 4\n  id      age gender ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 002      14 M             64\n2 009      15 M             35\n3 012      13 F             70\n4 013      13 F             66\n5 022      12 F             59\n\n\n\ncharlotte &lt;- list_of_df[[2]]\ncharlotte\n\n# A tibble: 5 × 4\n  id      age gender ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 002      14 M             64\n2 009      15 M             35\n3 012      13 F             70\n4 013      13 F             66\n5 022      12 F             59\n\n\nOf course, now we have a bunch of repetition again! Alternatively, we could have also used the map_dfr(), which always returns a data frame created by row-binding results or an error. You can think of map_dfr() as taking the three data frames above and passing them to the bind_rows() function and returning that result:\n\n# Passing list_of_df to bind_rows()\nbind_rows(list_of_df)\n\n# A tibble: 15 × 7\n   pid     age sex   ses_score id    gender   ses\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 001      13 F            88 &lt;NA&gt;  &lt;NA&gt;      NA\n 2 003      13 F            78 &lt;NA&gt;  &lt;NA&gt;      NA\n 3 007      14 M            83 &lt;NA&gt;  &lt;NA&gt;      NA\n 4 014      12 F            76 &lt;NA&gt;  &lt;NA&gt;      NA\n 5 036      13 M            84 &lt;NA&gt;  &lt;NA&gt;      NA\n 6 &lt;NA&gt;     14 &lt;NA&gt;         64 002   M         NA\n 7 &lt;NA&gt;     15 &lt;NA&gt;         35 009   M         NA\n 8 &lt;NA&gt;     13 &lt;NA&gt;         70 012   F         NA\n 9 &lt;NA&gt;     13 &lt;NA&gt;         66 013   F         NA\n10 &lt;NA&gt;     12 &lt;NA&gt;         59 022   F         NA\n11 004      13 F            NA &lt;NA&gt;  &lt;NA&gt;      84\n12 011      14 M            NA &lt;NA&gt;  &lt;NA&gt;      66\n13 018      12 M            NA &lt;NA&gt;  &lt;NA&gt;      92\n14 023      12 M            NA &lt;NA&gt;  &lt;NA&gt;      89\n15 030      13 F            NA &lt;NA&gt;  &lt;NA&gt;      83\n\n\n\n# Using map_dfr() to directly produce the same result\ncities &lt;- map_dfr(\n  .x = excel_sheets(path),\n  .f = ~ read_excel(path, sheet = .x)\n)\n\ncities\n\n# A tibble: 15 × 7\n   pid     age sex   ses_score id    gender   ses\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 001      13 F            88 &lt;NA&gt;  &lt;NA&gt;      NA\n 2 003      13 F            78 &lt;NA&gt;  &lt;NA&gt;      NA\n 3 007      14 M            83 &lt;NA&gt;  &lt;NA&gt;      NA\n 4 014      12 F            76 &lt;NA&gt;  &lt;NA&gt;      NA\n 5 036      13 M            84 &lt;NA&gt;  &lt;NA&gt;      NA\n 6 &lt;NA&gt;     14 &lt;NA&gt;         64 002   M         NA\n 7 &lt;NA&gt;     15 &lt;NA&gt;         35 009   M         NA\n 8 &lt;NA&gt;     13 &lt;NA&gt;         70 012   F         NA\n 9 &lt;NA&gt;     13 &lt;NA&gt;         66 013   F         NA\n10 &lt;NA&gt;     12 &lt;NA&gt;         59 022   F         NA\n11 004      13 F            NA &lt;NA&gt;  &lt;NA&gt;      84\n12 011      14 M            NA &lt;NA&gt;  &lt;NA&gt;      66\n13 018      12 M            NA &lt;NA&gt;  &lt;NA&gt;      92\n14 023      12 M            NA &lt;NA&gt;  &lt;NA&gt;      89\n15 030      13 F            NA &lt;NA&gt;  &lt;NA&gt;      83\n\n\nThere would be absolutely nothing wrong with taking this approach and then cleaning up the combined data you see above. However, in this case, the preference was to import each sheet as a separate data frame, clean up each separate data frame, and then combine ourselves. If your preference is to use map_dfr() instead, then you definitely should.\n\n\n37.2.3 why we didn’t assign the return value of walk() to an object?\nAs we discussed above, the only thing walk() “returns” is exactly what was passed to its .x argument. No matter what you pass to the .f argument, the object passed to .x will be returned by walk() unmodified. In this case, that would just be the sheet names:\n\nreturned_by_walk &lt;- walk(\n  .x = excel_sheets(path),\n  .f = function(x) {\n    new_nm &lt;- tolower(x)\n    assign(new_nm, read_excel(path, sheet = x), envir = .GlobalEnv)\n  }\n)\n\nreturned_by_walk\n\n[1] \"Houston\"   \"Atlanta\"   \"Charlotte\"\n\n\nDon’t be confused, the data frames are still being imported and assigned to the global environment via the anonymous function we passed to .f above. But, but those data frames aren’t the values returned by walk() – They are a side-effect of the operations taking place inside of walk().\nFinally, we could make our original walk() code slightly more concise by using the purrr-style lambda syntax to write our anonymous function like this:\n\npath &lt;- \"city_ses.xlsx\"\n  \nwalk(\n  .x = excel_sheets(path),\n  .f = ~ assign(tolower(.), read_excel(path, sheet = .), envir = .GlobalEnv)\n)\n\n\nhouston\n\n# A tibble: 5 × 4\n  pid     age sex   ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 001      13 F            88\n2 003      13 F            78\n3 007      14 M            83\n4 014      12 F            76\n5 036      13 M            84\n\n\n\natlanta\n\n# A tibble: 5 × 4\n  id      age gender ses_score\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 002      14 M             64\n2 009      15 M             35\n3 012      13 F             70\n4 013      13 F             66\n5 022      12 F             59\n\n\n\ncharlotte\n\n# A tibble: 5 × 4\n  pid     age sex     ses\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 004      13 F        84\n2 011      14 M        66\n3 018      12 M        92\n4 023      12 M        89\n5 030      13 F        83",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Using the purrr Package</span>"
    ]
  },
  {
    "objectID": "chapters/using_purrr/using_purrr.html#using-purrr-for-data-management",
    "href": "chapters/using_purrr/using_purrr.html#using-purrr-for-data-management",
    "title": "37  Using the purrr Package",
    "section": "37.3 Using purrr for data management",
    "text": "37.3 Using purrr for data management\n\n37.3.1 Example 1: Adding NA at multiple positions\nWe’ll start this section with a relatively simple example using the same data we used to start the chapter on column-wise operations and the chapter on writing for loops.\n\nset.seed(123)\ndf_xyz &lt;- tibble(\n  x = rnorm(10),\n  y = rnorm(10),\n  z = rnorm(10)\n) %&gt;% \n  print()\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560   1.22  -1.07 \n 2 -0.230   0.360 -0.218\n 3  1.56    0.401 -1.03 \n 4  0.0705  0.111 -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  -1.69 \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97   0.153\n 9 -0.687   0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nIn those chapters, we used the code below to add missing values to our data frame:\n\ndf_xyz$x[2] &lt;- NA_real_\ndf_xyz$y[4] &lt;- NA_real_\ndf_xyz$z[6] &lt;- NA_real_\ndf_xyz\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560   1.22  -1.07 \n 2 NA       0.360 -0.218\n 3  1.56    0.401 -1.03 \n 4  0.0705 NA     -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  NA    \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97   0.153\n 9 -0.687   0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nDealing with those missing values, rather than adding those missing values was the point of the previously mentioned examples. So, we ignored the unnecessary repetition in the code above. But, for all the reasons we’ve been discussing, we should strive to write more robust code. Imagine, for example, that you were adding missing data to hundreds or thousands of rows as part of a simulation study. Using the method above would become problematic pretty quickly.\nIn this case, it might be useful to start our solution with writing a function (click here to review function writing). Let’s name our function add_na_at() because it helps us add an NA value to a vector at a position of our choosing. Logically, then, it follows that we will need to be able to pass our function a vector that we want to add the NA value to, and a position to add the NA value at. So, our first attempt might look something like this:\n\nadd_na_at &lt;- function(vect, pos) {\n  vect[[pos]] &lt;- NA\n}\n\nLet’s test it out:\n\nadd_na_at(df_xyz$x, 2) %&gt;% print()\n\n[1] NA\n\n\nIs a single NA the result we wanted? Nope! If this result is surprising to you, please review the section of the writing functions chapter on return values. Briefly, the last line of our function body is the single value df_xyz$x[[2]], which was set to be equal to NA. But, we don’t want our function to return just one position of the vector – we want it to return the entire vector. So, let’s reference the entire vector on the last line of the function body:\n\nadd_na_at &lt;- function(vect, pos) {\n  vect[[pos]] &lt;- NA\n  vect\n}\n\n\nadd_na_at(df_xyz$x, 2)\n\n [1] -0.56047565          NA  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\n\nThat’s better! Again, we know that data frame columns are vectors, so we can use our new function inside of mutate to add NA values to each column in our data frame at a position of our choosing:\n\ndf_xyz %&gt;% \n  mutate(\n    x = add_na_at(x, 2),\n    y = add_na_at(y, 4),\n    z = add_na_at(z, 6)\n  )\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560   1.22  -1.07 \n 2 NA       0.360 -0.218\n 3  1.56    0.401 -1.03 \n 4  0.0705 NA     -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  NA    \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97   0.153\n 9 -0.687   0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nI can hear what you are saying now. “Sure, that’s the result we wanted, but we didn’t eliminate very much repetitive code.” You are not wrong. A case could be made that this code is easier to quickly glance at and understand, but it isn’t much less repetitive. That’s where purrr comes in. Let’s try using purrr to come up with a better solution now.\nThe first question we might ask ourselves is, “which map function should we choose?” Well, we know we want our end result to be a data frame, so it makes sense for us to choose either map_dfr or map_dfc. However, it might be useful to start with the plain map() function that returns a list as we begin to experiment with solving a problem using purrr. This is because R can put almost anything into a list, and therefore, we will almost always get something returned to us (as opposed to an error) by map(). Further, the thing returned to us typically can provide us with some insight into what’s going on inside .f.\nNext, we know that we want to iterate over every column of the df_xyz data frame. So, we can pass it to the .x argument.\nWe also know that we want each column to get passed to the vect argument of add_na_at() iteratively. So, we want to pass add_na_at (without parentheses) to the .f argument.\nFinally, we can’t supply add_na_at() with just one argument – the vector – can we?\n\nadd_na_at(df_xyz$x)\n\nError in vect[[pos]] &lt;- NA: missing subscript\n\n\nNo way! We have to give it position as well. Do you remember which argument allows us to pass any additional arguments to the function we passed to the .f argument?\nThe ... argument is where we pass any additional arguments to the function we passed to the .f argument. But remember, we don’t actually type out ... =. We simply type additional arguments, separated by commas, after the function name supplied to .f:\n\nmap(\n  .x = df_xyz,\n  .f = add_na_at, 2\n)\n\n$x\n [1] -0.56047565          NA  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\n$y\n [1]  1.2240818         NA  0.4007715         NA -0.5558411  1.7869131\n [7]  0.4978505 -1.9666172  0.7013559 -0.4727914\n\n$z\n [1] -1.0678237         NA -1.0260044 -0.7288912 -0.6250393         NA\n [7]  0.8377870  0.1533731 -1.1381369  1.2538149\n\n\nOr alternatively, we can use the purrr-style lambda to pass our function to .f:\n\nmap(\n  .x = df_xyz,\n  .f = ~ add_na_at(.x, 2)\n)\n\n$x\n [1] -0.56047565          NA  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\n$y\n [1]  1.2240818         NA  0.4007715         NA -0.5558411  1.7869131\n [7]  0.4978505 -1.9666172  0.7013559 -0.4727914\n\n$z\n [1] -1.0678237         NA -1.0260044 -0.7288912 -0.6250393         NA\n [7]  0.8377870  0.1533731 -1.1381369  1.2538149\n\n\nNotice that we have to use the special .x symbol inside the function call where we would normally want to type the name of the column we want the function to operate on. We saw something similar before in the chapter on column-wise operations.\nNow, let’s discuss the result we are getting. The result you see above is a list, which is what map() will always return to us. Specifically, this is a list with three elements – x, y, and z. Each element of the list is a vector of numbers. Does this feel familiar? Does it seem sort of similar to a data frame? If so, good intuition! In R, a data frame is a list. It’s simply a special case of a list. It’s a special case because all vectors in the data frame must have the length, and because R knows to print each vector to the screen as a column. In fact, we can easily convert the list above to a data frame by passing it to the as.data.frame() function:\n\nmap(\n  .x = df_xyz,\n  .f = ~ add_na_at(.x, 2)\n) %&gt;% \n  as.data.frame()\n\n             x          y          z\n1  -0.56047565  1.2240818 -1.0678237\n2           NA         NA         NA\n3   1.55870831  0.4007715 -1.0260044\n4   0.07050839         NA -0.7288912\n5   0.12928774 -0.5558411 -0.6250393\n6   1.71506499  1.7869131         NA\n7   0.46091621  0.4978505  0.8377870\n8  -1.26506123 -1.9666172  0.1533731\n9  -0.68685285  0.7013559 -1.1381369\n10 -0.44566197 -0.4727914  1.2538149\n\n\nAlternatively, we could just use map_dfc as a shortcut instead:\n\nmap_dfc(\n  .x = df_xyz,\n  .f = ~ add_na_at(.x, 2)\n)\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560   1.22  -1.07 \n 2 NA      NA     NA    \n 3  1.56    0.401 -1.03 \n 4  0.0705 NA     -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  NA    \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97   0.153\n 9 -0.687   0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nWhy map_dfc instead of map_dfr? Because we want to combine x, y, and z together as columns, not as rows.\nOk, so we almost have the solution we want. There’s just one problem. In the code above, the NA is always being put into the second position because we have 2 hard-coded into add_na_at(.x, 2). We need a way to iterate over our columns and a set of numbers simultaneously in order to get the final result we want. Fortunately, that’s exactly what the map2 variants (i.e., map2_dbl(), map2_int(), map2_lgl(), etc.) of each of the map functions allows us to do.\nInstead of supplying map a single object to iterate over (i.e., .x) we can supply it with two objects to iterate over (i.e., .x and .y):\n\nmap2_dfc(\n  .x = df_xyz,\n  .y = c(2, 4, 6),\n  .f = ~ add_na_at(.x, .y)\n)\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560   1.22  -1.07 \n 2 NA       0.360 -0.218\n 3  1.56    0.401 -1.03 \n 4  0.0705 NA     -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  NA    \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97   0.153\n 9 -0.687   0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nThis can sometimes take a second to wrap your mind around. Here’s an illustration that may help:\n\n\n\n\n\nIllustrating how map iterates over two objects simultaneously - I\n\n\n\n\nIn the first iteration, .x took on the value of the first column in the df_xyz data frame (i.e., x) and .y took on the value of the first element in the numeric vector that we passed to the .y argument (i.e., 2). Then, the .x and .y were replaced with df_xyz$x and 2 respectively in the function we passed to .f. The result of that iteration was a vector of numbers that was identical to df_xyz$x except that its second element was an NA.\n\n\n\n\n\nIllustrating how map iterates over two objects simultaneously - II\n\n\n\n\nIn the second iteration, .x took on the value of the second column in the df_xyz data frame (i.e., y) and .y took on the value of the second element in the numeric vector that we passed to the .y argument (i.e., 4). Then, the .x and .y were replaced with df_xyz$y and 4 respectively in the function we passed to .f. The result of that iteration was a vector of numbers that was identical to df_xyz$y except that its fourth element was an NA.\n\n\n\n\n\nIllustrating how map iterates over two objects simultaneously - III\n\n\n\n\nIn the third iteration, .x took on the value of the third column in the df_xyz data frame (i.e., z) and .y took on the value of the third element in the numeric vector that we passed to the .y argument (i.e., 6). Then, the .x and .y were replaced with df_xyz$z and 6 respectively in the function we passed to .f. The result of that iteration was a vector of numbers that was identical to df_xyz$z except that its sixth element was an NA.\nFinally, map2_dfc() passed all of these vectors to bind_cols() (invisibly to us) and returned them as a data frame.\nThe code above gives us our entire solution. But, if we really were using this code in a simulation with hundreds or thousands of columns, we probably wouldn’t want to manually supply a vector of column positions to the .y argument. Instead, we could use the sample() function to supply random column positions to the .y argument like this:\n\nset.seed(8142020)\n\nmap2_dfc(\n  .x = df_xyz,\n  .y = sample(1:10, 3, TRUE),\n  .f = ~ add_na_at(.x, .y)\n)\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560  NA     -1.07 \n 2 NA       0.360 -0.218\n 3  1.56    0.401 -1.03 \n 4  0.0705 NA     -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  NA    \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97  NA    \n 9 NA       0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nPretty nice, right?\nBefore moving on, note that we did not have to create the add_na_at() function ahead of time the way we did. If we didn’t think we would need to use add_na_at() in any other part of our program, we might have decided to pass the code inside of add_na_at() to the .f argument as an anonymous function instead.\nAs a reminder, here is what our named function looks like:\n\nadd_na_at &lt;- function(vect, pos) {\n  vect[[pos]] &lt;- NA\n  vect\n}\n\nAnd here is what our purrr code would look like if we used an anonymous function instead:\n\nmap2_dfc(\n  .x = df_xyz,\n  .y = c(2, 4, 6),\n  .f = function(vect, pos) {\n    vect[[pos]] &lt;- NA\n    vect\n  }\n)\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560   1.22  -1.07 \n 2 NA       0.360 -0.218\n 3  1.56    0.401 -1.03 \n 4  0.0705 NA     -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  NA    \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97   0.153\n 9 -0.687   0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nOr, if we used a purrr-style lambda anonymous function instead:\n\nmap2_dfc(\n  .x = df_xyz,\n  .y = c(2, 4, 6),\n  .f = ~ {\n    .x[[.y]] &lt;- NA\n    .x\n  }\n)\n\n# A tibble: 10 × 3\n         x      y      z\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560   1.22  -1.07 \n 2 NA       0.360 -0.218\n 3  1.56    0.401 -1.03 \n 4  0.0705 NA     -0.729\n 5  0.129  -0.556 -0.625\n 6  1.72    1.79  NA    \n 7  0.461   0.498  0.838\n 8 -1.27   -1.97   0.153\n 9 -0.687   0.701 -1.14 \n10 -0.446  -0.473  1.25 \n\n\nWhichever style you choose to use is largely just a matter of preference in this case (as it is in many cases).\n\n\n37.3.2 Example 2. Detecting matching values by position\nIn the chapter on writing functions, we created an is_match() function. In that scenario, we wanted to see if first name, last name, and street name matched at each ID between our data frames. More specifically, we wanted to combine the two data frames into a single data frame and create three new dummy variables that indicated whether first name, last name, and address matched respectively.\nHere are the data frames we simulated and combined:\n\npeople_1 &lt;- tribble(\n  ~id_1, ~name_first_1, ~name_last_1, ~street_1,\n  1,     \"Easton\",      NA,           \"Alameda\",\n  2,     \"Elias\",       \"Salazar\",    \"Crissy Field\",\n  3,     \"Colton\",      \"Fox\",        \"San Bruno\",\n  4,     \"Cameron\",     \"Warren\",     \"Nottingham\",\n  5,     \"Carson\",      \"Mills\",      \"Jersey\",\n  6,     \"Addison\",     \"Meyer\",      \"Tingley\",\n  7,     \"Aubrey\",      \"Rice\",       \"Buena Vista\",\n  8,     \"Ellie\",       \"Schmidt\",    \"Division\",\n  9,     \"Robert\",      \"Garza\",      \"Red Rock\",\n  10,    \"Stella\",      \"Daniels\",    \"Holland\"\n)\n\npeople_2 &lt;- tribble(\n  ~id_2, ~name_first_2, ~name_last_2, ~street_2,\n  1,     \"Easton\",      \"Stone\",      \"Alameda\",\n  2,     \"Elas\",        \"Salazar\",    \"Field\",\n  3,     NA,            \"Fox\",        NA,\n  4,     \"Cameron\",     \"Waren\",      \"Notingham\",\n  5,     \"Carsen\",      \"Mills\",      \"Jersey\",\n  6,     \"Adison\",      NA,           NA,\n  7,     \"Aubrey\",      \"Rice\",       \"Buena Vista\",\n  8,     NA,            \"Schmidt\",    \"Division\",\n  9,     \"Bob\",         \"Garza\",      \"Red Rock\",\n  10,    \"Stella\",      NA,           \"Holland\"\n)\n\npeople &lt;- people_1 %&gt;% \n  bind_cols(people_2) %&gt;% \n  print()\n\n# A tibble: 10 × 8\n    id_1 name_first_1 name_last_1 street_1      id_2 name_first_2 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n 1     1 Easton       &lt;NA&gt;        Alameda          1 Easton       Stone      \n 2     2 Elias        Salazar     Crissy Field     2 Elas         Salazar    \n 3     3 Colton       Fox         San Bruno        3 &lt;NA&gt;         Fox        \n 4     4 Cameron      Warren      Nottingham       4 Cameron      Waren      \n 5     5 Carson       Mills       Jersey           5 Carsen       Mills      \n 6     6 Addison      Meyer       Tingley          6 Adison       &lt;NA&gt;       \n 7     7 Aubrey       Rice        Buena Vista      7 Aubrey       Rice       \n 8     8 Ellie        Schmidt     Division         8 &lt;NA&gt;         Schmidt    \n 9     9 Robert       Garza       Red Rock         9 Bob          Garza      \n10    10 Stella       Daniels     Holland         10 Stella       &lt;NA&gt;       \n# ℹ 1 more variable: street_2 &lt;chr&gt;\n\n\nHere is the function we wrote to help us create the dummy variables:\n\nis_match &lt;- function(value_1, value_2) {\n  result &lt;- value_1 == value_2\n  result &lt;- if_else(is.na(result), FALSE, result)\n  result\n}\n\nAnd here is how we applied the function we wrote to get our results:\n\npeople %&gt;% \n  mutate(\n    name_first_match = is_match(name_first_1, name_first_2),\n    name_last_match  = is_match(name_last_1, name_last_2),\n    street_match     = is_match(street_1, street_2)\n  ) %&gt;% \n  # Order like columns next to each other for easier comparison\n  select(id_1, starts_with(\"name_f\"), starts_with(\"name_l\"), starts_with(\"s\"))\n\n# A tibble: 10 × 10\n    id_1 name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2     2 Elias        Elas         FALSE            Salazar     Salazar    \n 3     3 Colton       &lt;NA&gt;         FALSE            Fox         Fox        \n 4     4 Cameron      Cameron      TRUE             Warren      Waren      \n 5     5 Carson       Carsen       FALSE            Mills       Mills      \n 6     6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7     7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8     8 Ellie        &lt;NA&gt;         FALSE            Schmidt     Schmidt    \n 9     9 Robert       Bob          FALSE            Garza       Garza      \n10    10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\n🚩However, in the code chunk above, we still have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition from our code. Because we are using dplyr, and all of our data resides inside of a single data frame, your first instinct might be to use across() inside of mutate() to perform column-wise operations. Unfortunately, that method won’t work in this scenario.\nThe across() function will apply the function we pass to the .fns argument to each column passed to the .cols argument, one at a time. But, we need to pass two columns at a time to the is_match() function. For example, name_first_1 and name_first_2. That makes this task a little trickier than most. But, here’s how we accomplished it using a for loop:\n\ncols &lt;- c(\"name_first\", \"name_last\", \"street\")\n\nfor(i in seq_along(cols)) {\n  col_1   &lt;- paste0(cols[[i]], \"_1\")\n  col_2   &lt;- paste0(cols[[i]], \"_2\")\n  new_col &lt;- paste0(cols[[i]], \"_match\")\n  people[[new_col]] &lt;- is_match(people[[col_1]], people[[col_2]])\n}\n\npeople %&gt;% \n  select(id_1, starts_with(\"name_f\"), starts_with(\"name_l\"), starts_with(\"s\"))\n\n# A tibble: 10 × 10\n    id_1 name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1     1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2     2 Elias        Elas         FALSE            Salazar     Salazar    \n 3     3 Colton       &lt;NA&gt;         FALSE            Fox         Fox        \n 4     4 Cameron      Cameron      TRUE             Warren      Waren      \n 5     5 Carson       Carsen       FALSE            Mills       Mills      \n 6     6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7     7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8     8 Ellie        &lt;NA&gt;         FALSE            Schmidt     Schmidt    \n 9     9 Robert       Bob          FALSE            Garza       Garza      \n10    10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\nNow, let’s go over one way to get the same result using purrr. The first method very closely resembles our for loop. In fact, we will basically just copy and paste our for loop body into an anonymous function being passed to the .f argument:\n\nmap_dfc(\n  .x = c(\"name_first\", \"name_last\", \"street\"),\n  .f = function(col, data = people) {\n    col_1 &lt;- paste0(col, \"_1\")\n    col_2 &lt;- paste0(col, \"_2\")\n    new_nm &lt;- paste0(col, \"_match\")\n    data[[new_nm]] &lt;- data[[col_1]] == data[[col_2]]\n    data[[new_nm]] &lt;- if_else(is.na(data[[new_nm]]), FALSE, data[[new_nm]])\n    data[c(col_1, col_2, new_nm)]\n  }\n)\n\n# A tibble: 10 × 9\n   name_first_1 name_first_2 name_first_match name_last_1 name_last_2\n   &lt;chr&gt;        &lt;chr&gt;        &lt;lgl&gt;            &lt;chr&gt;       &lt;chr&gt;      \n 1 Easton       Easton       TRUE             &lt;NA&gt;        Stone      \n 2 Elias        Elas         FALSE            Salazar     Salazar    \n 3 Colton       &lt;NA&gt;         FALSE            Fox         Fox        \n 4 Cameron      Cameron      TRUE             Warren      Waren      \n 5 Carson       Carsen       FALSE            Mills       Mills      \n 6 Addison      Adison       FALSE            Meyer       &lt;NA&gt;       \n 7 Aubrey       Aubrey       TRUE             Rice        Rice       \n 8 Ellie        &lt;NA&gt;         FALSE            Schmidt     Schmidt    \n 9 Robert       Bob          FALSE            Garza       Garza      \n10 Stella       Stella       TRUE             Daniels     &lt;NA&gt;       \n# ℹ 4 more variables: name_last_match &lt;lgl&gt;, street_1 &lt;chr&gt;, street_2 &lt;chr&gt;,\n#   street_match &lt;lgl&gt;\n\n\nIn the code above, we used roughly the same amount of code to complete the task with a loop that we used to complete it without a loop. However, this code still has some advantages. We only typed “name_first”, “name_last”, and “street” once at the beginning of the code chunk. Therefore, we didn’t have to worry about forgetting to change a column name after copying and pasting code. Additionally, if we later decide that we also want to compare other columns (e.g., middle name, birth date, city, state, zip code), we only have to update the code in one place – where we create the cols vector.",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Using the purrr Package</span>"
    ]
  },
  {
    "objectID": "chapters/using_purrr/using_purrr.html#using-purrr-for-analysis",
    "href": "chapters/using_purrr/using_purrr.html#using-purrr-for-analysis",
    "title": "37  Using the purrr Package",
    "section": "37.4 Using purrr for analysis",
    "text": "37.4 Using purrr for analysis\nLet’s return to the examples from the column-wise operations chapter and the chapter on writing for loops for our discussion of using the purrr package to remove unnecessary repetition from our analyses. We will once again use the simulated for the examples below.\n\nstudy &lt;- tibble(\n  age       = c(32, 30, 32, 29, 24, 38, 25, 24, 48, 29, 22, 29, 24, 28, 24, 25, \n                25, 22, 25, 24, 25, 24, 23, 24, 31, 24, 29, 24, 22, 23, 26, 23, \n                24, 25, 24, 33, 27, 25, 26, 26, 26, 26, 26, 27, 24, 43, 25, 24, \n                27, 28, 29, 24, 26, 28, 25, 24, 26, 24, 26, 31, 24, 26, 31, 34, \n                26, 25, 27, NA),\n  age_group = c(2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n                1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, \n                1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, \n                2, 1, 1, 1, NA),\n  gender    = c(2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, \n                1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, \n                1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, \n                1, 1, 2, 1, NA),\n  ht_in     = c(70, 63, 62, 67, 67, 58, 64, 69, 65, 68, 63, 68, 69, 66, 67, 65, \n                64, 75, 67, 63, 60, 67, 64, 73, 62, 69, 67, 62, 68, 66, 66, 62, \n                64, 68, NA, 68, 70, 68, 68, 66, 71, 61, 62, 64, 64, 63, 67, 66, \n                69, 76, NA, 63, 64, 65, 65, 71, 66, 65, 65, 71, 64, 71, 60, 62, \n                61, 69, 66, NA),\n  wt_lbs    = c(216, 106, 145, 195, 143, 125, 138, 140, 158, 167, 145, 297, 146, \n                125, 111, 125, 130, 182, 170, 121, 98, 150, 132, 250, 137, 124, \n                186, 148, 134, 155, 122, 142, 110, 132, 188, 176, 188, 166, 136, \n                147, 178, 125, 102, 140, 139, 60, 147, 147, 141, 232, 186, 212, \n                110, 110, 115, 154, 140, 150, 130, NA, 171, 156, 92, 122, 102, \n                163, 141, NA),\n  bmi       = c(30.99, 18.78, 26.52, 30.54, 22.39, 26.12, 23.69, 20.67, 26.29, \n                25.39, 25.68, 45.15, 21.56, 20.17, 17.38, 20.8, 22.31, 22.75, \n                26.62, 21.43, 19.14, 23.49, 22.66, 32.98, 25.05, 18.31, 29.13, \n                27.07, 20.37, 25.01, 19.69, 25.97, 18.88, 20.07, NA, 26.76, \n                26.97, 25.24, 20.68, 23.72, 24.82, 23.62, 18.65, 24.03, 23.86, \n                10.63, 23.02, 23.72, 20.82, 28.24, NA, 37.55, 18.88, 18.3, \n                19.13, 21.48, 22.59, 24.96, 21.63, NA, 29.35, 21.76, 17.97, \n                22.31, 19.27, 24.07, 22.76, NA),\n  bmi_3cat  = c(3, 1, 2, 3, 1, 2, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, \n                1, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, NA, 2, 2, 2, 1, 1, 1, 1, \n                1, 1, 1, 1, 1, 1, 1, 2, NA, 3, 1, 1, 1, 1, 1, 1, 1, NA, 2, 1, \n                1, 1, 1, 1, 1, NA)\n) %&gt;% \n  mutate(\n    age_group = factor(age_group, labels = c(\"Younger than 30\", \"30 and Older\")),\n    gender    = factor(gender, labels = c(\"Female\", \"Male\")),\n    bmi_3cat  = factor(bmi_3cat, labels = c(\"Normal\", \"Overweight\", \"Obese\"))\n  ) %&gt;% \n  print()\n\n# A tibble: 68 × 7\n     age age_group       gender ht_in wt_lbs   bmi bmi_3cat  \n   &lt;dbl&gt; &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     \n 1    32 30 and Older    Male      70    216  31.0 Obese     \n 2    30 30 and Older    Female    63    106  18.8 Normal    \n 3    32 30 and Older    Female    62    145  26.5 Overweight\n 4    29 Younger than 30 Male      67    195  30.5 Obese     \n 5    24 Younger than 30 Female    67    143  22.4 Normal    \n 6    38 30 and Older    Female    58    125  26.1 Overweight\n 7    25 Younger than 30 Female    64    138  23.7 Normal    \n 8    24 Younger than 30 Male      69    140  20.7 Normal    \n 9    48 30 and Older    Male      65    158  26.3 Overweight\n10    29 Younger than 30 Male      68    167  25.4 Overweight\n# ℹ 58 more rows\n\n\n\n37.4.1 Example 1: Continuous statistics\nIn this first example, we will use purrr to calculate a set of statistics for multiple continuous variables in our data frame. We will start by creating the same function we created at the beginning of the chapter on writing functions.\n\ncontinuous_stats &lt;- function(var) {\n  study %&gt;% \n    summarise(\n      n_miss = sum(is.na({{ var }})),\n      mean   = mean({{ var }}, na.rm = TRUE),\n      median = median({{ var }}, na.rm = TRUE),\n      min    = min({{ var }}, na.rm = TRUE),\n      max    = max({{ var }}, na.rm = TRUE)\n    )\n}\n\nNow, let’s once again use the function we just created above to calculate the descriptive measures we are interested in.\n\ncontinuous_stats(age)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1  26.9     26    22    48\n\n\n\ncontinuous_stats(ht_in)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      3  66.0     66    58    76\n\n\n\ncontinuous_stats(wt_lbs)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      2  148.   142.    60   297\n\n\n\ncontinuous_stats(bmi)\n\n# A tibble: 1 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      4  23.6   22.9  10.6  45.2\n\n\n🚩Once again, you notice that we have essentially the same code copied more than twice. That’s a red flag that we should be thinking about removing unnecessary repetition. We’ve already seen how to accomplish this goal using the across() function. Now, let’s learn how to accomplish this goal using the purrr package.\n\nmap_dfr(\n  .x = quos(age, ht_in, wt_lbs, bmi),\n  .f = continuous_stats\n)\n\n# A tibble: 4 × 5\n  n_miss  mean median   min   max\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1  26.9   26    22    48  \n2      3  66.0   66    58    76  \n3      2 148.   142.   60   297  \n4      4  23.6   22.9  10.6  45.2\n\n\n👆Here’s what we did above:\n\nWe used the map_dfr() function from the purrr package to iteratively pass the columns age, ht_in, wt_lbs, and bmi to our continuous_stats function and row-bind the results into a single results data frame.\nWe haven’t seen the quos() function before. It’s another one of those tidy evaluation functions. You can type ?rlang::quos in your console to read more about it. When we can wrap a single column name with the quo() function, or a list of column names with the quos() function, we are telling R to look for them in the data frame being passed to a dplyr verb rather than looking for them as objects in the global environment.\n\nAt this point, you may be wondering which row in the results data frame above corresponds to which variable? Great question! When we were using continuous_stats() to analyze one variable at a time, it didn’t really matter that the variable name wasn’t part of the output. However, now that we are apply continuous_stats() to multiple columns, it would really be nice to have the column name in the results. Luckily, we can easily make that happen with one small tweak to our continuous_stats() function.\n\ncontinuous_stats &lt;- function(var) {\n  study %&gt;% \n    summarise(\n      variable = quo_name(var), # Add variable name to the output\n      n_miss   = sum(is.na({{ var }})),\n      mean     = mean({{ var }}, na.rm = TRUE),\n      median   = median({{ var }}, na.rm = TRUE),\n      min      = min({{ var }}, na.rm = TRUE),\n      max      = max({{ var }}, na.rm = TRUE)\n    ) \n}\n\n👆Here’s what we did above:\n\nWe used the quo_name() function to grab the name of the column being passed to the summarise() function and turn it into a character string. Then, we assigned that character string to column in the results data frame called variable. So, when the age column is passed to summarise() inside of the function body, quo_name(var) returns the value \"age\" and then that value is assigned to the variable column in the expression variable = quo_name(var).\n\nLet’s try out our new and improved continuous_stats() function:\n\nmap_dfr(\n  .x = quos(age, ht_in, wt_lbs, bmi),\n  .f = continuous_stats\n)\n\n# A tibble: 4 × 6\n  variable n_miss  mean median   min   max\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age           1  26.9   26    22    48  \n2 ht_in         3  66.0   66    58    76  \n3 wt_lbs        2 148.   142.   60   297  \n4 bmi           4  23.6   22.9  10.6  45.2\n\n\nThat works great, but we’d probably like to be able to use continuous_stats() with other data frames too. Currently, we can’t do that because we have the study data frame hard-coded into our function. Luckily, we’ve already seen how to replace a hard-coded data frame by adding a data argument to our function like this:\n\ncontinuous_stats &lt;- function(data, var) {\n  data %&gt;%  # Don't forget to replace \"study\" with \"data\" here too!\n    summarise(\n      variable = quo_name(var),\n      n_miss   = sum(is.na({{ var}} )),\n      mean     = mean({{ var }}, na.rm = TRUE),\n      median   = median({{ var }}, na.rm = TRUE),\n      min      = min({{ var }}, na.rm = TRUE),\n      max      = max({{ var }}, na.rm = TRUE)\n    ) \n}\n\nAnd now, we can analyze all the continuous variables in the study data:\n\nmap_dfr(\n  .x = quos(age, ht_in, wt_lbs, bmi),\n  .f = continuous_stats, data = study\n)\n\n# A tibble: 4 × 6\n  variable n_miss  mean median   min   max\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age           1  26.9   26    22    48  \n2 ht_in         3  66.0   66    58    76  \n3 wt_lbs        2 148.   142.   60   297  \n4 bmi           4  23.6   22.9  10.6  45.2\n\n\nAnd all the continuous variables in the df_xyz data:\n\nmap_dfr(\n  .x = quos(x, y, z),\n  .f = continuous_stats, data = df_xyz\n)\n\n# A tibble: 3 × 6\n  variable n_miss   mean  median   min   max\n  &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 x             1  0.108  0.0705 -1.27  1.72\n2 y             1  0.220  0.401  -1.97  1.79\n3 z             1 -0.284 -0.625  -1.14  1.25\n\n\n\n\n37.4.2 Example 2: Categorical statistics\nFor our second example of using the purrr package for analysis, we’ll once again write some code to iteratively analyze all the categorical variables in our study data frame. In the last chapter, we learned how to use a for loop to do this analysis. As a refresher, here is the final solution we arrived at:\n\n# Structure 1. An object to contain the results.\n  # Create the data frame structure that will contain our results\ncat_table &lt;- tibble(\n  variable = vector(\"character\"), \n  category = vector(\"character\"), \n  n        = vector(\"numeric\")\n) \n\n# Structure 2. The actual for loop.\n  # For each column, get the column name, category names, and count.\n  # Then, add them to the bottom of the results data frame we created above.\nfor(i in c(\"age_group\", \"gender\", \"bmi_3cat\")) {\n  cat_stats &lt;- study %&gt;% \n    count(.data[[i]]) %&gt;% # Use .data to refer to the current data frame.\n    mutate(variable = names(.)[1]) %&gt;% # Use . to refer to the current data frame.\n    rename(category = 1)\n  \n  # Here is where we update cat_table with the results for each column\n  cat_table &lt;- bind_rows(cat_table, cat_stats)\n}\n\n\ncat_table\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nTo use purrr instead, we can pretty much copy and paste the code from the for loop body above as an anonymous function to the .f argument to map_dfr() like this:\n\nmap_dfr(\n  .x = c(\"age_group\", \"gender\", \"bmi_3cat\"),\n  .f = function(x) {\n    study %&gt;% \n      count(.data[[x]]) %&gt;% \n      mutate(variable = names(.)[1]) %&gt;% \n      rename(category = 1) %&gt;% \n      select(variable, category, n)\n  }\n)\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;fct&gt;           &lt;int&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nAs you can see, the code that is doing the analysis is exactly the same in our for loop solution and our purrr solution. However, in this case, the purrr solution requires a lot less code around the analysis code. And, for some, the purrr code is easier to read.\nIf we didn’t want to type our column names in quotes, we could use tidy evaluation again. All we have to do is pass the column names to the quos() function in the .x argument and change the .data[[x]] being passed to the count() function to { x } like this:\n\nmap_dfr(\n  .x = quos(age_group, gender, bmi_3cat), # Change c() to quos()\n  .f = function(x) {\n    study %&gt;% \n      count({{ x }}) %&gt;% # Change .data[[x]] to {{ x }}\n      mutate(variable = names(.)[1]) %&gt;% \n      rename(category = 1) %&gt;% \n      select(variable, category, n)\n  }\n)\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;fct&gt;           &lt;int&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nAnd as before, we’d probably like to be able to use this code with other data frames too. So, we will once again replace a hard-coded data frame by adding a data argument to our function:\n\nmap_dfr(\n  .x = quos(age_group, gender, bmi_3cat),\n  .f = function(x, data = study) {\n    data %&gt;% # Don't forget to replace \"study\" with \"data\" here too!\n      count({{ x }}) %&gt;% \n      mutate(variable = names(.)[1]) %&gt;% \n      rename(category = 1) %&gt;% \n      select(variable, category, n)\n  }\n)\n\n# A tibble: 10 × 3\n   variable  category            n\n   &lt;chr&gt;     &lt;fct&gt;           &lt;int&gt;\n 1 age_group Younger than 30    56\n 2 age_group 30 and Older       11\n 3 age_group &lt;NA&gt;                1\n 4 gender    Female             43\n 5 gender    Male               24\n 6 gender    &lt;NA&gt;                1\n 7 bmi_3cat  Normal             43\n 8 bmi_3cat  Overweight         16\n 9 bmi_3cat  Obese               5\n10 bmi_3cat  &lt;NA&gt;                4\n\n\nAnd that concludes the chapter! You might feel a little bit like your head is swimming at this point. It was a lot to take in! As was stated at the end of the for loop chapter, it is not recommended to memorize everything we covered in this chapter. Instead, we recommend that you read it until you sort of get the general idea of the purrr package and when it might be useful. Then, refer back to this chapter, or other online references that discuss the purrr package (there are many good ones out there), if you find yourself in a situation where you believe that the purrr package might be the right tool to help you complete a given programming task.\nIf you feel as though you want to take a deeper dive into the purrr package right away, then we suggest checking out the iteration chapter of R for Data Science. For an even deeper dive, the functionals chapter of Advanced R is recommended.\nThis concludes the repeated operations part of the book. If you aren’t feeling totally comfortable with the material we covered in this part of the book right now, that’s ok. You’re not expected to yet. It takes time and practice for most people to be able to wrap their head around repeated operations. You are on track at this point as long as you understand why unnecessary repetition in your code is generally something you want to avoid. Then, slowly start using any of the methods you feel most comfortable with to remove the unnecessary repetition from your code. Start by doing so in very simple cases and gradually work your way up to more complicated cases. With some practice, you may eventually think this stuff is even fun!",
    "crumbs": [
      "Repeated Operations",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Using the purrr Package</span>"
    ]
  },
  {
    "objectID": "chapters/intro_git_github/intro_git_github.html",
    "href": "chapters/intro_git_github/intro_git_github.html",
    "title": "38  Introduction to git and GitHub",
    "section": "",
    "text": "38.1 Versioning\nIf you read this book’s introductory material, specifically the section on Contributing to R4Epi, then you have already been briefly exposed to GitHub. If not, taking a quick look at that section may be useful. GitHub is a website specifically designed to facilitate collaboratively creating programming code. In many ways, GitHub is a cloud-based file storage service like Dropbox, Google Drive, and OneDrive, but with special tools built-in for collaborative coding. Git is the name of the versioning software that powers many of GitHub’s special tools. We will talk about what versioning means shortly.\nThe goal of this, and the next few, chapters isn’t to teach you everything you need to know about git and GitHub. Not even close! That would fill up its own book. The goal here is just to expose you to git and GitHub, show you a brief example of how they may be useful to you, and provide you with some resources you can use to learn more if you’re interested.\nBut, why should you be interested in the first place? Well, there are at least four overarching reasons why you should consider learning to use git and GitHub as part of your workflow when your projects include data and/or coding:\nWe’ll elaborate on what each of these means to us below. Then, we will introduce you to git and GitHub, and explain why they are some of the best tools currently available to help you with versioning and collaborating. We’ll go ahead and warn you now — git and GitHub can be hard to wrap your mind around at first. In fact, using git and GitHub still frequently causes us confusion and frustration at times. However, we still believe that the payoff is ultimately worth the upfront investment in time and frustration. Additionally, we will do our best to make this introduction as gentle, comprehensible, and practically applicable as possible.\nHave you ever worked on a paper or report and had a folder on your computer that looked something like this?\nSaving a bunch of different versions of a file like this is a real mess. It becomes even worse when you are trying to work with multiple people. What is contained in each document again? What order were the documents created in? What are the differences between the documents? Versioning helps us get around all of these problems.\nInstead of jumping straight into learning versioning with git and GitHub, we will start our discussion about versioning using a simple example in Google Docs. Not because Google Docs are especially relevant to anything else in this course, but because there are a lot of parallels between the Google Docs versioning system and the git versioning system when it is paired with Github. However, the Google Docs versioning system is a little bit more basic, easy to understand, and easy to experiment with. Later, we will refer back to some of these Google Docs examples when we are trying to explain how to use git and GitHub. If you’d like to do some experimenting of your own, feel free to navigate to https://docs.google.com/ now and follow along with the following demonstration.\nFirst, we will type a little bit of text in our Google Doc. It doesn’t really matter what we type — this is purely for demonstration purposes. In the example below, we type “Here is some text.”\nFigure 38.1: A gif about text in Google Docs.\nNow, let’s say that we decide to make a change to our text. Specifically, we decide to replace “some” with “just a little.”\nFigure 38.2: A gif about text change in Google Docs.\nNow, let’s say that we changed our mind again and we want to go back to using the original text. In this case, it would be really easy to go back to using the original text even without versioning. We could just use “undo” or even retype the previous text. But, let’s pretend for a minute that we changed a lot of text, and that we made those changes several weeks ago. Under those circumstances, how might we view the original version of the document? We can use the Google Docs versioning system. To do so, we can click File then Version history then See version history. This will bring up a new view that shows us all the changes we’ve made to this document, and when we made them.\nFigure 38.3: A gif about version history in Google Docs.\nThis is great! We don’t have to save a bunch of different files like we saw in the “messy” folder at the beginning of this section. Instead, there is only one document, and we can see all the versions of that document, who created the various versions of that document, when all the various versions of that document were created, and exactly what changed from one version to the next. In other words, we have a complete record of the evolution of this document in the version history — how we got from the blank document we started with to the current version of the document we are working with today.\nFurther, if we want to turn back the clock to a previous version of the document, we need only select that version and click the Restore this version button like this.\nFigure 38.4: A gif about restoring versions in Google Docs.\nBut, you can probably imagine how difficult it can be to find a previous version of a document by searching through a list of dates. In the example above, there were only three dates to look through, but in a real work document, there may be hundreds of versions saved. The dates, by themselves, aren’t very informative. Luckily, when we hit key milestones in the development of our document, Google Docs allows us to name them. That way, it will be easy to find that version in the future if we ever need to refer to it (assuming we give it an informative name).\nFor example, let’s say that we just added a table to our document that includes the mean values of the variables X and Y for two groups of people - Group 1 and Group 2. Completing this table is a key milestone in the evolution of our document and this is a great time to name the current version of the document just in case we ever need to refer back to it. To do so, we can click File then Version history then Name current version.\nFigure 38.5: A gif about naming current Google Doc versions.\nNotice that in the example above I used the word commit instead of the word save. In this case, they essentially mean the same thing, but soon you will see that git also uses the word commit to refer to taking a snapshot of the state of our project — similar to the way we just took a snapshot of the state of our document.\nNow let’s say that we decide to use medians in our table instead of means. After making that change, our document now looks like this.\nA gif about switching back to an old version in Google Docs.\nCan you guess what we are about to do next? That’s right! We changed our minds again and decided to switch back to using the mean values in the table. No problem! We can easily search for the version of the document that we committed, which includes the table of mean values. We can then restore that version as we did above.\nA gif about restoring versions in Google Docs.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/intro_git_github/intro_git_github.html#preservation",
    "href": "chapters/intro_git_github/intro_git_github.html#preservation",
    "title": "38  Introduction to git and GitHub",
    "section": "38.2 Preservation",
    "text": "38.2 Preservation\nIn addition to versioning, the ability to preserve all of your code and related project files in the cloud is another great reason to consider using GitHub. In other words, you don’t have to worry about losing your code if your computer is lost, damaged, or replaced. All of your project files can easily be retrieved and restored from GitHub. Although the same is true for other cloud-based file storage services like Dropbox, Google Drive, and OneDrive, remember that GitHub has special built-in tools that those services do not provide.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/intro_git_github/intro_git_github.html#reproducibility",
    "href": "chapters/intro_git_github/intro_git_github.html#reproducibility",
    "title": "38  Introduction to git and GitHub",
    "section": "38.3 Reproducibility",
    "text": "38.3 Reproducibility\nReproducibility, or more precisely, reproducible research, is a term that may be unfamiliar to many of you. Peng and Hichs (2021) give a nice introduction to reproducible research:1\n\nScientific progress has long depended on the ability of scientists to communicate to others the details of their investigations… In the past, it might have sufficed to describe the data collection and analysis using a few key words and high-level language. However, with today’s computing-intensive research, the lack of details about the data analysis in particular can make it impossible to recreate any of the results presented in a paper. Compounding these difficulties is the impracticality of describing these myriad details in traditional journal publications using natural language. To address this communication problem, a concept has emerged known as reproducible research, which aims to provide for others far more precise descriptions of an investigator’s work. As such, reproducible research is an extension of the usual communications practices of scientists, adapted to the modern era.\n\nThey go on to define reproducible research in the following way:1 2\n\nA published data analysis is reproducible if the analytic data sets and the computer code used to create the data analysis are made available to others for independent study and analysis.\n\nWe will not delve deeper into the general importance and challenges of reproducible research in this book; however, we encourage readers who are interested in learning more about reproducible research to take a look at both of the articles cited above. Additionally, we believe it’s important to highlight that GitHub is a great tool for making our research more reproducible. Specifically, it provides a platform where others can easily download the data (when we are allowed to make it available), computer code, and documentation needed to recreate our research results. This is a great asset for scientific progress, but only if researchers like us use it effectively.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/intro_git_github/intro_git_github.html#collaboration",
    "href": "chapters/intro_git_github/intro_git_github.html#collaboration",
    "title": "38  Introduction to git and GitHub",
    "section": "38.4 Collaboration",
    "text": "38.4 Collaboration\nIn the sections above, we discussed the ways in which git and GitHub are tools we can use for versioning, preserving our code in the cloud, and making our research more reproducible. All of these are important benefits of using git and GitHub even if we don’t routinely collaborate with others to complete our projects. However, the power of GitHub is even greater when we think about using it as a tool for collaboration — including collaboration with our future selves.\nFor example, one research project that we (the authors) both work on is the Detection of Elder abuse Through Emergency Care Technicians (DETECT) project. Let’s say that we would like to start collaborating with you on DETECT. Perhaps we need your help preprocessing some of the DETECT data and conducting an analysis. So, how do we get started?\nBecause we created a repository on GitHub for the DETECT project, all of the files and documentation you need to get started are easily accessible to you. In fact, you don’t even have to reach out to us first for access. They are freely available to anyone who is interested. Please go ahead and use the following URL to view the DETECT repository now: https://github.com/brad-cannell/detect_pilot_test_5w. GitHub repositories may look a little confusing at first, but you will get used to them with practice.\n\n\n\n\n\n\nNote\n\n\n\nRepository is a git term that can seem a little confusing or intimidating at first. However, it’s really no big deal. You can think of a git repository as a folder that holds all of the files related to your project. On GitHub, each repository has its own separate website where people from anywhere in the world can access the files and documents related to your project. They can also communicate with you through your GitHub repository, post issues to your GitHub repository if they encounter a problem, and contribute code to your project.\n\n\nWe could have emailed the files back and forth, but what if we accidentally forget to send you one? What if one of the files is too large to email? What if two people are working on the same file at the same time and send out their revisions via email? Which version should we use? In the chapters that follow, we will show you how using GitHub to share project files gets around these, and other, collaboration issues.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/intro_git_github/intro_git_github.html#summary",
    "href": "chapters/intro_git_github/intro_git_github.html#summary",
    "title": "38  Introduction to git and GitHub",
    "section": "38.5 Summary",
    "text": "38.5 Summary\nIn summary, git and GitHub are awesome tools to use when our projects involve research and/or data analysis. They allow us to store all of our files in the cloud with the added benefit of versioning and many other collaboration tools. The primary disadvantage of using GitHub instead of just emailing code files or using general-purpose cloud storage services is its learning curve. But, in the following chapters, we hope to give you enough knowledge to make GitHub immediately useful to you. Over time, you can continue to hone your GitHub skills and really take advantage of everything it has to offer. We think if you make this initial investment, it is unlikely that you will ever look back.\n\n\n\n\n1. Peng RD, Hicks SC. Reproducible research: A retrospective. Annu Rev Public Health. 2021;42:79-93.\n\n\n2. Peng RD. Reproducible research in computational science. Science. 2011;334(6060):1226-1227.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Introduction to git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html",
    "href": "chapters/using_git_github/using_git_github.html",
    "title": "39  Using git and GitHub",
    "section": "",
    "text": "39.1 Install git\nIn the previous chapter, we discussed why we should consider learning to use git and GitHub as part of our workflow when our projects include data and/or coding. In this chapter, we will begin to talk about how to use git and GitHub. We will also introduce a third tool, GitKraken, that makes it easier for us to use git and GitHub.\nBefore we can use git, we will need to install it on our computer. The following chapter of Pro Git provides instructions for installing git on Linux, Windows, and MacOS operating systems: Get Started Installing Git.\nIf you are using a Mac, it’s likely that you already have git — most Macs ship with git installed. To check, open your Terminal app. The Terminal app is located in the Utilities folder, which is located in the Applications folder. In the terminal app, type “git version”. If you see a version number, then it is already installed. If not, then please follow the installation instructions given in the link to Pro Git above.\nFigure 39.1: Checking git version in the MacOS terminal.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html#sign-up-for-a-github-account",
    "href": "chapters/using_git_github/using_git_github.html#sign-up-for-a-github-account",
    "title": "39  Using git and GitHub",
    "section": "39.2 Sign up for a GitHub account",
    "text": "39.2 Sign up for a GitHub account\nWe have already alluded to the fact that git and GitHub are not the same thing. You can use git locally on your computer without ever using GitHub. Conversely, you can browse GitHub, and even do some limited contributing to code, without ever installing git on your computer (e.g., see Contributing to R4Epi. However, git and GitHub work best when used together. You don’t need to download anything to start using GitHub, but you will need to sign up for a free GitHub account. To do so, just navigate to https://github.com/",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html#install-gitkraken",
    "href": "chapters/using_git_github/using_git_github.html#install-gitkraken",
    "title": "39  Using git and GitHub",
    "section": "39.3 Install GitKraken",
    "text": "39.3 Install GitKraken\nGit is software for our computer. However, unlike most of the software we are used to using, git does not have a graphical user interface (GUI - pronounced “gooey”). In other words, there is no git application that we can open and start clicking around in. Instead, by default, we interact with git by typing commands into the computer’s terminal – also called “command line” in GitHub’s documentation – like we saw in Figure 39.1. The commands we type to use git kind of look like their own programming language. In our experience, interacting with git in the terminal is awkward, inefficient, and unnecessary for most new git users. And learning to use git in this way is a barrier to getting started in the first place. 😩\nThankfully, other third-party vendors have made excellent GUI’s for git that we can download and use for free. Our current favorite is called GitKraken. To use GitKraken, you will first need to navigate to the GitKraken website (https://www.gitkraken.com/). If it helps, you can think of git and GitKraken as having a relationship that is very similar to the relationship between R and RStudio. R is the language. RStudio is the application that makes it easier for us to use the R language to work with data. Similarly, git is the language and GitKraken is the application that makes it easier for us to use git to track versions of our project files.\nBefore you use the GitKraken client, you will need to sign up for an account. It may say that you need to sign up for a free trial. Go ahead and do it. The free trial is just for the “Pro” version. At the end of the free trial, you will automatically be downgraded to the “Free” version, which is… free. And, the free version will do everything you need to do to follow along with this book.\n\n\n\n\n\n\n\n\n\nNext, you will need to click on the “Try Free” button. Then, download and install the GitKraken Client to your computer.\n\n\n\n\n\n\n\n\n\nAs you are installing GitKraken, it should ask you if you want to sign up with your GitHub account. Yes, you do! It will make your life much easier down the road. If you didn’t sign up for a GitHub account in the previous step, please go back and do so.\n\n\n\n\n\n\n\n\n\nThen click the green Continue authorization button.\n\n\n\n\n\n\n\n\n\nThen, you will be asked to sign into your GitHub account – possibly using your two-factor authentication. When you see the success screen, you can close your browser and return to GitKraken.\n\n\n\n\n\n\n\n\n\nThe next thing you will do is create a profile. After you create a profile, you will be asked if you want the Repo Tab first or the Terminal Tab first. We recommend that you select the Repo Tab option.\n\n\n\n\n\n\n\n\n\nOnce you have installed Git and GitKraken, and you’ve created your GitHub account, you will have all the tools you need to follow along with all of the examples in this book. Speaking of examples, let’s go ahead and take a look at a couple now.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html#example-1-contribute-to-r4epi",
    "href": "chapters/using_git_github/using_git_github.html#example-1-contribute-to-r4epi",
    "title": "39  Using git and GitHub",
    "section": "39.4 Example 1: Contribute to R4Epi",
    "text": "39.4 Example 1: Contribute to R4Epi\nIf you haven’t already done so, please read the contributing to R4Epi portion of the book’s welcome page. This will give you a gentle introduction to using GitHub, for a very practical purpose, without even needing to use git or GitKraken.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html#example-2-create-a-repository-for-a-research-project",
    "href": "chapters/using_git_github/using_git_github.html#example-2-create-a-repository-for-a-research-project",
    "title": "39  Using git and GitHub",
    "section": "39.5 Example 2: Create a repository for a research project",
    "text": "39.5 Example 2: Create a repository for a research project\nIn this example, we will learn how to create our very own git and GitHub repositories from scratch. We can immediately begin using the lessons from this example for our research projects – even if we aren’t collaborating with others on them. Remember, there are at least four overarching reasons why you should consider learning to use git and GitHub as part of your workflow for your projects, and collaboration is only one of them. Not to mention the fact that it is often useful to think of our future selves as other collaborators, which we have mentioned and/or alluded to many times in this book.\nThere are many possible ways we could set up our project to take advantage of all that git and GitHub have to offer. We’re going to show you one possible sequence of steps in this example, but you may decide that you prefer a different sequence as you get more experience, and that’s totally fine!\nThis example is long! So, we created a brief outline that you can quickly reference in the future. Details are below.\nStep 1: Create a repository on GitHub\nStep 2: Clone the repository to your computer\nStep 3: Add an R project file to the repository\nStep 4: Update and commit gitignore\nStep 5: Keep adding and committing files\n\nStep 1: Create a repository on GitHub\nThe first thing we will do is create a repository on GitHub. Repositories are the fundamental organizational units of your GitHub account. Other cloud storage services like Dropbox are organized into file folders at every level. Meaning, you have your main Dropbox folder, which has other folders nested inside of it – many of which may have their own nested folders. Your GitHub account also stores all your files in file folders; however, the level one folders — those that aren’t nested inside of another folder — are called repositories (represented by the book icon in the image below and on the GitHub website). Typically, each repository is an entire, self-contained project. Like a file folder, each repository can contain other folders, code files, media files, data sets, and any other type of file needed to reproduce your research project.\n\n\n\n\n\nGitHub repositories compared to Dropbox.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nJust because we can upload data to GitHub doesn’t mean we should upload data to GitHub. Often, the data we use in epidemiology contains protected health information (PHI) that we must go to great lengths to keep secure. In general, GitHub is NOT considered a secure place to store our data and should not be used for this purpose. Below, we will demonstrate how to make sure our data isn’t uploaded to GitHub with the rest of the files in our repository.\n\n\nTo create a new repository in GitHub, we will simply click the green Create repository button. This button will look slightly different depending on where we are at in the GitHub website. The screenshot below was taken from Arthur Epi’s (our fictitious research assistant) main landing page (i.e., https://github.com/).\n\n\n\n\n\n\n\n\n\nAfter clicking the green Create repository button, the next page Arthur will see is the setup page for his repository. For the purposes of this example, he will use the following information to set it up.\n\nRepository name: As the on-screen prompt says, great repository names are short and memorable. Further, the repository name must be unique to his account (i.e., he can’t have two repositories with the same name), and it can only include letters, numbers, dashes (-), underscores (_), and periods (.). We recommend using underscores to separate words to be consistent with the object naming guidelines from coding-best-practices. For this example, he will name the repository r4epi_example_project.\nDescription: The description is optional, but we like to fill it in. Arthur’s description should also be brief. Ideally it will allow others scanning our repository to quickly determine what it’s all about. For this example, the description will say, “An example repository that accompanies the git and GitHub chapters in the R4Epi book.”\nPublic/Private: We can choose to make our repositories public or private. If we make them public, they can be viewed by anyone on the internet. If we make them private, we can control who is able to view them. At first, you may be tempted to make your repositories private. It can feel vulnerable to put your project/code out there for the entire internet to view. However, we are going to recommend that you make all of your repositories public and be thoughtful about the files/documents/information you choose to upload to them. For example, we NEVER want to upload data containing information with PHI or individual identifiers in it. So, we will often need to figure out a different way to share our data with others who legitimately need access to it, but we can often use GitHub to share all other files related to the project. Making our repository public makes it easier for others to locate our work and potentially collaborate with us.\nAdd a README file: A README file has a special place in GitHub. Under the hood, it is just a markdown file. No different than the Quarto files we learned about in the chapter on Quarto files (. However, naming it README gives it a special status. When we include a README file in our repository, GitHub will automatically add it to our repository’s homepage. We should use it to give others more information about our project, what our repository does, how to use the files in our repository, and/or how to contribute. So, we will definitely want a README file. Arthur may as well go ahead and check the box to create it along with his repository (although, we can always add it later).\nAdd .gitignore: We will discuss .gitignore later. Briefly, you can think of it as a list of files we are telling GitHub to ignore (i.e., not to track). This gets back to versioning, which we discussed in the [Versioning] section of the introduction to git and GitHub chapter. For now, Arthur will just leave it as is.\nLicense: The GitHub documentation states that, “Public repositories on GitHub are often used to share open-source software. For your repository to truly be open source, you’ll need to license it so that others are free to use, change, and distribute the software.”1 Because we aren’t currently using our repository to create and distribute open-source software (like R!!), we don’t need to worry about adding a license. That isn’t to say that you won’t ever need to worry about a license. For more on choosing a license, we can consult the GitHub documentation or potentially consult with our employer or study sponsor. For example, our universities have officials that help us determine if our repositories need a license.\n\n\n\n\n\n\n\n\n\n\nNow, that he has completed all the setup steps, Arthur can click the green Create repository button. This will create his repository and take him to its homepage on GitHub. As you can see in the screenshot below (you can also navigate to the website yourself), GitHub creates a basic little website for the repository. The top middle portion of the page (outlined in red below) displays all of the files and folders in the repository. Currently, the repository only contains one file – README.md – but Arthur will add others soon.\n\n\n\n\n\n\n\n\n\nTo the right of files and folders section of the homepage is the About section of the page. This section (outlined in red below) contains the repository’s description, tags, and other information that we will ignore for now.\n\n\n\n\n\n\n\n\n\nBelow the files and folders section of the page is where the README file is displayed. Notice that by default, GitHub added the repository’s name and description to the README file. Not a bad start, but we can add all kinds of cool stuff to README – including tables, figured, images, links, and other media. In fact, you can add almost anything to a README file that you can add to any other website. This is a great place to get creative and really make your project stand out!\n\n\n\n\n\n\n\n\n\nNow, Arthur has a working GitHub repository up and running. Let’s pause for a moment to and celebrate! 🎉\nOkay, celebration complete. Now, what does he do with this new GitHub repository? Well, he does the four things covered in Introduction to git and GitHub\n\nHe will start adding files to his repository and document their purpose and evolution with versioning.\n\nIn the process, he will preserve his files, and by extension, his project.\n\nDoing so will help to make his research more reproducible.\n\nAnd make it easier for him to collaborate with others – including his future self.\n\nLet’s start by taking a look at versioning in GitHub. As we discussed in the Versioning section of the Introduction to git and GitHub chapter, GitHub uses the word commit to refer to taking a snapshot of the state of our project, similar to how we might typically think about saving a version of a document we are working on. We saw how we could view the version history of our Google Doc by clicking File then Version history then See version history. In GitHub, we can similarly view the version history (also called the commit history) of our repository. To do so, we navigate to our repository’s homepage, and click on the word commit in the top right corner of the files section (outlined in red below).\n\n\n\n\n\n\n\n\n\nThis will take us to our repository’s version history page. Currently, this repository only has one commit – the “Initial commit”. This name is used by convention in the GitHub community to refer to the first commit in the repository. The history also tells us when the commit was made and who made it. On the right side of the commit, there are three buttons.\n\n\n\n\n\n\n\n\n\n\nThe first button on the left that looks like two partially overlapping boxes will copy the commit’s ID so that we can paste it elsewhere if we want. In GitHub, every commit is assigned a unique ID, which is also called an “SHA” or “hash”. The commit ID is a string of 40 characters that can be used to refer to a specific commit. The 274519 displayed on the middle button is the first 7 characters of this commit’s ID.\nAs noted above, the middle button is labeled with the first 7 characters of this commit’s ID - 274519. Clicking on it will take us to a new screen with the details of what this commit does to the files in the repository (i.e., additions, edits, and deletions). Arthur will click it so we take a look momentarily.\nThe button on the far right, which is labeled with two angle brackets (&lt; &gt;) will take us back to the repository’s homepage. However, the files in the repository will be set back to the state they were in when the commit was made. In this case, there is only one commit. So, there’s no difference between the current state of the repository and the state it would be in if Arthur clicked this button. However, this button can be useful. If Arthur makes some changes to a file and then later wants to see what the file looked like before he made those changes, he can use this button to take a look.\n\nNow, Arthur will click the middle button labeled with the short version of the commit ID.\nOn the page he is taken to, we can see more details about what commit 274519 does to the files in the repository. The top section of the page (outlined in red below) contains pretty much the same information we saw on the previous page. The little symbol on the left that looks kind of like a backwards 4 with open circles at the ends of the lines tells us which branch we are operating on. Branches are a more advanced topic that we will discuss later. Currently, our repository only has one branch – the default main branch – and the symbol followed by the word “main” is telling us that this commit is on the main branch. To the far right of this section, there is a button that says Browse files. Clicking this button does the exact same thing as the button on the previous page that was labeled with two angle brackets (&lt; &gt;). Below the Browse files button, are the words 0 parents and commit 277451996a7e9a0a6e583124d762db2a9cd439a2. This tells us that this commit doesn’t have any parent commits and that the full commit ID is 277451996a7e9a0a6e583124d762db2a9cd439a2. We discussed commit ID’s above. The parent commit is the commit or commits that this commit is based on. In other words, what were the other things that happened to get us to this point? Because this is the initial commit, there are no parent commits.\n\n\n\n\n\n\n\n\n\nThe middle section of the commit details page tells us that applying this commit to the repository changes 1 file. In that file, there are two additions and no deletions. Below this text we can see which file was changed - README.md. This is also called the diff view because we can see the differences between this version of the file and previous versions of the file. In this case, because there wasn’t a previous version of the file, we just see the two additions that were made to the file. They are the level one header that was added to the first line of the file (i.e., # r4epi_example_project) and our project’s description was added to the second line of the file. These additions were made automatically by GitHub. We know they are additions because the background color is green and there is a little plus sign immediately to their left. We know which lines of the file were changed because GitHub shows us the line number immediately to the left of the plus signs.\n\n\n\n\n\n\n\n\n\nThe final section of the commit details page shows us any existing comments that Arthur, or others, made about this commit. It also allows us, or others to create a new comment, using the text box.\n\n\n\n\n\n\n\n\n\nIn the screenshot below, we can see an example comment. Note all the cool things features GitHub comments allow us to use. We can format the text, add bullets, add links, and even add clickable checkboxes.\n\n\n\n\n\n\n\n\n\nFinally, clicking the green Comment on this commit button adds our comment to the commit details page.\n\n\n\n\n\n\n\n\n\nLet’s pause here for a moment and try to appreciate how powerful GitHub already is compared to other cloud-based file storage services like Dropbox, Google Drive, or OneDrive. Like those file storage services, all of our files are backed up and preserved in the cloud and can easily be shared with others. However, unlike Dropbox, Google Drive, and OneDrive, we can turn our repository’s homepage into a little website describing our project, we can view all the changes that have been made to our project over time, we can see which specific lines of each file have changed and how, and we can gather all comments, questions, and concerns about the files in one place. Oh, and it’s Free!\n\n\nStep 2: Clone the repository to your computer\nAt this point, Arthur’s repository, which is just a fancy file folder, and the one file in his repository (README.md), only exist on the GitHub cloud.\n\n\n\n\n\n\nNote\n\n\n\nWhat is “the GitHub cloud”? For our purposes, the cloud just refers to a specific type of computer – called a server – that physically exists somewhere else in the world, which we can connect to over the internet. GitHub owns many servers, and our files are stored on one of them. After we connect to the GitHub server, we can pass files back and forth between our computer and GitHub’s computer (i.e., the server).\n\n\n\n\n\n\n\n\n\n\nFigure 39.2: GitHub Cloud.\n\n\n\n\n\nSo, how does he get the repository from the GitHub cloud to his computer so that he can start making changes to it?\nHe will clone the repository to his computer. Don’t get thrown off by the funny name. You can simply think “make a copy of” whenever you see the word “clone” for now. So, he will “make a copy of” the repository on his computer. However, cloning the repository actually does two very useful things at once:\n\nIt creates a copy of our repository, and all of the files and folders in it, on our computer.\n\nIt creates a connection between our computer and the GitHub cloud that allows us to pass files back and forth.\n\nThere are multiple possible ways we could clone our repository, but we’re going to use GitKraken in this book. If you did not already download GitKraken and connect it with your GitHub account as demonstrated at the beginning of the chapter, please do so now.\nWhen we open GitKraken, we should see something similar to the screenshot below. Arthur will start the cloning process by clicking the Clone a repo button.\n\n\n\n\n\n\n\n\n\nWhen the Repository Management dialogue box opens, he will need to make 3 changes.\n\nClick GitHub.com in the clone menu. This tells GitKraken that the repository he wants to clone currently lives on his GitHub account. Note that it has to be on his account in order for it to show up on this list – not someone else’s account. We will learn how to get files from someone else’s account later.\nSet the path where he wants the repository to be cloned to. Remember, the repository is a just a folder with some files in it. When we clone the repository to our computer, those files and folders will live on our computer somewhere. We need to tell GitKraken where we want them to live. In the screenshot below, Arthur is just cloning the repository to his computer’s desktop.\nTell GitKraken which repository on his GitHub account he wants to clone. We can use the drop-down arrow to search a list of all of our repositories. In the screenshot below, Arthur selected the r4epi_example_project repository.\n\n\n\n\n\n\n\n\n\n\nFinally, he will click the green Clone the repo! button. Now, he has successfully cloned his repository to his computer! 🎉\nBefore moving on, let’s pause and review what just happened.\n\n\n\n\n\n\n\n\n\nAs we discussed above, Arthur’s repository already existed on the GitHub cloud see Figure 39.2. In git terminology, the GitHub cloud called a remote repository, or “repo” for short. Remote repositories are just copies of our repository that live on the internet or some other network. Arthur then cloned his remote repository to his computer. That means, he made a copy of all of the files and folders on his computer. In git terminology, the repository on our computer is called a local repository.\nNow that he has successfully cloned his repository, he should be able to view it in two different ways.\nFirst, he should be able to see his repository’s file folder on his desktop (because that’s the location he chose above).\n\n\n\n\n\n\n\n\n\nSecond, he should be able to open a tab in GitKraken with all the versioning information about his repository.\n\n\n\n\n\n\n\n\n\nLet’s pause here and watch a brief video from GitKraken that orients us to the GitKraken user interface. For now, the first three minutes of the video is all we need. There may be some unfamiliar terms in the video. Don’t stress about it! We will cover the most important parts after the video and learn some of the other terms in future examples.\n\nMoving back to Arthur’s repository, we can see that the repository graph in the middle section of the user interface has only on commit – the initial commit. This matches what we saw on GitHub.\n\n\n\n\n\n\n\n\n\nIf we zoom in on the upper left corner of the left sidebar menu (outlined in red below), we can see that GitKraken is aware of two different places where the repository lives. First, it tells us that Arthur has a local repository on his computer with one branch – the main branch. Next, it tells us that there is one remote location for the repository – called “origin” – with one branch – the main branch.\nThe term “origin” is used by convention in the git language to refer to the remote repository that we originally cloned from. It uses the nickname “origin” instead of using the remote repository’s full URL (i.e., web address). Arthur could change this name if he wanted, but there’s really no need.\n\n\n\n\n\n\n\n\n\nAnother useful thing we can see in the current view, is that the local repository and the remote repository on GitHub are in sync. Meaning, the files and folders in the repository on Arthur’s computer are identical to the files and folders in the repository on the GitHub cloud. We know this because the little white and gray picture that represents the remote repository and the little picture of the laptop that represents the local repository are located side-by-side on the repository graph (see red arrow below). When we have made changes in one location or another, but haven’t synced those changes to the other location, the two icons will be in different rows of the repository graph. We will see an example of this soon.\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Add an R project file to the repository\nThis step is technically optional, but we highly recommend it! We introduced R projects earlier in the book. Arthur will go ahead and add an R project file to his repository now. This will make his life easier later. To create a new R project, he just needs to click the drop-down arrow next to the words Project: (None) to open the projects menu. Then, he will click the New Project... option.\n\n\n\n\n\n\n\n\n\nThat will open the new project dialogue box. This time, he will click the Existing Directory option instead of clicking the New Directory option. Why? Because the directory (i.e., folder) he wants to contain his R project already exists on his computer. Arthur cloned it to his desktop in [step 2][Step 2: Clone the repository] above.\n\n\n\n\n\n\n\n\n\nAll Arthur has to do now, is tell RStudio where to find the r4epi_example_project directory on his computer using the Browse... button. In this case, on his desktop. Finally, he will click the Create Project button.\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Update and commit gitignore\nLet’s take a look at Arthur’s RStudio files pane. Notice that there are now three files in the project directory. There is the README file, the .Rproj file, and a file called .gitignore. RStudio created this file automatically when Arthur designated the directory as an R project.\nOutside of the name – .gitignore – there is nothing special about this file. It’s just a plain text file. But naming it .gitignore tells the git software that it contains a list of files that git should ignore. By ignore, we mean, “pretend they don’t exist.”\n\n\n\n\n\n\n\n\n\nArthur will now open the .gitignore file and see what’s there.\n\n\n\n\n\n\n\n\n\nCurrently, there are four files on the .gitignore list. These files were added automatically by RStudio to try to help him out. Tracking versions of these files typically isn’t useful. Because these files are on the .gitignore list, git and GitHub won’t even notice if Arthur creates, edits, or deletes any of them. This means that they also won’t ever be uploaded to GitHub.\nAt this point, Arthur is going to go ahead and add one more file to the .gitignore list. He will add .DS_store to the list. .DS_store is a file that the MacOS operating system creates automatically when a Mac user navigates to a file or folder using Finder. None of that really matters for our purposes, though. What does matter is that there is no need to track versions of this file and it will be a constant annoyance if Arthur doesn’t ignore it.\nIf Arthur were using a Windows PC instead of a Mac, the .DS_store file should not be an issue. However, adding .DS_store to .gitignore isn’t a bad idea even when using a Windows PC for at least two reasons. First, there is no harm in doing so. Second, if Arthur ever collaborates with someone else on this project who is using a Mac, then the .DS_store file could find its way into the repository and become an annoyance. Therefore, we recommend always adding .DS_store to the .gitignore list regardless of the operating system you personally use.\nAdding .DS_store (or any other file name) to the .gitignore list is as simple as typing .DS_store on its own line of the .gitignore file and clicking Save.\n\n\n\n\n\n\n\n\n\nTypically, the next thing we would do after creating our repository is to start creating and adding the files we need to complete our analyses.\nNow, Arthur will open GitKraken so we can take a look. Notice that Arthur’s GitKraken looks different than it did the last time we viewed it. That’s because we’ve been making changes to the repository. Specifically, we’ve added two files since the last commit was made. There are at least two ways we can tell that is the case.\nFirst, the repository graph in the middle section of the user interface has now has two rows. The bottom row is still the initial commit, but now there is a row above it that says // WIP and has a + 2 symbol. WIP stands for work in progress and the + 2 indicates that there are two files that have changed (in this case, they were added) since the last commit. So, Arthur has been working on two files since his last commit.\nAdditionally, the commit panel on the right side of the screen shows that there are two new uncommitted and unstaged files in the directory. They are .gitignore and r4epi_example_project.Rproj.\n\n\n\n\n\n\n\n\n\nAt this point, Arthur wants to take a snapshot of the state of his repository. Meaning, he wants to save a version of his repository as it currently exists. To do that, he first needs to stage the changes since the previous commit that he wants to be included in this commit. In this case, he wants to include all changes. So, he will click the green Stage all changes button located in the commit panel.\n\n\n\n\n\n\n\n\n\nAfter clicking the Stage all changes button, the two new files are moved down to the Staged Files window of the commit panel.\n\n\n\n\n\n\n\n\n\nNext, Arthur will write a commit message. Just like there are best practices for writing R code, there are also best practices for writing commit messages. Here is a link to a blog post that we think does a good job of explaining these best practices: https://cbea.ms/git-commit.\nThe first line is called the commit message. You can think of the commit message as a brief summary of what this commit does to the repository. This message will help Arthur and his collaborators find key commits later in the future. In this context, “brief” means 72 characters or less. GitKraken tries to help us out by telling us how many characters we’ve typed in our commit message. Additionally, the commit message should be written in the imperative voice – like a command. Another way to think about it is that the commit message should typically complete the phrase, “If applied, this commit will…”. The screenshot below shows that Arthur wrote Add Rproj and gitignore to project (red arrow 1).\nIn addition to the commit message, there is also a description box we can use to add more details about the commit. Sometimes, this is unnecessary. However, when we do choose to add a description, it is best practice to use it to explain what the commit does or why we chose to do it rather than how it does whatever it does. That’s in the code. In the screenshot below, you can see that Arthur added some bulleted notes to the description (red arrow 2).\nFinally, Arthur will click the green commit button at the bottom of the commit panel (red arrow 3). This will commit (save) a version of our repository that includes the changes to any of the files in the Staged Files window.\n\n\n\n\n\n\n\n\n\nAnd here is what his GitKraken screen looks like after committing.\n\n\n\n\n\n\n\n\n\nLet’s pay special attention to what is being displayed in a couple of different areas. We’ll start by zooming in on the commit panel.\nAt the top of the commit panel, we can see the short version of the commit ID – 4a394b. Below that, we can see the commit message and description. Below that, we can see who created the commit and when. This tends to be more useful when we are collaborating with others. To the right of that information, GitKraken also shows us the commit ID for this commit’s parent commit – 277451. Finally, it shows us the file changes that this commit applies to our repository. More specifically, it shows us the changes that commit 4a394b makes to commit 277451.\n\n\n\n\n\n\n\n\n\nAt this point, you may be wondering what this whole parent-child thing is and why we keep talking about it. The diagram below is a very simple graphical representation of how git views our repository. It views it as a series of commits that chronologically build our repository when they are applied to each other in sequence. Familial terms are often used in the git community to describe the relationship between commits. For example, in the diagram below commit 4a394b is a child of commit 288451. Child commits are always more recent than parent commits. This knowledge is not incredibly useful to us at this point, but it can be helpful when we start to learn about more advanced topics like merging commits. For now, just be aware of the terminology.\n\n\n\n\n\n\n\n\n\nIt is also important to point out that Arthur’s most recent commit (4a394b) only exists in his local repository. That is, the repository on his computer. He has not yet shared the commit – or the new files associated with the commit – to the remote repository on GitHub.\n\n\n\n\n\n\n\n\n\nHow do we know? Well, one way we can tell is by looking at Arthur’s GitKraken window. In the repository graph, the local repository (i.e., the little laptop icon) and the remote repository (i.e., the little gray and white icon) are on different rows. Additionally, there is a little 1 next to an up arrow displayed to the left of the main branch of our local repository in the left panel of GitKraken. Both of these indicate that the most recent commits contained in each repository are different. Specifically, that the local repository is one commit ahead of the remote repository.\nThis concept is important to understand. [In Google Docs][versioning], when we made a change to our document locally, that change was automatically synced to Google’s servers. We didn’t have to do anything to save/create a version of the document. We had to put in a little effort if we wanted to name a particular version, but the version itself was already saved – identified using a date-time stamp. Conversely, git does not automatically make commits (i.e., save snapshots of the state of the files in our repository), nor does our local repository automatically sync up with our remote repository (in this case, GitHub). We have to do both of these things manually. This will create a little extra work for us, but it will also give us a lot more control.\n\n\n\n\n\n\n\n\n\nAs one additional check, Arthur can go look at the repository’s commit history on GitHub. As shown in the screenshot below, the commit history still only shows one commit – the initial commit.\n\n\n\n\n\n\n\n\n\nLet’s quickly pause and recap what Arthur has done so far.\n\n\n\n\n\n\n\n\n\nFirst, Arthur created a repository on GitHub. It was a remote repository because he accesses it over the internet. Then, he cloned (i.e., made a copy of) the remote repository to his computer. This copy is referred to as a local repository. Next, Arthur made some changes to the repository locally and committed them. At this point, the local repository is 1 commit ahead of the remote repository, and the changes that Arthur made locally are not currently reflected on GitHub.\nSo, how does Arthur sync the changes he made locally with GitHub? He will push them to GitHub, which GitKraken makes incredibly easy. All he needs to do is click the Push button at the top of his GitKraken window (see below).\n\n\n\n\n\n\n\n\n\nAfter doing so, we will once again see some changes. What changes do you notice in the screenshot below?\n\n\n\n\n\n\n\n\n\nIn the repository graph, the local repository (i.e., the little laptop icon) and the remote repository (i.e., the little gray and white icon) are back on the same row. Additionally, the little 1 next to an up arrow is no longer displayed in the left panel. Both of these changes indicate that the most recent commits contained in each repository are the same.\nAnd if Arthur once again checks GitHub…\n\n\n\n\n\n\n\n\n\nHe will now see that the GitHub repository also has two commits. He can click on the text that says 2 commits to view each commit in the commit history.\n\n\n\n\n\n\n\n\n\nIn the commit history, he can now see commit 4a394b7. Let’s take another pause here and recap.\n\n\n\n\n\n\n\n\n\nFirst, Arthur created a repository on GitHub. Then, he cloned the remote (i.e., GitHub) repository to his computer. Next, Arthur made some changes to the repository locally and committed them locally. Finally, he pushed the local commit up to GitHub. Now, his GitHub repository and local repository are in sync with each other.\nWe realize that it probably seems like it took a lot of work for Arthur to get everything set up. But in reality, all of the steps up to this point will only take a couple of minutes once you’ve gone through them a few times.\n\n\nStep 5: Keep adding and committing files\nAt this point, Arthur has his repositories all set up and is ready to start rocking and rolling on his actual data analysis. To round out this example, Arthur will add some data to his repository that he will eventually analyze using R.\n\n\n\n\n\n\n\n\n\nThe screenshot above shows that Arthur created a new folder inside the R project directory called data. He created it in the same way he would create any other new folder in his computer’s operating system. Then, he added a data set to the data folder he created. This particular data set happens to be stored in an Excel file named form_20.xlsx.\nNow, when Arthur checks GitKraken, this is what he sees in the commit panel.\n\n\n\n\n\n\n\n\n\nJust like before, GitHub is telling Arthur that he has a new unstaged file in the repository. Stop for a moment and think. What should Arthur do next?\nWas your answer, “stage and commit the new file”? If so, slow down and think again. Remember, in general, we don’t ever want to commit our research data to our GitHub repository. GitHub is not typically considered secure or private. So, how can Arthur keep the data in his local repository so that he can work with it, keep his local repository synced with GitHub, but make sure the data doesn’t get pushed up to GitHub?\nDo you remember earlier when Arthur told git and GitHub to ignore the .DS_Store file? In exactly the same way, Arthur can tell git and GitHub to ignore this data set. And once it’s ignored, it won’t ever be pushed to GitHub. Remember, our local git repository only includes files it’s tracking in commits, and it only pushes commits (and the files included in them) up to GitHub.\nIn the screenshot below, Arthur added data/ to line 6 of the .gitignore file. He could have added form_20.xlsx instead. That would have told git to ignore the form_20.xlsx data set specifically. However, Arthur doesn’t want to push any data to GitHub – including any data sets that he may add in the future. By adding data/ to the .gitignore file, he is telling git to ignore the entire folder named data and all of the files it contains – now and in the future.\n\n\n\n\n\n\n\n\n\nAfter saving the updated .gitignore file, the commit pane in GitKraken changes once again.\n\n\n\n\n\n\n\n\n\nThe new file data/form_20.xlsx is no longer showing up as an unstaged change. Instead, the only unstaged change showing up is the edited .gitignore file. We can tell that the changes to the .gitignore file are edits – as opposed to adding the file for the first time – because there is a little pencil icon to the left of the file name instead of a little green plus icon. Now what should Arthur do next?\nWas your answer, “stage and commit the edited file”? If so, you are correct! Now it is safe for Arthur to go ahead and commit these changes.\nAfter doing so, he can see that the GitHub repository contains 3 commits. Additionally, as shown the red box below, the data folder is nowhere to be found among the files contained in the GitHub repository.\n\n\n\n\n\n\n\n\n\nArthur will now add one final file to the r4epi_example_project as part of this example. He will add an Quartofile with a little bit of R code in it. The code will import form_20.xlsx into the global environment as a data frame.\n\n\n\n\n\n\n\n\n\nAn then he will commit and push the data_01_import.Rmd to GitHub in the same way he committed and pushed previous files to Github.\n\n\n\n\n\nA gif about data import.\n\n\n\n\nArthur can continue adding files to his local repository and then pushing them to GitHub in this fashion for the remainder of the time he is working on this project, and the introduction to git and GitHub chapter discusses why he should consider doing so.\nAfter going through this example, many students have three lingering questions:\n\nHow often should we commit?\nHow often should we push our commits to GitHub?\nIf we can’t use GitHub to share our data, how should we share data?\n\nWe will answer questions 1 & 2 immediately below. We will answer the third question in the next example.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html#committing-and-pushing",
    "href": "chapters/using_git_github/using_git_github.html#committing-and-pushing",
    "title": "39  Using git and GitHub",
    "section": "39.6 Committing and pushing",
    "text": "39.6 Committing and pushing\nAs we are learning to use git and GitHub, it is reasonable to ask how often we should commit our work as we go along. For better or worse, there is no hard-and-fast rule we can give you here. In Happy Git and GitHub for the useR, Dr. Jennifer (Jenny) Bryan writes that we should commit “every time you finish a valuable chunk of work, probably many times a day.”2 This seems like a pretty good starting place to us.\nOf course, a natural follow-up question is to ask how often we should push our commits to GitHub. We could automatically push every commit we make to GitHub as soon as we make it. However, this isn’t always a good idea. It is much easier to edit or rollback commits that we have only made locally than it is to edit or rollback commits that we’ve pushed to our remote repository. For example, if we accidentally include a data set in a commit and push it to GitHub, this is a much bigger problem than if we accidentally include a data set in a commit and catch it before we push to GitHub. For this reason, we don’t suggest that you automatically push every commit you make to GitHub. So, how often should you push? Well, once again, there is no hard-and-fast rule. And once again, we think Dr. Bryan’s advice is a good starting point. She writes, “Do this [push] a few times a day, but possibly less often than you commit.”2 It is also worth noting that how often you commit and push will also be dictated, at least partially, by the dynamics of the group of people who are contributing to the repository. So far, we have really only seen a repository with a single contributor (i.e., Arthur Epi). That will change in the next example.\nThe advice above about committing and pushing may seem a little vague to you right now. It is a little vague. We apologize for that. However, we believe it’s also the best we can do. On the bright side, as you practice with git and GitHub, you will eventually fall into a rhythm that works well for you. Just give it a little time!",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html#example-3-contribute-to-a-research-project",
    "href": "chapters/using_git_github/using_git_github.html#example-3-contribute-to-a-research-project",
    "title": "39  Using git and GitHub",
    "section": "39.7 Example 3: Contribute to a research project",
    "text": "39.7 Example 3: Contribute to a research project\nWhen our research assistants begin helping us with data management and analysis projects, we often have them start by going to the project’s GitHub repository to read the existing documentation and clone all the existing code to their computer. This example is going to walk through that process step-by-step. For demonstration purposes, we will work with the example repository that our fictitious research assistant named Arthur Epi created in Example 2 above.\n\n\n\n\n\n\nNote\n\n\n\nIt’s probably worth noting that in most real-world scenarios the roles here would be reversed. That is, we (Brad or Doug) would have created the original repository and Arthur would be working off of it. However, the example repository above was already created using Arthur’s GitHub account, and we will continue to work off of it in this example. If you are a research assistant working with us (i.e., Brad or Doug) in real life, and using this example to walk yourself through getting started on a real project, you should insert yourself (and your GitHub account) into Brad’s role (and GitHub account) in the example below.\n\n\nIn this example, we’re going to work collaboratively with Arthur on the r4epi_example_project. Arthur could have just emailed us all of the project files, but sometimes that might be many files, some of them may be very large, and he runs the risk of forgetting to send some of them by accident. Further, every time any of the contributors adds or updates a file, they will have to email all the other contributors the new file(s) and an explanation of the updates they’ve made. This process is typically inefficient and error prone. Conversely, Arthur could set up a shared folder on a cloud-based file storage service like Dropbox, Google Drive, or OneDrive. Doing so would circumvent the issues caused by emailing files that we just mentioned (i.e., many files, large files, forgetting files, and manually sending updates). However, Dropbox, Google Drive, and OneDrive aren’t designed to take advantage of all that git and GitHub have to offer (e.g., project documentation, versioning and version history, viewing differences between code versions, issue tracking, creating static websites for research dissemination, and more). Because Arthur created his repository on GitHub, all of the files and documentation we need to get started assisting him are easily accessible to us. All, he has to do is send us the repository’s web address, which is https://github.com/arthur-epi/r4epi_example_project.\nAfter navigating to a GitHub repository, the first thing we typically want to do is read the README. It should have some useful information for us about what the repository does, how it is organized, and how to use it. Because this is a fictitious, minimal example for the book, the current README in the r4epi_example_project project isn’t that useful, impressive, or informative. Matias Singers maintains a list of great READMEs at the following link that you may want to check out: https://github.com/matiassingers/awesome-readme. If you want to see an example README from a real research project that we worked on, you can check out this link: https://github.com/brad-cannell/detect_pilot_test_5w. After we read over the README file, we are ready to start making edits and additions to the project. But how do we do that?\nWhile it is technically possible for us to edit code files directly on GitHub (see [Contributing to R4Epi]), this is typically only a good idea for extremely minor edits (e.g., a typo in the documentation). Typically, we will want to make a copy of all the code files on our computer so that we can experiment with the edits we are making. Said another way, we can suggest edits to R code files directly on GitHub, but we can’t run those files in R directly on GitHub to make sure they do what we intend for them to do. To test our changes in R, we will need all of the repository’s files on our local computer. And how do we do that?\n\n39.7.1 Forking a repository\nIf your answer the question above was, “we clone the r4epi_example_project repository to our computer” you were close, but that isn’t our best option here. While we technically can clone public repositories that aren’t on our account, we can’t push any changes to them. And this is a good thing! Think about it, do we really want any person out there on the internet to be able to make changes to our repository anytime they want without any oversight from us? No way!\n\n\n\n\n\n\n\n\n\nIn this case, forking the repository is going to be the better option. This is another funny name, but we are once again just talking about making a copy of the repository. However, this time we are copying the repository from the original GitHub account (i.e., Arthur’s) to our GitHub account. With cloning, we were copying the repository from the original GitHub account to our computer. Do you see the difference? Let’s try to visualize it.\n\n\n\n\n\n\n\n\n\nThe purple arrow above indicates that we are forking (i.e., making a copy of) the original r4epi_example_project repository on Arthur’s GitHub account to Brad’s GitHub account. And doing so is really easy. All Brad has to do is log in to GitHub and navigate to Arthur’s r4epi_example_project repository located at https://github.com/arthur-epi/r4epi_example_project. Then, he needs to click on the Fork button located near the top-right corner of the screen.\n\n\n\n\n\n\n\n\n\nThen Brad will click the green Create fork button on the next page.\n\n\n\n\n\n\n\n\n\nAnd after a few moments, this will create an entirely new repository on Brad’s GitHub account. It will contain an exact copy of the all the files that were on the repository in Arthur’s GitHub account, but Brad is the owner of this repository on his account (shown in the screenshot below).\n\n\n\n\n\n\n\n\n\nBecause Brad is the owner of this repository, he can clone it to his local computer, work on it, and push changes up to GitHub in exactly the same way that Arthur did in the example above. Just to be clear, the changes that Brad pushes to his GitHub repository will have no effect on Arthur’s GitHub repository.\n\n\n\n\n\n\nNote\n\n\n\nAs we’ve pointed out multiple times in this chapter, we generally do not want to upload research data to GitHub. Why? Because it isn’t typically considered private or secure. However, in order for Brad to do work on this project, he will need to access the data somehow. This will require Arthur to share to data with Brad through some means other than GitHub. Different organizations have different rules about what is considered secure. For example, it may be an encrypted email or it may be a link to a shared drive on a secure server. However the data is shared, it is important for Brad to create the same file structure on his computer that Arthur has on his computer. Otherwise, the R code will not work on both computers. Remember from the example above that Arthur created a data/ folder in his local repository and he moved the form_20.xlsx data to that folder. Then, in the data_01_import Quartofile, he imports the data using the relative path data/form_20.xlsx. In the chapter on file paths we discussed the advantages of using relative file paths when working collaboratively. Just remember, in order for this relative file path to work identically on Arthur’s computer and Brad’s computer, the folder structure and file names must also be identical. So, if Brad put the form_20.xlsx data in a folder in his local repository called data sets/ instead of data/, then the code in the data_01_import Quartofile would throw an error.\n\n\n\n\n\n\n\n\n\n\n\nNotice that in the diagram above, Arthur’s original repository is totally unaffected by any changes that Brad is pushing from his local computer to the repository on his GitHub account. There is no arrow from Brad’s remote repository going into Arthur’s remote repository. Again, this is a good thing. Literally anyone else in the world with a GitHub account could just as easily fork the repository and start making changes. If they also had the ability to make changes to the original repository at will, they could potentially do a lot of damage!\nHowever, in this case, Arthur and Brad do know each other and they are working collaboratively on this project. And at some point, the work that Brad is doing needs to be synced up with the work that Arthur is doing. In order to make that happen, Brad will need to send Arthur a request to pull the changes from Brad’s remote repository into Arthur’s remote repository. This is called a pull request.\n\n\n\n\n\n\n\n\n\n\n\n39.7.2 Creating a pull request\nTo make this section slightly more realistic, let’s say that Brad adds some code to data_01_import.Qmd. Specifically, he adds some code that will coerce the date_received column from character strings to dates (code below).\n\n\n\n\n\n\n\n\n\nThen, Brad commits the changes and pushes them up to his GitHub account. Now, when he checks his GitHub account he can see that his remote repository is 1 commit ahead of Arthur’s remote repository. And that makes sense, right? Brad just updated the code in data_01_import.Qmd, committed that changed, and pushed the commit to his GitHub account, but nothing has changed in the repository on Arthur’s GitHub account.\n\n\n\n\n\n\n\n\n\nNow, Brad needs to create a pull request. This pull request will let Arthur know that Brad has made some changes to the code that he wants to share with Arthur. To do so, Brad will click Contribute and then click the green Open pull request button as shown below.\n\n\n\n\n\n\n\n\n\nThe top section of the next screen, which is outlined in red below, allows Brad to select the repository and branch on his GitHub account that he wants to share with Arthur (to the right of the arrow). More specifically, he is sending a request to Arthur asking him to merge his code into Arthur’s code. In this case, the code he wants to ask Arthur to merge is on the main branch of the brad-cannell/r4epi_example_project repository (Brad’s repository only has one branch – the main branch – at this point). To the left of the arrow, Brad can select the repository and branch on Arthur’s GitHub account that he wants to ask Arthur to merge the code into. In this case, the main branch of the arthur-epi/r4epi_example_project repository (Arthur’s repository only has the main branch at this point as well).\nBelow the red box, GitHub is telling Brad about the commits that will be sent in this pull request and the changes that will be made to Arthur’s files if he merges the pull request into his repository. In this case, only one file in Arthur’s repository would be altered – data_01_import.Rmd. Below that, Brad can see that the exact differences between his version of data_01_import.Rmd and the version that currently exists in Arthur’s repository. How cool is that that Brad and Arthur can actually see exactly how this pull request changes the file state down to individual lines of code?\nBecause Brad is satisfied with what he sees here, he clicks the green Create pull request button shown in the middle right of the screenshot below.\n\n\n\n\n\n\n\n\n\nLet’s pause here and get explicit about two things.\n\nAs we’ve tried to really drive home above, this pull request will not automatically make any changes to Arthur’s repository. Rather, it will only send Arthur Brad’s code, ask him to review it, and then allow him to choose whether to incorporate it into his repository or not.\nPull requests are sent at the branch level not at the file level. Meaning, if Arthur accepts Brad’s pull request, it will make all of the files on his main branch identical to all of the files on Brad’s main branch (the main branch because that is the branch Brad chose in the screenshot above – and currently the only branch in either repository). In this case, that means that the only file that would change as a result of copying over the entire branch is data_01_import.Rmd. However, if Brad had made changes to data_01_import.Rmd and another file, Arthur would only have the option to merge both files or neither file. He would not have the option of merging data_01_import.Rmd only. Pull requests merge the entire branch, not specific files. We are emphasizing this because this may affect how you commit, push, and create pull requests when you are working collaboratively. More specifically, you may want to commit, push, and send pull requests more frequently than you would if you were working on a project independently.\n\nOn the next screen, Brad is given an opportunity to give the pull request a title and add a message for Arthur that give him some additional details. In general, it’s a good idea to fill this part out using similar conventions to those described above for commit messages.\nAfter filling out the commit message, Brad will click the green Create pull request button on last time, and he is done. This will send Arthur the pull request.\n\n\n\n\n\n\n\n\n\nThe next time Arthur checks the r4epi_example_project on GitHub, he will see that he has a new pull request.\n\n\n\n\n\n\n\n\n\nIf he clicks on the text Pull requests text, he will be taken to his pull requests page. It will show him all pending pull requests. In this case, there is just the one pull request that Brad sent.\n\n\n\n\n\n\n\n\n\nWhen he clicks on it, he will see a screen like the one in the screenshot below. Scanning from top to bottom, it will tell him which branch Brad is requesting to merge the code into, show him the message Brad wrote, tell him that he can merge this branch without any conflicts if he so chooses, and give him an opportunity to write a message back to Brad before deciding whether to merge this pull request or close it.\n\n\n\n\n\n\n\n\n\nHe also has the option to view some additional details by clicking the Commits tab, Checks tab, and/or Files changed tab towards the top of the screen. Let’s say he decides to click on the Files changed tab.\nOn the Files changed tab, Arthur can see each of the files that the pull request would change if he were to merge it into his repository (in this case, only one file). For each file, he can see (and even comment on) each specific line of code that would change. In this case, Arthur is pleased with the changes and navigates back to the Conversation tab by clicking on it.\n\n\n\n\n\n\n\n\n\nBack on the Conversation tab (see screenshot below), Arthur has some options. If he wants more clarification about the pull request, he can send leave a comment for Brad using the comment box near the bottom of the screen. If he knows that he does NOT want to merge this pull request into his code, he can click the Close pull request button at the bottom of the screen. This will close the pull request and his code will remain unchanged. In this case, Arthur wants to incorporate the changes that Brad sent over, so he clicks the green Merge pull request button in the middle of the screen.\n\n\n\n\n\n\n\n\n\nThen, he is given an opportunity to add some details about the changes this merge will make to the repository once it is committed. You can once again think of this message as having a very similar purpose to commit messages, which were discussed above. In fact, it will appear as a commit in the repository’s commit history.\nFinally, he clicks the green Confirm merge button.\n\n\n\n\n\n\n\n\n\nAnd if Arthur navigates back to his commit history page, he can see two new commits. Brad’s commit with the updated data_01_import.Qmd file, and the commit that was automatically created when Arthur merged the branches together.\n\n\n\n\n\n\n\n\n\nNow, Arthur takes a look at data_01_import.Qmd on his computer. To his surprise, the code to coerce date_received into dates isn’t there. Why not?\n\n\n\n\n\n\n\n\n\nWell, let’s open GitKraken on Arthur’s computer and see if we can help him figure it out. In the repository graph, Arthur’s local repository (i.e., the little laptop icon) and the remote repository (i.e., the little gray and white icon) are on different rows. Additionally, there is a little 2 next to a down arrow displayed to the left of the main branch of our local repository in the left panel of GitKraken. Both of these indicate that the most recent commits contained in each repository are different. Specifically, that the local repository is two commits behind the remote repository.\n\n\n\n\n\n\n\n\n\nSo, let’s pause here for a second and review what we’ve done so far. As shown in the figure below:\n\nBrad made some updates to the code on his computer and then committed those changes to his local repository. At this point, his local repository is out of sync with his remote repository, Arthur’s remote repository, and Arthur’s local repository.\nNext, Brad pushed that commit from his local repository up to his remote repository on GitHub. After doing so, his local repository and remote repository are synced with each other, but they are still out of sync with Arthur’s remote repository and Arthur’s local repository.\nThen, Brad created a pull request for Arthur. The request was for Arthur to pull the latest commit from Brad’s remote repository into Arthur’s remote repository.\nArthur accepted and merged Brad’s pull request. After doing so, his remote repository, Brad’s remote repository, and Brad’s local repository are all contain the updated data_01_import.Qmd file, but Arthur’s local repository still does not.\n\n\n\n\n\n\n\n\n\n\nSo, how does Arthur get his local repository in sync with his remote repository?\nArthur just needs to use the pull command to download the files from his updated remote repository and merge them into his local repository (step 5 below).\n\n\n\n\n\n\n\n\n\nAnd GitKraken makes pulling the files from his remote repository really easy. All Arthur needs to do is click the pull button shown in the screenshot below. GitKraken will download (also called fetch) the updated repository and merge the changes into his local repository.\n\n\n\n\n\n\n\n\n\nAnd as shown in the screenshot below, Arthur can now see that his local repository is now in sync with his remote repository once again! 🎉\n\n\n\n\n\n\n\n\n\nBut, what about Brad’s repository? Well, as you can see in the screenshot below, Brad’s remote repository is now 1 commit behind Arthur’s. Why?\nThis one is kind of weird/tricky. Although the code in Brad’s repository is now identical to the code in Arthur’s repository, the commit history is not. Remember, Arthur’s commit history from above? When he merged Brad’s code into his own, that automatically created an additional commit. And that additional commit does not currently exist in Brad’s commit history. It’s an easy fix though!\n\n\n\n\n\n\n\n\n\nAll Brad needs to do is a quick fetch from Arthur’s remote repository to merge that last commit into his commit history, and then pull it down to his local repository.\n\n\n\n\n\n\n\n\n\nTo do so, Brad will first click Fetch upstream followed by the green Fetch and merge button.\n\n\n\n\n\n\n\n\n\nAfter a few seconds, GitHub will show him that his remote repository is now synced up with Arthur’s remote repository. All he as to do now is a quick pull in GitHub.\n\n\n\n\n\n\n\n\n\nAnd now we have seen the basic process for collaboratively coding with git and GitHub. Don’t feel bad if you are still feeling a little bit confused. Git and GitHub are confusing at times even for experienced programmers. But that doesn’t mean that they aren’t still valuable tools! They are!\nWe also recognize that it might seem like that was a ton of steps above. Again, we went through this process slowly and methodically because we are all trying to learn here. In a real-life project with two experienced collaborators, the steps in this example would typically be completed in a matter of minutes. No big deal.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/using_git_github/using_git_github.html#summary",
    "href": "chapters/using_git_github/using_git_github.html#summary",
    "title": "39  Using git and GitHub",
    "section": "39.8 Summary",
    "text": "39.8 Summary\nThere is so much more to learn about git and GitHub, but that’s not what this book is about. So, we will stop here. We hope the examples above demonstrate some of the potential value of using git and GitHub in your project workflow. We also hope they give you enough information to get you started.\nHere are some free resources we recommend if you want to learn even more:\n\nChacon S, Straub B. Pro Git. Second. Apress; 2014. Accessed June 13, 2022. https://git-scm.com/book/en/v2\nGitHub. Getting started with GitHub. GitHub Docs. Accessed June 13, 2022. https://ghdocs-prod.azurewebsites.net/en/get-started\nBryan J. Happy Git and GitHub for the useR.; 2016. Accessed June 2, 2022. https://happygitwithr.com/index.html\nKeyes D. How to Use Git/GitHub with R. R for the Rest of Us. Published February 13, 2021. Accessed June 13, 2022. https://rfortherestofus.com/2021/02/how-to-use-git-github-with-r/\nWickham H, Bryan J. Chapter 18 Git and GitHub. In: R Packages. Accessed June 13, 2022. https://r-pkgs.org/git.html\n\n\n\n\n\n1. GitHub. Licensing a repository. Published online May 2022.\n\n\n2. Bryan J. Happy Git and GitHub for the useR.; 2016.",
    "crumbs": [
      "Collaboration",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Using git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html",
    "href": "chapters/creating_word_tables/creating_word_tables.html",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "",
    "text": "40.1 Table 1\nAt this point, you should all know that it is generally a bad idea to submit raw R output as part of a report, presentation, or publication. You should also understand when it is most appropriate to use tables, as opposed to charts and graphs, to present your results. If not, please stop here and read Chapter 7 of Successful Scientific Writing, which discusses the “why” behind much of what we will show you “how” to do in this chapter.1\nR for Epidemiology is predominantly a book about using R to manage, visualize, and analyze data in ways that are common in the field of epidemiology. However, in most modern work/research environments it is difficult to escape the requirement to share your results in a Microsoft Word document. And often, because we are dealing with data, those results include tables of some sort. However, not all tables communicate your results equally well. In this chapter, we will walk you through the process of starting with some results you calculated in R and ending with a nicely formatted table in Microsoft Word. Specifically, we are going to create a Table 1.\nIn epidemiology, medicine, and other disciplines, “Table 1” has a special meaning. Yes, it’s the first table shown to the reader of your article, report, or presentation, but the special meaning goes beyond that. In many disciplines, including epidemiology, when you speak to a colleague about their “Table 1” it is understood that you are speaking about a table that describes (statistically) the relevant characteristics of the sample being studied. Often, but not always, the sample being studied is made up of people, and the relevant descriptive characteristics about those people include sociodemographic and/or general health information. Therefore, it is important that you don’t label any of your tables as “Table 1” arbitrarily. Unless you have a really good reason to do otherwise, your Table 1 should always be a descriptive overview of your sample.\nHere is a list of other traits that should consider when creating your Table 1:",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#table-1",
    "href": "chapters/creating_word_tables/creating_word_tables.html#table-1",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "",
    "text": "All other formatting best practices that apply to scientific tables in general. This includes formatting requirements specific to wherever you are submitting your table (e.g., formatting requirements in the American Journal of Public Health).\nTable 1 is often, but not always, stratified into subgroups (i.e., descriptive results are presented separately for each subgroup of the study sample in a way that lends itself to between-group comparisons).\nWhen Table 1 is stratified into subgroups, the variable that contains the subgroups is typically the primary exposure/predictor of interest in your study.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#opioid-drug-use",
    "href": "chapters/creating_word_tables/creating_word_tables.html#opioid-drug-use",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.2 Opioid drug use",
    "text": "40.2 Opioid drug use\nAs a motivating example, let’s say that we are working at the North Texas Regional Health Department and have been asked to create a report about drug use in our region. Our stakeholders are particularly interested in opioid drug use. To create this report, we will analyze data from a sample of 9,985 adults who were asked about their use of drugs. One of the first analyses that we did was a descriptive comparison of the sociodemographic characteristics of 3 subgroups of people in our data. We will use these analyses to create our Table 1.\nYou can view/download the data by clicking here\n\n\nRows: 9985 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): age, edu, female, use\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n    var              cat    n n_total   percent        se   t_crit      lcl\n1 use_f        Non-users 8315    9985 83.274912 0.3734986 1.960202 82.52992\n2 use_f  Use other drugs 1532    9985 15.343015 0.3606903 1.960202 14.64925\n3 use_f Use opioid drugs  138    9985  1.382073 0.1168399 1.960202  1.17080\n        ucl\n1 83.994296\n2 16.063453\n3  1.630841\n\n\n\n\n  var use    n     mean       sd   t_crit        sem      lcl      ucl\n1 age   0 8315 36.80173 9.997545 1.960249 0.10963828 36.58681 37.01665\n2 age   1 1532 21.98362 2.979511 1.961515 0.07612296 21.83431 22.13294\n3 age   2  138 17.34740 3.081049 1.977431 0.26227634 16.82877 17.86603\n\n\n\n\n  row_var row_cat  col_var col_cat    n n_row n_total percent_total   se_total\n1     use       0 female_f      No 3077  8315    9985    30.8162243 0.46210382\n2     use       0 female_f     Yes 5238  8315    9985    52.4586880 0.49979512\n3     use       1 female_f      No  796  1532    9985     7.9719579 0.27107552\n4     use       1 female_f     Yes  736  1532    9985     7.3710566 0.26150858\n5     use       2 female_f      No   91   138    9985     0.9113671 0.09510564\n6     use       2 female_f     Yes   47   138    9985     0.4707061 0.06850118\n  t_crit_total  lcl_total  ucl_total percent_row    se_row t_crit_row  lcl_row\n1     1.960202 29.9178650 31.7293461    37.00541 0.5295162   1.960202 35.97359\n2     1.960202 51.4781679 53.4373162    62.99459 0.5295162   1.960202 61.95076\n3     1.960202  7.4565108  8.5197562    51.95822 1.2768770   1.960202 49.45247\n4     1.960202  6.8745702  7.9003577    48.04178 1.2768770   1.960202 45.54583\n5     1.960202  0.7426382  1.1179996    65.94203 4.0488366   1.960202 57.62323\n6     1.960202  0.3538217  0.6259604    34.05797 4.0488366   1.960202 26.61786\n   ucl_row\n1 38.04924\n2 64.02641\n3 54.45417\n4 50.54753\n5 73.38214\n6 42.37677\n\n\n\n\n   row_var row_cat col_var               col_cat    n n_row n_total\n1      use       0   edu_f Less than high school 3908  8315    9985\n2      use       0   edu_f           High school 2494  8315    9985\n3      use       0   edu_f          Some college  915  8315    9985\n4      use       0   edu_f      College graduate  998  8315    9985\n5      use       1   edu_f Less than high school  322  1532    9985\n6      use       1   edu_f           High school  567  1532    9985\n7      use       1   edu_f          Some college  321  1532    9985\n8      use       1   edu_f      College graduate  322  1532    9985\n9      use       2   edu_f Less than high school   36   138    9985\n10     use       2   edu_f           High school   36   138    9985\n11     use       2   edu_f          Some college   40   138    9985\n12     use       2   edu_f      College graduate   26   138    9985\n   percent_total   se_total t_crit_total  lcl_total  ucl_total percent_row\n1     39.1387081 0.48845160     1.960202 38.1855341 40.1002400    46.99940\n2     24.9774662 0.43322925     1.960202 24.1379137 25.8362747    29.99399\n3      9.1637456 0.28874458     1.960202  8.6132458  9.7456775    11.00421\n4      9.9949925 0.30017346     1.960202  9.4217947 10.5989817    12.00241\n5      3.2248373 0.17680053     1.960202  2.8957068  3.5899942    21.01828\n6      5.6785178 0.23161705     1.960202  5.2411937  6.1499638    37.01044\n7      3.2148222 0.17653492     1.960202  2.8862151  3.5794637    20.95300\n8      3.2248373 0.17680053     1.960202  2.8957068  3.5899942    21.01828\n9      0.3605408 0.05998472     1.960202  0.2601621  0.4994549    26.08696\n10     0.3605408 0.05998472     1.960202  0.2601621  0.4994549    26.08696\n11     0.4006009 0.06321673     1.960202  0.2939655  0.5457064    28.98551\n12     0.2603906 0.05100282     1.960202  0.1773397  0.3821865    18.84058\n      se_row t_crit_row  lcl_row  ucl_row\n1  0.5473707   1.960202 45.92799 48.07358\n2  0.5025506   1.960202 29.01822 30.98824\n3  0.3432094   1.960202 10.34925 11.69521\n4  0.3564220   1.960202 11.32112 12.71881\n5  1.0412961   1.960202 19.04975 23.13209\n6  1.2339820   1.960202 34.62587 39.46012\n7  1.0401075   1.960202 18.98696 23.06466\n8  1.0412961   1.960202 19.04975 23.13209\n9  3.7515606   1.960202 19.42163 34.07250\n10 3.7515606   1.960202 19.42163 34.07250\n11 3.8761776   1.960202 22.00774 37.12261\n12 3.3408449   1.960202 13.13952 26.26721\n\n\nAbove, we have the results of several different descriptive analyses we did in R. Remember that we never want to present raw R output. Perhaps you’ve already thought to yourselves, “wow, these results are really overwhelming. We’re not sure what we’re even looking at.” Well, that’s exactly how many of the people in your audience will feel as well. In its current form, this information is really hard for us to process. We want to take some of the information from the output above and use it to create a Table 1 in Word that is much easier to read.\nSpecifically, we want our final Table 1 to look like this:\n\n\n\n\n\n\n\n\n\nYou may also click here to view/download the Word file that contains Table 1.\nNow that you’ve seen the end result, let’s learn how to make this Table 1 together, step-by-step. Go ahead and open Microsoft Word now if you want to follow along.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#table-columns",
    "href": "chapters/creating_word_tables/creating_word_tables.html#table-columns",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.3 Table columns",
    "text": "40.3 Table columns\nThe first thing we typically do is figure out how many columns and rows our table will need. This is generally pretty straightforward; although, there are exceptions. For a basic Table 1 like the one we are creating above we need the following columns:\nOne column for our row headers (i.e., the names and categories of the variables we are presenting in our analysis).\n\n\n\n\n\n\n\n\n\nOne column for each subgroup that we will be describing in our table. In this case, there are 3 subgroups so we will need 3 additional columns.\n\n\n\n\n\n\n\n\n\nSo, we will need 4 total columns.\n\n\n\n\n\n\nNote\n\n\n\nIf you are going to describe the entire sample overall without stratifying it into subgroups then you would simply have 2 columns. One for the row headers and one for the values.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#table-rows",
    "href": "chapters/creating_word_tables/creating_word_tables.html#table-rows",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.4 Table rows",
    "text": "40.4 Table rows\nNext, we need to figure out how many rows our table will need. This is also pretty straightforward. Generally, we will need the following rows:\nOne row for the title. Some people write their table titles outside (above or below) the actual table. We like to include the title directly in the top row of the table. That way, it moves with the table if the table gets moved around.\n\n\n\n\n\n\n\n\n\nOne row for the column headers. The column headers generally include a label like “Characteristic” for the row headers column and a descriptive label for each subgroup we are describing in our table.\n\n\n\n\n\n\n\n\n\nOne row for each variable we will analyze in our analysis. In this example, we have three – age, sex, and education. NOTE that we do NOT need a separate row for each category of each variable.\n\n\n\n\n\n\n\n\n\nOne row for the footer.\n\n\n\n\n\n\n\n\n\nSo, we will need 6 total rows.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#make-the-table-skeleton",
    "href": "chapters/creating_word_tables/creating_word_tables.html#make-the-table-skeleton",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.5 Make the table skeleton",
    "text": "40.5 Make the table skeleton\nNow that we know we need to create a table with 4 columns and 6 rows, let’s go ahead and do that in Microsoft Word. We do so by clicking the Insert tab in the ribbon above our document. Then, we click the Table button and select the number of columns and rows we want.\n\n\n\n\n\n\n\n\nFigure 40.1: A gif showing how to insert a table into a Microsoft Word document.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#fill-in-column-headers",
    "href": "chapters/creating_word_tables/creating_word_tables.html#fill-in-column-headers",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.6 Fill in column headers",
    "text": "40.6 Fill in column headers\nNow we have our table skeleton. The next thing we would typically do is fill in the column headers. Remember that our column headers look like this:\n\n\n\n\n\n\n\n\n\nHere are a couple of suggestions for filling in your column headers:\n\nPut your column headers in the second row of the empty table shell. The title will eventually go into the first row. We don’t add the title right away because it is typically long and will distort the table’s dimensions. Later, we will see how to horizontally merge table cells to remove this distortion, but we don’t want to do that now. Right now, we want to leave all the cells unmerged so that we can easily resize our columns.\nThe first column header is generally a label for our row headers. Because the rows are typically characteristics of our sample, we almost always use the word “characteristic” here. If you come up with a better word, please feel free to use it.\nThe rest of the column headers are generally devoted to the subgroups we are describing.\n\nThe subgroups should be ordered in a way that is meaningful. For example, by level of severity or chronological order. Typically, ordering in alphabetical order isn’t that meaningful.\nThe subgroup labels should be informative and meaningful, but also succinct. This can sometimes be a challenge.\nwe have seen terms like “Value”, “All”, and “Full Sample” used when Table 1 was describing the entire sample overall rather than describing the sample by subgroups.\n\n\n\n40.6.1 Group sample sizes\nYou should always include the group sample size in the column header. They should typically be in the format “(n = sample size)” and typed in the same cell as the label, but below the label (i.e., hit the return key). The group sample sizes can often provide important context to the statistics listed below in the table, and clue the reader into missing data issues.\n\n\n\n\n\n\n\n\n\n\n\n40.6.2 Formatting column headers\nwe generally bold our column headers, horizontally center them, and vertically align them to the bottom of the row.\nAt this point, your table should look like this in Microsoft Word:",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#fill-in-row-headers",
    "href": "chapters/creating_word_tables/creating_word_tables.html#fill-in-row-headers",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.7 Fill in row headers",
    "text": "40.7 Fill in row headers\nThe next thing we would typically do is fill in the row headers. Remember, that our row headers look like this:\n\n\n\n\n\n\n\n\n\nHere are a couple of suggestions for filling in your row headers:\n\nThe variables should be organized in a way that is meaningful. In our example, we have only 3 sociodemographic variables. However, if we also had some variables about health status and some variables related to criminal history, then we would almost certainly want the variables that fit into each of these categories to be vertically arranged next to each other.\nLike the column headers, the row headers should be informative and meaningful, but also succinct. Again, this can sometimes be a challenge. In our example, we use “Age”, “Sex”, and “Education”. Something like “Highest level of formal education completed” would have also been informative and meaningful, but not succinct. Something like “Question 6” is succinct, but isn’t informative or meaningful at all.\n\n\n40.7.1 Label statistics\nYou should always tell the reader what kind of statistics they are looking at – don’t assume that they know. For example, the highlighted numbers in figure @ref(fig:what-stats) are 36.8 and 10. What is 36.8? The mean, the median? The percentage of people who had a non-missing value for age? What is 10? The sample size? The standard error of the mean? An odds ratio? You know that 36.8 is a mean and 10 is the standard deviation because we identified what they were in the row header. @ref(fig:identify-stats) When you label the statistics in the row headers as we’ve done in our example, they should take the format you see in figure @ref(fig:identify-stats). That is, the variable name, followed by a comma, followed by the statistics used in that row. Also notice the use of parentheses. We used parentheses around the letters “sd” (for standard deviation) because the numbers inside the parentheses in that row are standard deviations. So, the label used to identify the statistics should give the reader a blueprint for interpreting the statistics that matches the format of the statistics themselves.\n\n\n\n\n\nWhat are these numbers?\n\n\n\n\n\n\n\n\n\nIdentifying statistics in the row header.\n\n\n\n\nThe statistics can, and sometimes are, labeled in the column header instead of the row header. This can sometimes be a great idea. However, it can also be a source of confusion. For example, in the figure below, the column headers include labels (i.e., n (%)) for the statistics below. However, not all the statistics below are counts (n) and percentages!\n\n\n\n\n\n\n\n\n\nEven though the Age variable has its own separate statistics label in the row header, this is still generally a really bad idea! Therefore, we highly recommend only labeling your statistics in the column header when those labels are accurate for every value in the column. For example:\n\n\n\n\n\n\n\n\n\n\n\n40.7.2 Formatting row headers\n\nWhenever possible, make sure that variable name and statistic identifier fit on one line (i.e., they don’t carry over into the line below).\nAlways type the category labels for categorical variables in the same cell as the variable name. However, each category should have its own line (i.e., hit the return key).\nWhenever possible, make sure that each category label fits on one line (i.e., it doesn’t carry over into the line below).\nIndent each category label two spaces to the left of the variable name.\nHit the return key once after the last category for each categorical variable. This creates a blank line that adds vertical separation between row headers and makes them easier to read.\n\nAt this point, your table should look like this in Microsoft Word:",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#fill-in-data-values",
    "href": "chapters/creating_word_tables/creating_word_tables.html#fill-in-data-values",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.8 Fill in data values",
    "text": "40.8 Fill in data values\nSo, we have some statistics visible to us on the screen in RStudio. Somehow, we have to get those numbers over to our table in Microsoft Word. There are many different ways we can do this. We’re going to compare a few of those ways here.\n\n40.8.1 Manually type values\nOne option is to manually type the numbers into your word document.\n👍 If you are in a hurry, or if you just need to update a small handful of statistics in your table, then this option is super straightforward. However, there are at least two big problems with this method.\n👎 First, it is extremely error prone. Most people are very likely to type a wrong number or misplace a decimal here and there when they manually type statistics into their Word tables.\n👎 Second, it isn’t very scalable. What if you need to make very large tables with lots and lots of numbers? What if you update your data set and need to change every number in your Word table? This is not fun to do manually.\n\n\n40.8.2 Copy and paste values\nAnother option is to copy and paste values from RStudio into Word. This option is similar to above, but instead of typing each value into your Word table, you highlight and copy the value in RStudio and paste it into Word.\n👍 If you are in a hurry, or if you just need to update a small handful of statistics in your table, then this option is also pretty straightforward. However, there are still issues associated with this method.\n👎 First, it is still somewhat error prone. It’s true that the numbers and decimal placements should always be correct when you copy and paste; however, you may be surprised by how often many people accidentally paste the values into the wrong place or in the wrong order.\n👎 Second, I’ve noticed that there are often weird formatting things that happen when we copy from RStudio and paste into Word. They are usually pretty easy to fix, but this is still a small bit of extra hassle.\n👎 Third, it isn’t very scalable. Again, if we need to make very large tables with lots and lots of numbers or update our data set and need to change every number in your Word table, this method is time-consuming and tedious.\n\n\n40.8.3 Knit a Word document\nSo far, we have only used the HTML Notebook output type for our R markdown files. However, it’s actually very easy have RStudio create a Word document from you R markdown files. We don’t have all the R knowledge we need to fully implement this method yet, so we don’t want to confuse you by going into the details here. But, we do want to mention that it is possible.\n👍 The main advantages of this method are that it is much less error prone and much more scalable than manually typing or copying and pasting values.\n👎 The main disadvantages are that it requires more work on the front end and still requires you to open Microsoft Word a do a good deal of formatting of the table(s).\n\n\n40.8.4 flextable and officer\nA final option we’ll mention is to create your table with the flextable and officer packages. This is our favorite option, but it is also definitely the most complicated. Again, we’re not going to go into the details here because they would likely just be confusing for most readers.\n👍 This method essentially overcomes all of the previous methods’ limitations. It is the least error prone, it is extremely scalable, and it allows us to do basically all the formatting in R. With a push of a button we have a complete, perfectly formatted table output to a Word document. If we update our data, we just push the button again and we have a new perfectly formatted table.\n👎 The primary downside is that this method requires you to invest some time in learning these packages, and requires the greatest amount of writing code up front. If you just need to create a single small table that you will never update, this method is probably not worth the effort. However, if you absolutely need to make sure that your table has no errors, or if you will need to update your table on a regular basis, then this method is definitely worth learning.\n\n\n40.8.5 Significant digits\nNo matter which of the methods above you choose, you will almost never want to give your reader the level of precision that R will give you. For example, the first row of the R results below indicates that 83.274912% of our sample reported that they don’t use drugs.\n\n\n    var              cat    n n_total   percent        se   t_crit      lcl\n1 use_f        Non-users 8315    9985 83.274912 0.3734986 1.960202 82.52992\n2 use_f  Use other drugs 1532    9985 15.343015 0.3606903 1.960202 14.64925\n3 use_f Use opioid drugs  138    9985  1.382073 0.1168399 1.960202  1.17080\n        ucl\n1 83.994296\n2 16.063453\n3  1.630841\n\n\nNotice the level of precision there. R gives us the percentage out to 6 decimal places. If you fill your table with numbers like this, it will be much harder for your readers to digest your table and make comparisons between groups. It’s just the way our brains work. So, the logical next question is, “how many decimal places should we report?” Unfortunately, this is another one of those times that we have to give you an answer that may be a little unsatisfying. It is true that there are rules for significant figures (significant digits); however, the rules are not always helpful to students in my experience. Therefore, we’re going to share with you a few things we try to consider when deciding how many digits to present.\n\nwe don’t recall ever presenting a level of precision greater than 3 decimal places in the research we’ve been involved with. If you are working in physics or genetics and measuring really tiny things it may be totally valid to report 6, 8, or 10 digits to the right of the decimal. But, in epidemiology – a population science – this is rarely, if ever, useful.\nWhat is the overall message we are trying to communicate? That is the point of the table, right? We’re trying to clearly and honestly communicate information to our readers. In general, the simpler the numbers are to read and compare, the clearer the communication. So, we tend to err on the side of simplifying as much as possible. For example, in the R results below, we could say that 83.274912% of our sample reported that they don’t use drugs, 15.343015% reported that they use drugs other than opioids, and 1.382073% reported that they use opioid drugs. Is saying it that way really more useful than saying that “83% of our sample reported that they don’t use drugs, 15% reported that they use drugs other than opioids, and 1% reported that they use opioid drugs”? Are we missing any actionable information by rounding our percentages to the nearest integer here? Are our overall conclusions about drug use any different? No, probably not. And, the rounded percentages are much easier to read, compare, and remember.\nBe consistent – especially within a single table. We have experienced some rare occasions where it made sense to round one variable to 1 decimal place and another variable to 3 decimal places in the same table. But, circumstances like this are definitely the exception. Generally speaking, if you round one variable to 1 decimal place then you want to round them all to one decimal place.\n\nLike all other calculations we’ve done in this book, we suggest you let R do the heavy lifting when it comes to rounding. In other words, have R round the values for you before you move them to Word. R is much less likely to make a rounding error than you are! You may recall that we learned how to round in the chapter on numerical descriptions of categorical variables.\n\n\n40.8.6 Formatting data values\nNow that we have our appropriately rounded values in our table, we just need to do a little formatting before we move on.\nFirst, make sure to fix any fonts, font sizes, and/or background colors that may have been changed if you copied and pasted the values from RStudio into Word.\nSecond, make sure the values line up horizontally with the correct variable names and category labels.\nThird, we tend to horizontally center all our values in their columns.\nAt this point, your table should look like this in Microsoft Word:",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#fill-in-title",
    "href": "chapters/creating_word_tables/creating_word_tables.html#fill-in-title",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.9 Fill in title",
    "text": "40.9 Fill in title\nAt this point in the process, we will typically go ahead and add the title to the first cell of our Word table. The title should always start with “Table #.” In our case, it will start with “Table 1.” In general, we use bold text for this part of the title. What comes next will change a little bit from table to table but is extremely important and worth putting some thought into.\nRemember, all tables and figures need to be able to stand on their own. What does that mean? It means that if we pick up your report and flip straight to the table, we should be able to understand what it’s about and how to read it without reading any of the other text in your report. The title is a critical part of making a table stand on its own. In general, your title should tell the reader what the table contains (e.g., sociodemographic characteristics) and who the table is about (e.g., results of the Texas Opioid Study). We will usually also add the size of the sample of people included in the table (e.g., n = 9985) and the year the data was collected (e.g., 2020).\nIn different circumstances, more or less information may be needed. However, always ask yourself, “can this table stand on its own? Can most readers understand what’s going on in this table even if they didn’t read the full report?”\nAt this point, your table should look like this in Microsoft Word:\n\n\n\n\n\n\n\n\n\nDon’t worry about your title being all bunched up in the corner. We will fix it soon.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#fill-in-footnotes",
    "href": "chapters/creating_word_tables/creating_word_tables.html#fill-in-footnotes",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.10 Fill in footnotes",
    "text": "40.10 Fill in footnotes\nFootnotes are another tool we can use to help our table stand on its own. The footnotes give readers additional information that they may need to read and understand our table. Again, there are few hard and fast rules regarding what footnotes you should include, but we can give you some general categories of things to think about.\nFirst, use footnotes to explain any abbreviations in your table that aren’t standard and broadly understood. These abbreviations are typically related to statistics used in the table (e.g., RR = risk ratio) and/or units of measure (e.g., mmHg = millimeters of mercury). Admittedly, there is some subjectivity associated with “standard and broadly understood.” In our example, we did not provide a footnote for “n”, “sd”, or “%” because most researchers would agree that these abbreviations are standard and broadly understood, but we typically do provide footnotes for statistics like “OR” (odds ratio) and “RR” (relative risk or risk ratio).\nAdditionally, we mentioned above that it is desirable, but sometimes challenging, to get your variable names and category labels to fit on a single line. Footnotes can sometimes help with this. In our example, instead of writing “Age in years, mean (sd)” as a row header we wrote “Age, mean (sd)” and added a footnote that tells the reader that age is measured in years. This may not be the best possible example, but hopefully you get the idea.\n\n40.10.1 Formatting footnotes\nWhen using footnotes, you need to somehow let the reader know which element in the table each footnote goes with. Sometimes, there will be guidelines that require you to use certain symbols (e.g., *, †, and ‡), but we typically use numbers to match table elements to footnotes when we can. In the example below, there is a superscript “1” immediately after the word “Age” that lets the reader know that footnote number 1 is adding additional information to this part of the table.\n\n\n\n\n\n\n\n\n\nIf you do use numbers to match table elements to footnotes, make sure you do so in the order people read [English], which is left to right and top to bottom. For example, the following would be inappropriate because the number 2 comes before the number 1 when reading from top to bottom:\n\n\n\n\n\n\n\n\n\nAs another example, the following would be inappropriate because the number 2 comes before the number 1 when reading from left to right:\n\n\n\n\n\n\n\n\n\nAdditionally, when using numbers to match table elements to footnotes, it’s a good idea to superscript the numbers in the table. This makes it clear that the number is being used to identify a footnote rather than being part of the text or abbreviation. Formatting a number as a superscript is easy in Microsoft Word. Just highlight the number you want to format and click the superscript button.\n\n\n\n\n\n\n\n\nFigure 40.2: A gif showing the superscript button in Microsoft Word.\n\n\n\n\n\nAt this point, your table should look like this in Microsoft Word:",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#final-formatting",
    "href": "chapters/creating_word_tables/creating_word_tables.html#final-formatting",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.11 Final formatting",
    "text": "40.11 Final formatting\nWe have all of our data and explanatory text in our table. The last few remaining steps are just about formatting our table to make it as easy to read and digest as possible.\n\n40.11.1 Adjust column widths\nAs I’ve already mentioned more than once, we don’t want our text carryover onto multiple lines whenever we can help it. In my experience, this occurs most often in the row headings. Therefore, we will often need to adjust (widen) the first column of our table. You can do that by clicking on the black border that separates the columns and moving your mouse left or right.\n\n\n\n\n\n\n\n\nFigure 40.3: A gif showing the superscript button in Microsoft Word.\n\n\n\n\n\nAfter you adjust the width of your first column, the widths of the remaining columns will likely be uneven. To distribute the remaining space in the table evenly among the remaining columns, first select the columns by clicking immediately above the first column you want to select and dragging your cursor across the remaining columns. Then, click the layout tab in the ribbon above your document and the Distribute Columns button.\n\n\n\n\n\n\n\n\nFigure 40.4: A gif showing how to distribute columns evenly in Microsoft Word.\n\n\n\n\n\nIn our particular example, there was no need to adjust column widths because all of our text fit into the default widths.\n\n\n40.11.2 Merge cells\nNow, we can finally merge some cells so that our title and footnote spread the entire width of the table. We waited until now to merge cells because if we had done so earlier it would have made the previous step (i.e., adjusting column widths) more difficult.\nTo spread our title out across the entire width of the table, we just need to select all the cells in the first row, then right click and select merge cells.\n\n\n\n\n\n\n\n\nFigure 40.5: A gif showing how to merge table cells in Microsoft Word.\n\n\n\n\n\nAfter merging the footnote cells in exactly the same way, your table should look like this:\n\n\n\n\n\n\n\n\n\n\n\n40.11.3 Remove cell borders\nThe final step is to clean up our borders. In my experience, students like to do all kinds of creative things with cell borders. However, when it comes to borders, keeping it simple is usually the best approach. Therefore, we will start by removing all borders in the table. We do so by clicking the little cross with arrowheads that pops up diagonal to the top-left corner of the table when you move your mouse over it. Clicking this button will highlight your entire table. Then, we will click the downward facing arrow next to the borders button in the ribbon above your document. Then, we will click the No Border option.\n\n\n\n\n\n\n\n\nFigure 40.6: A gif showing how to remove table borders in Microsoft Word.\n\n\n\n\n\nOur final step will be to add a single horizontal border under the title, a single horizontal border under the column header row, and a single horizontal border above the footnotes. We will add the borders by highlighting the correct rows and selecting the correct options for the same borders dropdown menu we used above.\n\n\n\n\n\n\n\n\nFigure 40.7: A gif showing how to add borders to table cells in Microsoft Word.\n\n\n\n\n\nNotice that there are no vertical lines (borders) anywhere on our table. That should almost always be the case for your tables too.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "chapters/creating_word_tables/creating_word_tables.html#summary",
    "href": "chapters/creating_word_tables/creating_word_tables.html#summary",
    "title": "40  Creating Tables with R and Microsoft Word",
    "section": "40.12 Summary",
    "text": "40.12 Summary\nJust like with guidelines we’ve discussed about R coding style; you don’t have to create tables in exactly the same way that we do. But, you should have a good reason for all the decisions you make leading up\n\n\n\n\n1. Matthews JR, Matthews RW. Successful Scientific Writing: A Step-by-Step Guide for the Biological and Medical Sciences. Cambridge University Press; 2014.",
    "crumbs": [
      "Presenting Results",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Creating Tables with R and Microsoft Word</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "41  References",
    "section": "",
    "text": "1. Bryan J. Happy Git and GitHub\nfor the useR.; 2016.\n\n\n2. Field A, Miles J, Field Z. Discovering\nStatistics Using R. Sage; 2013.\n\n\n3. GitHub. Licensing a repository. Published\nonline May 2022.\n\n\n4. GitHub. About repositories. Published online\nDecember 2023.\n\n\n5. Grolemund G, Wickham H. R for\nData Science.; 2017.\n\n\n6. Ismay C, Kim AY. Chapter 1 getting started with\ndata in R. Published online November 2019.\n\n\n7. Matthews JR, Matthews RW. Successful\nScientific Writing: A Step-by-Step Guide for\nthe Biological and Medical Sciences. Cambridge University Press;\n2014.\n\n\n8. Peng\nRD. Reproducible research in computational science. Science.\n2011;334(6060):1226-1227.\n\n\n9. Peng\nRD, Hicks SC. Reproducible research: A retrospective. Annu Rev\nPublic Health. 2021;42:79-93.\n\n\n10. R\nCore Team. What Is r? R Foundation for Statistical Computing;\n2024.\n\n\n11. RStudio. RStudio. Published online\n2020.\n\n\n12. RStudio. FAQ: Tips for writing\nr-related questions. Published online September 2021.\n\n\n13. Stack Overflow. What are tags, and how should\nI use them? Published online January 2022.\n\n\n14. Stack Overflow. How do I ask a\ngood question? Published online January 2022.\n\n\n15. Wickham H. Tidy data. Journal of\nStatistical Software, Articles. 2014;59(10):1-23.\n\n\n16. Wickham H. Style guide. In: Advanced\nR.; 2019.\n\n\n17. Wickham H, François R, Henry L, Müller K,\nRStudio. Programming with Dplyr.; 2020.\n\n\n18. Wickham H, Çetinkaya-Rundel M, Grolemund G.\nWorkflow: Code style. In: R for Data Science.\nsecond.; 2023.",
    "crumbs": [
      "References",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/glossary.html",
    "href": "chapters/appendices/glossary.html",
    "title": "Appendix A — Glossary",
    "section": "",
    "text": "Anchor\n\nA regular expression (regex) metacharacter that anchors a match to a position in a string. The caret (^) anchors to the start of the string, and the dollar sign ($) anchors to the end of the string.\n\nArguments\n\nValues provided inside the parentheses of a function to specify what the function should act on or how it should behave.\n\nBivariate\n\nDescribes analyses or relationships involving exactly two variables.\n\nCollapse\n\nTo summarize a data set by grouping and reducing multiple observations into a single summary value per group, often using functions like summarise() in dplyr.\n\nComplete case analysis\n\nAn analysis that includes only observations with complete data for all variables in the model, excluding any rows with missing values.\n\nConsole\n\nThe interactive window in RStudio (usually bottom-left) where R commands can be typed and executed immediately. The R console is useful for testing small pieces of code and interactive data exploration. However, we recommend using R scripts or Quarto files for all but the simplest programming or data-analysis tasks.\n\nConditional Operations\n\nConditional operations control the flow of a program by executing different blocks of code depending on whether specified conditions are TRUE or FALSE. In R this includes the if / else family, vectorised helpers such as ifelse(), and higher-level wrappers like case_when().\n\nData Checks\n\nProcesses that verify the accuracy, completeness, or validity of data before analysis. Examples include type checks (e.g., numeric vs. character), range checks (e.g., no ages below 0), completeness checks (e.g., missing-value rates), and cross-field consistency checks (e.g., start ≤ end dates).\n\nData frame\n\nR’s primary data structure for storing tabular data. Each column is a vector, and all columns must have the same number of rows.\n\nFor loop\n\nA control structure that repeats a block of code once for each element in a sequence or vector.\n\nFrequency count\n\nThe number of times a value or category appears in a dataset. Also called a frequency, count, or n.\n\nFunctions\n\nA reusable block of code that performs a specific task when called. Functions take inputs (arguments) and return outputs. Functions promote modularity, abstraction, and reproducibility.\n\nGlobal environment\n\nThe main workspace in an R session where user-defined variables and functions are stored unless otherwise specified.\n\nIssue (GitHub)\n\nA GitHub feature used to track tasks, bugs, enhancements, or other requests within a project.\n\nIteratively\n\nA method of solving a problem by repeatedly executing a set of instructions in a step-by-step manner, often using loops. This approach can improve efficiency and help prevent errors.\n\nJoin\n\nAn operation that merges two tables based on shared key columns. In dplyr, functions like inner_join(), left_join(), etc., determine how unmatched rows are handled.\n\nKey\n\nA column or set of columns that uniquely identifies each row in a data set and enables precise merging with other tables.\n\nLexical scoping rules\n\nA set of rules that determine which variables are accessible in a function based on where they were defined in the code hierarchy.\n\nList-wise Deletion\n\nA method of handling missing data by excluding any row that has at least one missing value in variables of interest, leaving only complete cases.\n\nLong\n\nA tidy data format where each row represents one measurement at one time point for one unit, and columns contain values and their corresponding identifiers (e.g., variable name or time).\n\nMDL\n\nThe Minimum Description Length (MDL) principle is a model selection principle stating that the best model is the one that minimizes the combined complexity of the model and the data encoded using that model.\n\nMarginal totals\n\nRow and column totals in a contingency table, used to summarize the distribution of each variable and to calculate the overall total.\n\nMean\n\nThe arithmetic mean—often denoted \\(\\bar{x}\\)—is calculated by summing all values in a numeric variable and dividing by the total number of values.\n\nMedian\n\nThe middle number in an ordered list of values. When the list has an even number of elements, the median is the average of the two middle numbers. Compared with the mean, the median is relatively resistant to extreme values.\n\nMerge\n\nA base-R term (function merge()) for combining two data frames by matching rows on one or more key variables. Rows that do not match can be kept, dropped, or produce missing values depending on the arguments.\n\nMode\n\nThe value that occurs most often in a set of data. A data set may be unimodal (one mode), multimodal (many modes), or have no mode (all values are equally frequent).\n\nNon-standard Evaluation\n\nA programming behavior in which functions capture or modify expressions instead of immediately evaluating their values, commonly used in tidyverse packages.\n\nObjects\n\nNamed containers for storing data or functions in R. Common object types include vectors, lists, data frames, and functions.\n\nOutcome variable\n\nThe variable being predicted or explained in an analysis; also called the dependent variable.\n\nPass\n\nTo supply a value or object to a function’s argument when calling that function. For example, passing 2 to the from argument in seq(from = 2, to = 100, by = 2).\n\nPercentage\n\nA value representing a part per hundred. Calculated by dividing the number of occurrences by the total number of observations and multiplying by 100. For example, 25% means 25 out of 100.\n\nPerson-level\n\nDescribes data organized at the level of the individual, where each row corresponds to one person.\n\nPerson-period\n\nDescribes data structured with multiple rows per person, usually representing repeated measurements across time or conditions.\n\nPredictor variable\n\nA variable used to explain or predict the value of another variable; also called an independent variable or explanatory variable.\n\nProportion\n\nA number between 0 and 1 that represents the fraction of a total. Calculated by dividing the number of occurrences by the total number of observations.\n\nQuantifier\n\nA regular expression metacharacter that defines how many times a pattern must repeat. Common quantifiers include * (0 +), + (1 +), ? (0 – 1), and {m,n} (between m and n times).\n\nR\n\n“R is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues.”1 R is open source, and you can download it for free from The Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/.\n\nRange\n\nThe difference between the maximum and minimum values in a data set.\n\nRegular Expressions\n\nCompact strings that describe search patterns for text. Regular expressions (regexes) are used for tasks such as finding, extracting, replacing, or validating character data (stringr, grepl(), gsub(), etc.).\n\nRepository\n\n“A repository contains all of your code, your files, and each file’s revision history. You can discuss and manage your work within the repository.”2 A repository can exist locally on your computer or remotely on a server such as GitHub.\n\nReturn\n\nA command in a function that specifies what value to output when the function finishes running. Instead of saying, “the seq() function gives us a sequence of numbers…,” we could say, “the seq() function returns a sequence of numbers.”\n\nRStudio\n\nAn integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor with direct code execution, and tools for plotting, debugging, and work space management. RStudio is available as open-source desktop software and as server versions.3\n\nSplit - Apply - Combine\n\nA data-analysis strategy used by dplyr::group_by(), for example, that involves splitting data into smaller components, applying a calculation separately to each component, and then combining the individual results into a single output.\n\nStandard deviation\n\nA measure of spread equal to the square root of the variance—the average of the squared differences between each value and the mean.\n\nToken\n\nA basic unit in text processing, typically referring to individual pieces of data like words, numbers, or punctuation marks. Tokens include literal characters (a), metacharacters (\\d), or entire character classes ([A-Z]).\n\nTwo-way frequency table\n\nA table that displays the joint distribution of two categorical variables, showing the frequency of each combination of categories. Also called a crosstab or contingency table.\n\nUnivariate\n\nPertaining to a single variable. Univariate analysis describes or summarizes one variable at a time.\n\nVariance\n\nA measure of spread calculated as the average of the squared differences between each value and the mean.\n\nVectorization\n\nA programming technique in which operations are applied to entire vectors (or matrices/data frames) in a single step rather than iterating element-by-element. Vectorized code in R (x * 2, mean(x)) is clear and fast because the heavy computation occurs in compiled code under the hood.\n\nWide\n\nA data format where repeated measures or variables are spread across multiple columns (e.g., score_T1, score_T2, score_T3 for test scores across three terms), with one row per subject or unit.\n\n\n\n\n\n\n1. R Core Team. What Is r? R Foundation for Statistical Computing; 2024.\n\n\n2. GitHub. About repositories. Published online December 2023.\n\n\n3. RStudio. RStudio. Published online 2020.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Glossary</span>"
    ]
  }
]